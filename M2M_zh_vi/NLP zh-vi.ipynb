{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "906e57c2-e534-4298-95d4-4e781ddf192e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.9.1)\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.57.3)\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.10/site-packages (4.4.2)\n",
      "Requirement already satisfied: sentencepiece in ./.local/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: sacrebleu in ./.local/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.10/site-packages (1.12.0)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.local/lib/python3.10/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.local/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.local/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.local/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.local/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.local/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.local/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.local/lib/python3.10/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.local/lib/python3.10/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.local/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.local/lib/python3.10/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.local/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.local/lib/python3.10/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: portalocker in ./.local/lib/python3.10/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in ./.local/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in ./.local/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in ./.local/lib/python3.10/site-packages (from sacrebleu) (6.0.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.local/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers datasets sentencepiece sacrebleu accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7db049-583c-40da-b672-60e3c375839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    M2M100ForConditionalGeneration,\n",
    "    M2M100Tokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoModelForSeq2SeqLM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0056f36-4e7f-4120-97c5-69d91b429170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA H200\n",
      "VRAM: 150.02 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "MODEL_NAME = \"facebook/m2m100_418M\"\n",
    "\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(MODEL_NAME).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddcee659-1dc4-414b-940e-20936a0dc6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã mở băng toàn bộ Model. Sẵn sàng huấn luyện chuyên sâu.\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\"Đã mở băng toàn bộ Model. Sẵn sàng huấn luyện chuyên sâu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b80ff6-b45f-4889-8028-4caa6728f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "DATA_DIR = \"/home/admin/dataset\"\n",
    "\n",
    "def load_parallel(src_file, tgt_file):\n",
    "    with open(src_file, encoding=\"utf-8\") as f:\n",
    "        src = [l.strip() for l in f]\n",
    "    with open(tgt_file, encoding=\"utf-8\") as f:\n",
    "        tgt = [l.strip() for l in f]\n",
    "    assert len(src) == len(tgt)\n",
    "    return Dataset.from_dict({\n",
    "        \"src_text\": src,\n",
    "        \"tgt_text\": tgt\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47538052-1bbc-41eb-91c0-7661c1ac3a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 300348 | Dev: 1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = load_parallel(\n",
    "    f\"{DATA_DIR}/train2022.zh\",  \n",
    "    f\"{DATA_DIR}/train2022.vi\"    \n",
    ")\n",
    "\n",
    "dev_dataset = load_parallel(\n",
    "    f\"{DATA_DIR}/dev2022.zh.txt\",\n",
    "    f\"{DATA_DIR}/dev2022.vi.txt\"\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} | Dev: {len(dev_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca3dfe25-0441-46dc-8795-ec34f0c2efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "\n",
    "def preprocess(batch):\n",
    "    tokenizer.src_lang = \"zh\"\n",
    "    tokenizer.tgt_lang = \"vi\"\n",
    "    inputs = tokenizer(batch[\"src_text\"], truncation=True, max_length=MAX_LEN)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(batch[\"tgt_text\"], truncation=True, max_length=MAX_LEN)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df9bca88-95ad-4901-93b1-a76f17843aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce6f7135dfd49118fb81058ec977aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/300348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56973574d7a14d37a4d8cb91f212cef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=8\n",
    ")\n",
    "\n",
    "dev_dataset = dev_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dev_dataset.column_names,\n",
    "    num_proc=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abd111fc-5baa-4d6b-a54e-a8b445818418",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04eab62c-15fe-4ec1-8dbb-99c31a7be7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/home/admin/checkpoint\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=64,\n",
    "\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,\n",
    "    warmup_steps=1000,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1729c1ed-1b2c-4a0c-b18b-e4b111b5e148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15923/1190918925.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c2927f2-5c52-4528-8624-ff86da1545d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='23470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/23470 25:56 < 31:19, 6.10 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.007600</td>\n",
       "      <td>1.527786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.922700</td>\n",
       "      <td>1.519608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.897600</td>\n",
       "      <td>1.505428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.889400</td>\n",
       "      <td>1.495252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.780800</td>\n",
       "      <td>1.497468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.788200</td>\n",
       "      <td>1.492633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.777800</td>\n",
       "      <td>1.481292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.772400</td>\n",
       "      <td>1.471444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.772600</td>\n",
       "      <td>1.460001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.651100</td>\n",
       "      <td>1.475682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.654500</td>\n",
       "      <td>1.470401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.647500</td>\n",
       "      <td>1.455540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.657400</td>\n",
       "      <td>1.459552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.545400</td>\n",
       "      <td>1.462281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.534700</td>\n",
       "      <td>1.453260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.556400</td>\n",
       "      <td>1.451588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.549500</td>\n",
       "      <td>1.457677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.526200</td>\n",
       "      <td>1.456636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.442000</td>\n",
       "      <td>1.461777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12000, training_loss=1.3592805417378744, metrics={'train_runtime': 1556.9176, 'train_samples_per_second': 1929.119, 'train_steps_per_second': 15.075, 'total_flos': 2.789931480589271e+17, 'train_loss': 1.3592805417378744, 'epoch': 5.112910097997443})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0df3417-d43a-494e-959f-ff1069ca218a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/admin/checkpoint/best_zh_vi/tokenizer_config.json',\n",
       " '/home/admin/checkpoint/best_zh_vi/special_tokens_map.json',\n",
       " '/home/admin/checkpoint/best_zh_vi/vocab.json',\n",
       " '/home/admin/checkpoint/best_zh_vi/sentencepiece.bpe.model',\n",
       " '/home/admin/checkpoint/best_zh_vi/added_tokens.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"/home/admin/checkpoint/best_zh_vi\")\n",
    "tokenizer.save_pretrained(\"/home/admin/checkpoint/best_zh_vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9ec4b06-76ed-4905-8227-0b4f1138748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sacrebleu import corpus_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d311f272-55d9-49bb-b53e-f0cd71a87813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_zh_vi(src_file):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer.src_lang = \"zh\"\n",
    "    tokenizer.tgt_lang = \"vi\"\n",
    "\n",
    "    with open(src_file, encoding=\"utf-8\") as f:\n",
    "        lines = [l.strip() for l in f if l.strip()]\n",
    "\n",
    "    print(f\"Đang dịch {len(lines)} câu ZH → VI\")\n",
    "\n",
    "    for line in lines:\n",
    "        inputs = tokenizer(\n",
    "            line,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=256\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=tokenizer.get_lang_id(\"vi\"),  \n",
    "                num_beams=5,\n",
    "                max_length=256\n",
    "            )\n",
    "\n",
    "        outputs.append(\n",
    "            tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e04ff6f9-2f1d-473e-8008-751b37a77914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang dịch 1000 câu ZH → VI\n",
      "\n",
      "==============================\n",
      "KẾT QUẢ ZH-VI TRÊN H200\n",
      "SacreBLEU Score: 36.84\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "preds = translate_zh_vi(f\"{DATA_DIR}/test.vi-zh.2022.zh\")\n",
    "\n",
    "# Đọc file reference tương ứng\n",
    "with open(f\"{DATA_DIR}/test.vi-zh.2022.vi\", encoding=\"utf-8\") as f:\n",
    "    refs = [f.read().splitlines()]\n",
    "\n",
    "# Tính BLEU\n",
    "bleu = corpus_bleu(preds, refs, tokenize='intl')\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"KẾT QUẢ ZH-VI TRÊN H200\")\n",
    "print(f\"SacreBLEU Score: {bleu.score:.2f}\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45bfb66e-1a7d-4077-a3ab-8f91ca074a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CÂU NGẪU NHIÊN 1 (idx=654) ---\n",
      "ZH (SOURCE):\n",
      "公安力量帮助胡志明市居民购买粮食食品越南政府总理范明政在会上发表讲话时强调，既然决定牺牲经济利益，实施社交距离措施，就要成功控制疫情，要尽可能早和有效击退疫情。\n",
      "\n",
      "VI (MODEL):\n",
      "Trong bài phát biểu tại hội nghị, lực lượng công an hỗ trợ người dân mua lương thực và lương thực Thành phố Hồ Chí Minh, Thủ tướng Phạm Minh Chính khẳng định quyết định hy sinh lợi ích kinh tế, thực hiện các biện pháp cách mạng xã hội thì phải kiểm soát thành công dịch bệnh, sớm và hiệu quả nhất có thể.\n",
      "\n",
      "VI (REAL):\n",
      "Phát biểu tại hội nghị, người đứng đầu Chính phủ nhấn mạnh tinh thần chung: Việt Nam đã hy sinh về kinh tế - xã hội để thực hiện giãn cách thì phải đạt kết quả chống dịch thành công, phải ngăn chặn, đẩy lùi được dịch bệnh sớm, nhanh, hiệu quả.\n",
      "============================================================\n",
      "\n",
      "--- CÂU NGẪU NHIÊN 2 (idx=114) ---\n",
      "ZH (SOURCE):\n",
      "现在落实第128号决议，我们将审视、颁布新规定，确保符合决议目的、要求及卫生部相关防疫规定。\n",
      "\n",
      "VI (MODEL):\n",
      "Bây giờ thực hiện Nghị quyết 128 chúng ta sẽ rà soát, ban hành các quy định mới, bảo đảm phù hợp với mục đích, yêu cầu của Nghị quyết cũng như các quy định về phòng chống dịch bệnh của Bộ Y tế.\n",
      "\n",
      "VI (REAL):\n",
      "Khi triển khai Nghị quyết 128, chúng tôi rà soát, ban hành quy định mới để phù hợp với mục đích yêu cầu, đặc biệt là quy định mới của Bộ y tế.\n",
      "============================================================\n",
      "\n",
      "--- CÂU NGẪU NHIÊN 3 (idx=25) ---\n",
      "ZH (SOURCE):\n",
      "越南不能一直封闭，要重新开放，以解决就业、收入、经济社会发展等问题。\n",
      "\n",
      "VI (MODEL):\n",
      "Việt Nam không thể luôn đóng cửa, phải mở cửa lại để giải quyết các vấn đề như việc làm, thu nhập, phát triển kinh tế - xã hội.\n",
      "\n",
      "VI (REAL):\n",
      "Việt Nam không thể đóng cửa mãi, mà phải mở cửa để giải quyết việc làm, thu nhập, phát triển kinh tế - xã hội.\n",
      "============================================================\n",
      "\n",
      "--- CÂU NGẪU NHIÊN 4 (idx=759) ---\n",
      "ZH (SOURCE):\n",
      "但是，新冠肺炎疫情在一定程度上破坏了供应链，尤其是在机械和技术行业。\n",
      "\n",
      "VI (MODEL):\n",
      "Tuy nhiên, bệnh viêm phổi new coronate đã làm tổn hại đến chuỗi cung ứng, đặc biệt trong ngành công nghiệp máy móc và công nghệ.\n",
      "\n",
      "VI (REAL):\n",
      "Tuy nhiên, đại dịch COVID-19 đã làm chuỗi cung ứng phần nào bị đứt gãy, đặc biệt là các ngành máy móc, công nghệ.\n",
      "============================================================\n",
      "\n",
      "--- CÂU NGẪU NHIÊN 5 (idx=281) ---\n",
      "ZH (SOURCE):\n",
      "检测速度要快、科学、合理。\n",
      "\n",
      "VI (MODEL):\n",
      "Tốc độ kiểm nghiệm phải nhanh, khoa học, hợp lý.\n",
      "\n",
      "VI (REAL):\n",
      "Còn xét nghiệm là phải thần tốc, khoa học, hợp lý và hiệu quả.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "N = 5  # số câu muốn xem ngẫu nhiên\n",
    "\n",
    "# Đọc source ZH\n",
    "with open(f\"{DATA_DIR}/test.vi-zh.2022.zh\", encoding=\"utf-8\") as f:\n",
    "    src_lines = [l.strip() for l in f if l.strip()]\n",
    "\n",
    "# Đọc reference VI\n",
    "with open(f\"{DATA_DIR}/test.vi-zh.2022.vi\", encoding=\"utf-8\") as f:\n",
    "    ref_lines = [l.strip() for l in f if l.strip()]\n",
    "\n",
    "assert len(src_lines) == len(ref_lines)\n",
    "\n",
    "model.eval()\n",
    "tokenizer.src_lang = \"zh\"\n",
    "tokenizer.tgt_lang = \"vi\"\n",
    "\n",
    "idxs = random.sample(range(len(src_lines)), N)\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    src = src_lines[idx]\n",
    "    ref = ref_lines[idx]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        src,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.get_lang_id(\"vi\"),\n",
    "            num_beams=5,\n",
    "            max_length=256\n",
    "        )\n",
    "\n",
    "    pred = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n--- CÂU NGẪU NHIÊN {i+1} (idx={idx}) ---\")\n",
    "    print(\"ZH (SOURCE):\")\n",
    "    print(src)\n",
    "    print(\"\\nVI (MODEL):\")\n",
    "    print(pred)\n",
    "    print(\"\\nVI (REAL):\")\n",
    "    print(ref)\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a51335ec-37cb-4aa6-9a31-bfdab54af294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: /home/admin/checkpoint/best_zh_vi.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "src_dir = \"/home/admin/checkpoint/best_zh_vi\"\n",
    "zip_path = \"/home/admin/checkpoint/best_zh_vi\"\n",
    "\n",
    "shutil.make_archive(zip_path, 'zip', src_dir)\n",
    "\n",
    "print(\"Done:\", zip_path + \".zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9427e3-3c01-4f32-9585-749caf18f592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
