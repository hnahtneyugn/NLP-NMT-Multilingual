{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a8c21a5-f06f-4187-b4a3-bc99aafef9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.9.1)\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.57.3)\n",
      "Requirement already satisfied: sentencepiece in ./.local/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.10/site-packages (4.4.2)\n",
      "Requirement already satisfied: sacrebleu in ./.local/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.10/site-packages (1.12.0)\n",
      "Requirement already satisfied: pyvi in ./.local/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.local/lib/python3.10/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.local/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.local/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.local/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.local/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.local/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.local/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.local/lib/python3.10/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.local/lib/python3.10/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.local/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.local/lib/python3.10/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.local/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.local/lib/python3.10/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: portalocker in ./.local/lib/python3.10/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in ./.local/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in ./.local/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in ./.local/lib/python3.10/site-packages (from sacrebleu) (6.0.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.10/site-packages (from pyvi) (1.7.2)\n",
      "Requirement already satisfied: sklearn-crfsuite in ./.local/lib/python3.10/site-packages (from pyvi) (0.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.local/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.local/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.local/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.local/lib/python3.10/site-packages (from scikit-learn->pyvi) (3.6.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.7 in ./.local/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (0.9.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers sentencepiece datasets sacrebleu accelerate pyvi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cfc5fcf-4be3-4f05-923c-ea600a292b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA H200\n",
      "VRAM: 150.0217344 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup and imports\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    M2M100ForConditionalGeneration,\n",
    "    M2M100Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import random\n",
    "\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(\"VRAM:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4038682a-6b87-4c9e-bf62-483cca34489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khmer tokenizer loaded successfully!\n",
      "Vietnamese and Khmer tokenizers loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Khmer tokenizer\n",
    "# Load Khmer tokenizer from Hugging Face\n",
    "khmer_word_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"khopilot/km-tokenizer-khmer\", \n",
    "    use_fast=False\n",
    ")\n",
    "print(\"Khmer tokenizer loaded successfully!\")\n",
    "\n",
    "# Cell 3b: Define tokenization functions\n",
    "def tokenize_vietnamese(text):\n",
    "    \"\"\"Tokenize Vietnamese text using PyVi\"\"\"\n",
    "    try:\n",
    "        return ViTokenizer.tokenize(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing Vietnamese: {e}\")\n",
    "        return text\n",
    "\n",
    "def tokenize_khmer(text):\n",
    "    \"\"\"Tokenize Khmer text using khopilot/km-tokenizer-khmer\"\"\"\n",
    "    try:\n",
    "        tokens = khmer_word_tokenizer.tokenize(text)\n",
    "        return \" \".join(tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing Khmer: {e}\")\n",
    "        return text\n",
    "\n",
    "def tokenize_batch_vietnamese(texts):\n",
    "    \"\"\"Batch tokenize Vietnamese texts\"\"\"\n",
    "    print(f\"Tokenizing {len(texts)} Vietnamese texts...\")\n",
    "    results = []\n",
    "    for i, text in enumerate(texts):\n",
    "        results.append(tokenize_vietnamese(text))\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(texts)} Vietnamese texts\")\n",
    "    return results\n",
    "\n",
    "def tokenize_batch_khmer(texts):\n",
    "    \"\"\"Batch tokenize Khmer texts\"\"\"\n",
    "    print(f\"Tokenizing {len(texts)} Khmer texts...\")\n",
    "    results = []\n",
    "    for i, text in enumerate(texts):\n",
    "        results.append(tokenize_khmer(text))\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(texts)} Khmer texts\")\n",
    "    return results\n",
    "\n",
    "print(\"Vietnamese and Khmer tokenizers loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e32971b-df08-4c92-92bd-6eeac1009270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 2 model loaded!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"km_to_vi/phase2/best\"\n",
    ").cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"km_to_vi/phase2/best\"\n",
    ")\n",
    "\n",
    "print(\"Phase 2 model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "185d4593-a494-4605-93df-d7c551397af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params       : 483.9M\n",
      "Frozen params      : 206.8M\n",
      "Trainable params   : 277.1M\n",
      "Encoder trainable  : 126.0M\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Unfreeze Top 6 Encoder Layers (layers 6-11)\n",
    "# ============================================================\n",
    "# 1. Freeze toàn bộ encoder trước\n",
    "for param in model.model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. Unfreeze top 6 layers\n",
    "for i in range(6, 12):\n",
    "    for param in model.model.encoder.layers[i].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Verify\n",
    "total, frozen, trainable = 0, 0, 0\n",
    "encoder_trainable = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    n = param.numel()\n",
    "    total += n\n",
    "    if not param.requires_grad:\n",
    "        frozen += n\n",
    "    else:\n",
    "        trainable += n\n",
    "        if \"encoder\" in name:\n",
    "            encoder_trainable += n\n",
    "\n",
    "print(f\"Total params       : {total/1e6:.1f}M\")\n",
    "print(f\"Frozen params      : {frozen/1e6:.1f}M\")\n",
    "print(f\"Trainable params   : {trainable/1e6:.1f}M\")\n",
    "print(f\"Encoder trainable  : {encoder_trainable/1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0933c69c-a773-4ccd-a92b-6198d509785e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Vietnamese texts...\n",
      "Tokenizing 700000 Khmer texts...\n",
      "  Processed 10000/700000 Khmer texts\n",
      "  Processed 20000/700000 Khmer texts\n",
      "  Processed 30000/700000 Khmer texts\n",
      "  Processed 40000/700000 Khmer texts\n",
      "  Processed 50000/700000 Khmer texts\n",
      "  Processed 60000/700000 Khmer texts\n",
      "  Processed 70000/700000 Khmer texts\n",
      "  Processed 80000/700000 Khmer texts\n",
      "  Processed 90000/700000 Khmer texts\n",
      "  Processed 100000/700000 Khmer texts\n",
      "  Processed 110000/700000 Khmer texts\n",
      "  Processed 120000/700000 Khmer texts\n",
      "  Processed 130000/700000 Khmer texts\n",
      "  Processed 140000/700000 Khmer texts\n",
      "  Processed 150000/700000 Khmer texts\n",
      "  Processed 160000/700000 Khmer texts\n",
      "  Processed 170000/700000 Khmer texts\n",
      "  Processed 180000/700000 Khmer texts\n",
      "  Processed 190000/700000 Khmer texts\n",
      "  Processed 200000/700000 Khmer texts\n",
      "  Processed 210000/700000 Khmer texts\n",
      "  Processed 220000/700000 Khmer texts\n",
      "  Processed 230000/700000 Khmer texts\n",
      "  Processed 240000/700000 Khmer texts\n",
      "  Processed 250000/700000 Khmer texts\n",
      "  Processed 260000/700000 Khmer texts\n",
      "  Processed 270000/700000 Khmer texts\n",
      "  Processed 280000/700000 Khmer texts\n",
      "  Processed 290000/700000 Khmer texts\n",
      "  Processed 300000/700000 Khmer texts\n",
      "  Processed 310000/700000 Khmer texts\n",
      "  Processed 320000/700000 Khmer texts\n",
      "  Processed 330000/700000 Khmer texts\n",
      "  Processed 340000/700000 Khmer texts\n",
      "  Processed 350000/700000 Khmer texts\n",
      "  Processed 360000/700000 Khmer texts\n",
      "  Processed 370000/700000 Khmer texts\n",
      "  Processed 380000/700000 Khmer texts\n",
      "  Processed 390000/700000 Khmer texts\n",
      "  Processed 400000/700000 Khmer texts\n",
      "  Processed 410000/700000 Khmer texts\n",
      "  Processed 420000/700000 Khmer texts\n",
      "  Processed 430000/700000 Khmer texts\n",
      "  Processed 440000/700000 Khmer texts\n",
      "  Processed 450000/700000 Khmer texts\n",
      "  Processed 460000/700000 Khmer texts\n",
      "  Processed 470000/700000 Khmer texts\n",
      "  Processed 480000/700000 Khmer texts\n",
      "  Processed 490000/700000 Khmer texts\n",
      "  Processed 500000/700000 Khmer texts\n",
      "  Processed 510000/700000 Khmer texts\n",
      "  Processed 520000/700000 Khmer texts\n",
      "  Processed 530000/700000 Khmer texts\n",
      "  Processed 540000/700000 Khmer texts\n",
      "  Processed 550000/700000 Khmer texts\n",
      "  Processed 560000/700000 Khmer texts\n",
      "  Processed 570000/700000 Khmer texts\n",
      "  Processed 580000/700000 Khmer texts\n",
      "  Processed 590000/700000 Khmer texts\n",
      "  Processed 600000/700000 Khmer texts\n",
      "  Processed 610000/700000 Khmer texts\n",
      "  Processed 620000/700000 Khmer texts\n",
      "  Processed 630000/700000 Khmer texts\n",
      "  Processed 640000/700000 Khmer texts\n",
      "  Processed 650000/700000 Khmer texts\n",
      "  Processed 660000/700000 Khmer texts\n",
      "  Processed 670000/700000 Khmer texts\n",
      "  Processed 680000/700000 Khmer texts\n",
      "  Processed 690000/700000 Khmer texts\n",
      "  Processed 700000/700000 Khmer texts\n",
      "Tokenizing Khmer texts...\n",
      "Tokenizing 700000 Vietnamese texts...\n",
      "  Processed 10000/700000 Vietnamese texts\n",
      "  Processed 20000/700000 Vietnamese texts\n",
      "  Processed 30000/700000 Vietnamese texts\n",
      "  Processed 40000/700000 Vietnamese texts\n",
      "  Processed 50000/700000 Vietnamese texts\n",
      "  Processed 60000/700000 Vietnamese texts\n",
      "  Processed 70000/700000 Vietnamese texts\n",
      "  Processed 80000/700000 Vietnamese texts\n",
      "  Processed 90000/700000 Vietnamese texts\n",
      "  Processed 100000/700000 Vietnamese texts\n",
      "  Processed 110000/700000 Vietnamese texts\n",
      "  Processed 120000/700000 Vietnamese texts\n",
      "  Processed 130000/700000 Vietnamese texts\n",
      "  Processed 140000/700000 Vietnamese texts\n",
      "  Processed 150000/700000 Vietnamese texts\n",
      "  Processed 160000/700000 Vietnamese texts\n",
      "  Processed 170000/700000 Vietnamese texts\n",
      "  Processed 180000/700000 Vietnamese texts\n",
      "  Processed 190000/700000 Vietnamese texts\n",
      "  Processed 200000/700000 Vietnamese texts\n",
      "  Processed 210000/700000 Vietnamese texts\n",
      "  Processed 220000/700000 Vietnamese texts\n",
      "  Processed 230000/700000 Vietnamese texts\n",
      "  Processed 240000/700000 Vietnamese texts\n",
      "  Processed 250000/700000 Vietnamese texts\n",
      "  Processed 260000/700000 Vietnamese texts\n",
      "  Processed 270000/700000 Vietnamese texts\n",
      "  Processed 280000/700000 Vietnamese texts\n",
      "  Processed 290000/700000 Vietnamese texts\n",
      "  Processed 300000/700000 Vietnamese texts\n",
      "  Processed 310000/700000 Vietnamese texts\n",
      "  Processed 320000/700000 Vietnamese texts\n",
      "  Processed 330000/700000 Vietnamese texts\n",
      "  Processed 340000/700000 Vietnamese texts\n",
      "  Processed 350000/700000 Vietnamese texts\n",
      "  Processed 360000/700000 Vietnamese texts\n",
      "  Processed 370000/700000 Vietnamese texts\n",
      "  Processed 380000/700000 Vietnamese texts\n",
      "  Processed 390000/700000 Vietnamese texts\n",
      "  Processed 400000/700000 Vietnamese texts\n",
      "  Processed 410000/700000 Vietnamese texts\n",
      "  Processed 420000/700000 Vietnamese texts\n",
      "  Processed 430000/700000 Vietnamese texts\n",
      "  Processed 440000/700000 Vietnamese texts\n",
      "  Processed 450000/700000 Vietnamese texts\n",
      "  Processed 460000/700000 Vietnamese texts\n",
      "  Processed 470000/700000 Vietnamese texts\n",
      "  Processed 480000/700000 Vietnamese texts\n",
      "  Processed 490000/700000 Vietnamese texts\n",
      "  Processed 500000/700000 Vietnamese texts\n",
      "  Processed 510000/700000 Vietnamese texts\n",
      "  Processed 520000/700000 Vietnamese texts\n",
      "  Processed 530000/700000 Vietnamese texts\n",
      "  Processed 540000/700000 Vietnamese texts\n",
      "  Processed 550000/700000 Vietnamese texts\n",
      "  Processed 560000/700000 Vietnamese texts\n",
      "  Processed 570000/700000 Vietnamese texts\n",
      "  Processed 580000/700000 Vietnamese texts\n",
      "  Processed 590000/700000 Vietnamese texts\n",
      "  Processed 600000/700000 Vietnamese texts\n",
      "  Processed 610000/700000 Vietnamese texts\n",
      "  Processed 620000/700000 Vietnamese texts\n",
      "  Processed 630000/700000 Vietnamese texts\n",
      "  Processed 640000/700000 Vietnamese texts\n",
      "  Processed 650000/700000 Vietnamese texts\n",
      "  Processed 660000/700000 Vietnamese texts\n",
      "  Processed 670000/700000 Vietnamese texts\n",
      "  Processed 680000/700000 Vietnamese texts\n",
      "  Processed 690000/700000 Vietnamese texts\n",
      "  Processed 700000/700000 Vietnamese texts\n",
      "Total dataset size: 700000 examples\n",
      "Train dataset: 696000 examples (for training)\n",
      "Dev dataset  : 3000 examples (for validation during training)\n",
      "Test dataset : 1000 examples (for final evaluation)\n",
      "\n",
      "Data split and shuffle completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Load and prepare data\n",
    "DATA_DIR = \"dataset\"\n",
    "\n",
    "def load_parallel(src_file, tgt_file):\n",
    "    with open(src_file, encoding=\"utf-8\") as f:\n",
    "        src = [l.strip() for l in f]\n",
    "    with open(tgt_file, encoding=\"utf-8\") as f:\n",
    "        tgt = [l.strip() for l in f]\n",
    "    \n",
    "    assert len(src) == len(tgt)\n",
    "\n",
    "    print(\"Tokenizing Vietnamese texts...\")\n",
    "    src_tokenized = tokenize_batch_khmer(src)\n",
    "    \n",
    "    print(\"Tokenizing Khmer texts...\")\n",
    "    tgt_tokenized = tokenize_batch_vietnamese(tgt) \n",
    "    \n",
    "    return Dataset.from_dict({\n",
    "        \"src_text\": src_tokenized,\n",
    "        \"tgt_text\": tgt_tokenized\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "# Load data from train.khm and train.vi\n",
    "full_dataset = load_parallel(\n",
    "    f\"{DATA_DIR}/train_khmer_to_vi_shuf.khm\",\n",
    "    f\"{DATA_DIR}/train_khmer_to_vi_shuf.vi\"\n",
    ")\n",
    "\n",
    "print(f\"Total dataset size: {len(full_dataset)} examples\")\n",
    "\n",
    "# Split dataset\n",
    "test_size = 1000\n",
    "dev_size = 3000\n",
    "\n",
    "test_start_idx = len(full_dataset) - test_size\n",
    "dev_start_idx = test_start_idx - dev_size\n",
    "\n",
    "test_dataset = full_dataset.select(range(test_start_idx, len(full_dataset)))\n",
    "dev_dataset = full_dataset.select(range(dev_start_idx, test_start_idx))\n",
    "train_dataset = full_dataset.select(range(0, dev_start_idx))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} examples (for training)\")\n",
    "print(f\"Dev dataset  : {len(dev_dataset)} examples (for validation during training)\")\n",
    "print(f\"Test dataset : {len(test_dataset)} examples (for final evaluation)\")\n",
    "print(\"\\nData split and shuffle completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ad652-9554-4e60-b301-128f5cc3feff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "741ba8ab-4a4b-4822-a02c-aac3aa21bad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Preprocessing function\n",
    "MAX_LEN = 256\n",
    "\n",
    "def preprocess(batch):\n",
    "    tokenizer.src_lang = \"km\"  # Changed from \"lo\" to \"km\" for Khmer\n",
    "    tokenizer.tgt_lang = \"vi\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        batch[\"src_text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"tgt_text\"],\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58643cb0-efa6-421f-8df9-f23f8ea5caa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fdffb2191a4d10b60302413f457de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/696000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c476baea9948f78f4b0f72265455c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Apply preprocessing\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=8\n",
    ")\n",
    "\n",
    "dev_dataset = dev_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dev_dataset.column_names,\n",
    "    num_proc=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2b6d598-33d9-4cc7-af89-05809dcc82a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "907ead99-1041-4aab-8877-bfa90ce485d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# ============================================================\n",
    "# CELL 3: Training Arguments - Phase 3\n",
    "# ============================================================\n",
    "training_args_phase3 = TrainingArguments(\n",
    "    output_dir=\"./km_to_vi/phase3\",\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=100,\n",
    "    \n",
    "    # Batch size\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    \n",
    "    # Learning rate - LOWER than phase 2\n",
    "    learning_rate=3e-5,  # Lower for fine-tuning encoder\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    warmup_ratio=0.05,\n",
    "    \n",
    "    # Regularization\n",
    "    weight_decay=0.015,\n",
    "    max_grad_norm=0.8,\n",
    "    \n",
    "    # Precision\n",
    "    num_train_epochs=6,\n",
    "    \n",
    "    # FP16\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "\n",
    "    # Speed\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=8,\n",
    "    dataloader_pin_memory=True,\n",
    "    \n",
    "    # Best model\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "737b8522-e9d3-4b11-b687-f1d4644fa31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15765/2611886879.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_phase3 = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Create Trainer - Phase 3\n",
    "# ============================================================\n",
    "trainer_phase3 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_phase3,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=4)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cb82219-db62-465b-99e6-d8bd9afb19c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 3: Progressive Unfreezing (Top 6 Encoder Layers)\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16314' max='16314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16314/16314 1:22:46, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.844400</td>\n",
       "      <td>0.940557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.969100</td>\n",
       "      <td>0.922660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.974900</td>\n",
       "      <td>0.912176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.901583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.890488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.923000</td>\n",
       "      <td>0.879636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.919600</td>\n",
       "      <td>0.871153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.906600</td>\n",
       "      <td>0.863874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.905300</td>\n",
       "      <td>0.855147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>0.849605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.895100</td>\n",
       "      <td>0.846942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.859300</td>\n",
       "      <td>0.842173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.866100</td>\n",
       "      <td>0.836334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.850800</td>\n",
       "      <td>0.830378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.853700</td>\n",
       "      <td>0.825566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.843900</td>\n",
       "      <td>0.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.817800</td>\n",
       "      <td>0.818551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.820400</td>\n",
       "      <td>0.816260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.822400</td>\n",
       "      <td>0.814126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.817600</td>\n",
       "      <td>0.809016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.821300</td>\n",
       "      <td>0.806276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.795700</td>\n",
       "      <td>0.803491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.800700</td>\n",
       "      <td>0.803120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.803100</td>\n",
       "      <td>0.802018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.798903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.799700</td>\n",
       "      <td>0.800528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.797200</td>\n",
       "      <td>0.798126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.783100</td>\n",
       "      <td>0.797690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.788600</td>\n",
       "      <td>0.796827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.789100</td>\n",
       "      <td>0.796913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.797920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.785900</td>\n",
       "      <td>0.798066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16314, training_loss=0.8554002293713817, metrics={'train_runtime': 4968.9164, 'train_samples_per_second': 840.425, 'train_steps_per_second': 3.283, 'total_flos': 7.269915377805558e+17, 'train_loss': 0.8554002293713817, 'epoch': 6.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Train Phase 3\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 3: Progressive Unfreezing (Top 6 Encoder Layers)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "trainer_phase3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6e5fa3b-b88d-4a08-b9ec-7e5f480027fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 3 model saved!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Save Phase 3 Model\n",
    "# ============================================================\n",
    "trainer_phase3.save_model(\"./km_to_vi/phase3/best\")\n",
    "tokenizer.save_pretrained(\"./km_to_vi/phase3/best\")\n",
    "\n",
    "print(\"\\nPhase 3 model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4c57565-2698-48c0-8285-04f2745f1bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set size: 1000 examples\n",
      "\n",
      "Translating test set...\n",
      "Translated 320/1000\n",
      "Translated 640/1000\n",
      "Translated 960/1000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Evaluate Phase 3\n",
    "# ============================================================\n",
    "\n",
    "def translate_batch(texts, model, tokenizer, batch_size=32):\n",
    "    \"\"\"Batch translation for speed\"\"\"\n",
    "    model.eval()\n",
    "    tokenizer.src_lang = \"km\"\n",
    "    tokenizer.tgt_lang = \"vi\"\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=tokenizer.get_lang_id(\"vi\"),\n",
    "                num_beams=5,\n",
    "                max_length=256\n",
    "            )\n",
    "        \n",
    "        texts_out = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "        outputs.extend(texts_out)\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"Translated {i+len(batch)}/{len(texts)}\")\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Lấy dữ liệu test từ test_dataset (đã chia từ train.vi/train.lo)\n",
    "test_khm = test_dataset[\"src_text\"]\n",
    "test_vi = test_dataset[\"tgt_text\"]\n",
    "\n",
    "print(f\"\\nTest set size: {len(test_vi)} examples\")\n",
    "\n",
    "# Translate\n",
    "print(\"\\nTranslating test set...\")\n",
    "preds_phase3 = translate_batch(test_khm, model, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6d44262-cd8e-4ae5-8248-593d910bb3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 3 BLEU Score: 50.48\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "# Calculate BLEU\n",
    "bleu_phase3 = corpus_bleu(preds_phase3, [test_vi])\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PHASE 3 BLEU Score: {bleu_phase3.score:.2f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Save predictions\n",
    "with open(\"./km_to_vi/phase3/phase3_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(preds_phase3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c35efa81-634a-4f0c-b0a2-cc65b90b58d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAMPLE TRANSLATIONS\n",
      "======================================================================\n",
      "\n",
      "Example 1:\n",
      "Source    : ▁ទោះជាយ៉ាងណា ក៏ដោយ ▁ ក្នុងអំឡុងពេល ប្រតិបត្តិការ របស់ក្រុមហ៊ុន ▁T i C o ▁លោក ▁Th u an ▁គឺជា អ្នក ទទួលបន្ទុក ដោយផ្ទាល់ លើ ប្រតិបត្តិការ ទាំងអស់ ▁និង ទទួលខុសត្រូវ ទាំងស្រុង ។\n",
      "Reference : Tuy_nhiên , trong quá_trình Công_ty TiCo hoạt_động , Thuận là người trực_tiếp điều_hành mọi công_việc và chịu trách_nhiệm .\n",
      "Prediction: Tuy_nhiên , trong quá_trình hoạt_động của Tập_đoàn TiCo , ông Thuận là người trực_tiếp phụ_trách mọi hoạt_động và hoàn_toàn chịu trách_nhiệm .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Source    : ▁ ថ្នាំ ▁M e th ad on e ▁ ជួយ បន្ថយ រោគសញ្ញា នៃការ ដក ថ្នាំ ▁ដោយ កាត់បន្ថយ ការ ឃ្លា ន ▁ការ ប្រកួតប្រជែង ▁និង ផលប៉ះពាល់ នៃ ថ្នាំ ហេ រ៉ូ អ៊ីន យ៉ាងច្រើន ។\n",
      "Reference : Methadone có tác_dụng làm mất các biểu_hiện của hội_chứng_cai , giảm đáng_kể thèm nhớ , cạnh_tranh và khóa tác_động của heroin .\n",
      "Prediction: Methadone giúp làm dịu các triệu_chứng của việc loại_bỏ thuốc bằng cách giảm đáng_kể cơn đói , sự cạnh_tranh và tác_dụng phụ của heroin .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Source    : ▁ ទ ួ រ ប៊ី ន ចំនួន ប្រាំ មួយ មកពី ក្រុមហ៊ុន ឧស្សាហកម្ម អាល្លឺម៉ង់ ▁S i e m en s ▁ត្រូវបាន នាំយក មក ទីក្រុង ម៉ុង រ៉េ អាល់ សម្រាប់ការ ថែទាំ ▁នៅពេលដែល ប្រទេស កាណាដា បានប្រកាស ពី ទ ណ្ឌ កម្ម ប្រឆាំងនឹង ប្រទេស រុស្ស៊ី ដោយសារតែ ជម្លោះ នៅ អ៊ុ យ ក្រ ែន ។\n",
      "Reference : Có 6 tuabin của Tập_đoàn công_nghiệp Đức_Siemens đã được đưa đến Montreal để bảo_trì khi Canada tuyên_bố cấm_vận Nga do xung_đột ở Ukraine .\n",
      "Prediction: Sáu tuabin của công_ty công_nghiệp Siemens của Đức được đưa đến Montreal để bảo_trì khi Canada công_bố các lệnh trừng_phạt Nga do xung_đột ở Ukraine .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "Source    : ▁ ថ្មីៗនេះ ▁ ខោ <0xE2> <0x80> <0x8B> ដែល <0xE2> <0x80> <0x8B> មាន <0xE2> <0x80> <0x8B> រាង <0xE2> <0x80> <0x8B> ប៉ ោង <0xE2> <0x80> <0x8B> បាន <0xE2> <0x80> <0x8B> ចាប់ផ្តើម <0xE2> <0x80> <0x8B> វិល <0xE2> <0x80> <0x8B> មក <0xE2> <0x80> <0x8B> រក <0xE2> <0x80> <0x8B> ភាព <0xE2> <0x80> <0x8B> ពេញ <0xE2> <0x80> <0x8B> និយម <0xE2> <0x80> <0x8B> វិញ ▁ហើយ <0xE2> <0x80> <0x8B> កាន់តែ <0xE2> <0x80> <0x8B> មាន <0xE2> <0x80> <0x8B> ប្រជាប្រិយ ភាព <0xE2> <0x80> <0x8B> នៅ <0xE2> <0x80> <0x8B> ក្នុង <0xE2> <0x80> <0x8B> ពិភព <0xE2> <0x80> <0x8B> ម៉ូ ដ ។\n",
      "Reference : Thời_gian gần đây , quần ông loe bắt_đầu quay trở_lại và trở_nên phổ_biến hơn trong giới thời_trang .\n",
      "Prediction: Thời_gian gần đây , quần ống_xả đã bắt_đầu trở_lại trạng_thái_cực_kỳ_vị_thế và trở_nên nổi_bật hơn .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "Source    : ▁បន្ទាប់ពី ការប្រកួត ស្មើ ▁ ០ - ០ ▁ ជាមួយ ក្រុម ▁ U 2 3 ▁ ហ្វីលីពីន ▁អ្នក យុទ្ធសាស្ត្រ កូរ៉េខាងត្បូង បានដាក់ ការ សង្កត់ធ្ងន់ ជាពិសេស ទៅលើ សមត្ថភាព ស៊ុ ត បញ្ចូល ទី របស់ ខ្សែ ប្រយុទ្ធ ។\n",
      "Reference : Sau trận hòa 0 - 0 với U23_Philippines , chiến_lược gia người Hàn_Quốc đặc_biệt chú_trọng đến khả_năng ghi_bàn của các tiền_đạo .\n",
      "Prediction: Sau trận hòa không bàn thắng trước U23_Philippines , chiến_lược gia người Hàn_Quốc đặc_biệt chú_trọng đến khả_năng ghi_bàn của các tiền_đạo .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 6:\n",
      "Source    : ▁ តម្រូវការ ជា មុន មួយ សម្រាប់ គម្រោង ស មាស ភាគ ទាំង បួន នៃ ផ្លូវ ល្បឿនលឿន ជើង - ត្បូង ▁( ផ្នែក ខាងកើត ) ▁ ដែលត្រូវ បញ្ចប់ នៅចុង ឆ្នាំ ▁២០ ២២ ▁គឺ ការ ជំនួស ផ្ទះ យ៉ាង ឆាប់ រហ័ស ។\n",
      "Reference : Một trong những yếu_tố tiên_quyết để 4 dự_án thành_phần cao_tốc Bắc - Nam phía Đông_kịp hoàn_thành vào cuối năm 2022 là phải thay_thế nhanh các nhà .\n",
      "Prediction: Một trong những điều_kiện tiên_quyết để 4 dự_án thành_phần cao_tốc Bắc - Nam phía Đông hoàn_thành vào cuối năm 2022 là nhanh_chóng thay_thế nhà ở .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 7:\n",
      "Source    : ▁ អ្នកស្រាវជ្រាវ បានរកឃើញ ថា ▁ការ ទុក កង ្ ហា រ ឱ្យ បើក យូរ ពេក អាច នាំឱ្យ មានការ ខ្ សោះ ជាតិ ទឹក ▁ មាត់ ▁និង ច្រមុះ ស្ងួត ▁ដោយសារតែ ញ ើ ស ▁និង សំណើ ម ហួត ចេញពី រាងកាយ ។\n",
      "Reference : Các nhà nghiên_cứu đã phát_hiện ra rằng việc bật quạt quá lâu có_thể dẫn đến mất nước và làm khô miệng , mũi do mồ_hôi và hơi ẩm bốc_hơi từ cơ_thể .\n",
      "Prediction: Các nhà nghiên_cứu phát_hiện ra rằng để quạt mở quá lâu có_thể dẫn đến mất nước_bọt và khô mũi do mồ_hôi và độ_ẩm bốc ra khỏi cơ_thể .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 8:\n",
      "Source    : ▁ការ សម្រេចចិត្ត របស់ វៀតណាម ក្នុងការ នាំ ការប្រកួត ត្រឡប់ទៅ រក លទ្ធផល ស្មើ វិញ បាន ធ្វើឱ្យ ការប្រកួត កាន់តែ គួរឱ្យ រំភើប ▁និង តា ន តឹង នៅ នាទី បន្ត បន្ទាប់ ។\n",
      "Reference : Việc đưa trận_đấu trở về vạch xuất_phát của tuyển Việt_Nam đã giúp cho trận_đấu càng trở_nên hấp_dẫn và quyết_liệt hơn ở những phút sau đó .\n",
      "Prediction: Việc Việt_Nam đưa trận_đấu trở_lại với tỷ_số hòa khiến trận_đấu trở_nên hấp_dẫn và căng_thẳng hơn trong những phút tiếp_theo .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 9:\n",
      "Source    : ▁សូម ឱ្យ យើង អម ដំណើរ អ្នក ក្នុង ដំណើរ កម្សាន្ត ហា យ៉ាង ដែល មានតម្លៃ សមរម្យ របស់អ្នក ពី ទីក្រុង ហូជីមិញ ▁ហើយ ជួយ អ្នក ឱ្យ ទទួលបាន បទពិសោធន៍ ដ៏អស្ចារ្យ !\n",
      "Reference : Hãy để chúng_tôi đồng_hành cùng bạn trong hành_trình tour du_lịch Hà_Giang từ TP HCM giá tốt và thu về những trải nghiệm tuyệt_vời nhé !\n",
      "Prediction: Hãy cùng chúng_tôi đi cùng bạn trong chuyến du_lịch Hà_Giang giá rẻ từ TPHCM và giúp bạn có những trải nghiệm tuyệt_vời nhé !\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 10:\n",
      "Source    : ▁ ថ្មីៗនេះ ▁ រដ្ឋបាល លោក ▁B id en ▁បាន ព្យាយាម ដាក់ សម្ពាធ លើ ប្រទេស ហូ ឡង់ ឱ្យ ស្វែងរក មធ្យោបាយ ដើម្បី រារាំង ▁A S M L ▁ដែលជា ក្រុមហ៊ុន ផលិត បន្ ទះ ឈ ី ប ធំជាងគេ បំផុត របស់ ពិភពលោក ▁ ពីការ កាត់បន្ថយ កិច្ច សហប្រតិបត្តិការ របស់ខ្លួន ជាមួយ ក្រុមហ៊ុន ចិន ។\n",
      "Reference : Gần đây , chính_quyền Biden tìm cách gây áp_lực với Hà_Lan , yêu_cầu tìm cách can_ngăn ASML , nhà cung_cấp sản_xuất chip lớn nhất thế_giới , giảm bớt sự hợp_tác với các doanh_nghiệp Trung_Quốc .\n",
      "Prediction: Chính_quyền Tổng_thống Biden gần đây đã cố_gắng gây áp_lực để Hà_Lan tìm cách ngăn_cản ASML , nhà sản_xuất chip lớn nhất thế_giới , cắt_giảm hợp_tác với các công_ty Trung_Quốc .\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 15: Sample Translations\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE TRANSLATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Source    : {test_khm[i]}\")\n",
    "    print(f\"Reference : {test_vi[i]}\")\n",
    "    print(f\"Prediction: {preds_phase3[i]}\")\n",
    "    print(\"-\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb6eae-33de-4e55-89d1-2715d9d0e1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
