{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45656f5e-51f4-4995-879d-cbb4d8ae0db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.9.1)\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.57.3)\n",
      "Requirement already satisfied: sentencepiece in ./.local/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.10/site-packages (4.4.2)\n",
      "Requirement already satisfied: sacrebleu in ./.local/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.10/site-packages (1.12.0)\n",
      "Requirement already satisfied: pyvi in ./.local/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.local/lib/python3.10/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.local/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.local/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.local/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.local/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.local/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.local/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.local/lib/python3.10/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.local/lib/python3.10/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.local/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.local/lib/python3.10/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.local/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.local/lib/python3.10/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: portalocker in ./.local/lib/python3.10/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in ./.local/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in ./.local/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in ./.local/lib/python3.10/site-packages (from sacrebleu) (6.0.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.10/site-packages (from pyvi) (1.7.2)\n",
      "Requirement already satisfied: sklearn-crfsuite in ./.local/lib/python3.10/site-packages (from pyvi) (0.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.local/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.local/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.local/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.local/lib/python3.10/site-packages (from scikit-learn->pyvi) (3.6.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.7 in ./.local/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (0.9.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers sentencepiece datasets sacrebleu accelerate pyvi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b20a937-2520-4d69-b219-5e18e82b840b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA H200\n",
      "VRAM: 150.0217344 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup and imports\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    M2M100ForConditionalGeneration,\n",
    "    M2M100Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import random\n",
    "\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(\"VRAM:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f54bc32a-965b-4cb8-9a7a-c60bb3307e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khmer tokenizer loaded successfully!\n",
      "Vietnamese and Khmer tokenizers loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Khmer tokenizer\n",
    "# Load Khmer tokenizer from Hugging Face\n",
    "khmer_word_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"khopilot/km-tokenizer-khmer\", \n",
    "    use_fast=False\n",
    ")\n",
    "print(\"Khmer tokenizer loaded successfully!\")\n",
    "\n",
    "# Cell 3b: Define tokenization functions\n",
    "def tokenize_vietnamese(text):\n",
    "    \"\"\"Tokenize Vietnamese text using PyVi\"\"\"\n",
    "    try:\n",
    "        return ViTokenizer.tokenize(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing Vietnamese: {e}\")\n",
    "        return text\n",
    "\n",
    "def tokenize_khmer(text):\n",
    "    \"\"\"Tokenize Khmer text using khopilot/km-tokenizer-khmer\"\"\"\n",
    "    try:\n",
    "        tokens = khmer_word_tokenizer.tokenize(text)\n",
    "        return \" \".join(tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing Khmer: {e}\")\n",
    "        return text\n",
    "\n",
    "def tokenize_batch_vietnamese(texts):\n",
    "    \"\"\"Batch tokenize Vietnamese texts\"\"\"\n",
    "    print(f\"Tokenizing {len(texts)} Vietnamese texts...\")\n",
    "    results = []\n",
    "    for i, text in enumerate(texts):\n",
    "        results.append(tokenize_vietnamese(text))\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(texts)} Vietnamese texts\")\n",
    "    return results\n",
    "\n",
    "def tokenize_batch_khmer(texts):\n",
    "    \"\"\"Batch tokenize Khmer texts\"\"\"\n",
    "    print(f\"Tokenizing {len(texts)} Khmer texts...\")\n",
    "    results = []\n",
    "    for i, text in enumerate(texts):\n",
    "        results.append(tokenize_khmer(text))\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(texts)} Khmer texts\")\n",
    "    return results\n",
    "\n",
    "print(\"Vietnamese and Khmer tokenizers loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21fbba61-05a1-4bd5-b57a-9d7838cd9ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 model loaded!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"km_to_vi/phase1/best\"\n",
    ").cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"km_to_vi/phase1/best\"\n",
    ")\n",
    "\n",
    "print(\"Phase 1 model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5ed2ec5-3340-4fa2-949c-388da06bb0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params    : 483.9M\n",
      "Frozen params   : 282.3M\n",
      "Trainable params: 201.6M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for param in model.model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Verify\n",
    "total, frozen, trainable = 0, 0, 0\n",
    "for name, param in model.named_parameters():\n",
    "    n = param.numel()\n",
    "    total += n\n",
    "    if not param.requires_grad:\n",
    "        frozen += n\n",
    "    else:\n",
    "        trainable += n\n",
    "\n",
    "print(f\"Total params    : {total/1e6:.1f}M\")\n",
    "print(f\"Frozen params   : {frozen/1e6:.1f}M\")\n",
    "print(f\"Trainable params: {trainable/1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "669a18ba-6f24-4200-b04b-8479ab2aecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Vietnamese texts...\n",
      "Tokenizing 700000 Khmer texts...\n",
      "  Processed 10000/700000 Khmer texts\n",
      "  Processed 20000/700000 Khmer texts\n",
      "  Processed 30000/700000 Khmer texts\n",
      "  Processed 40000/700000 Khmer texts\n",
      "  Processed 50000/700000 Khmer texts\n",
      "  Processed 60000/700000 Khmer texts\n",
      "  Processed 70000/700000 Khmer texts\n",
      "  Processed 80000/700000 Khmer texts\n",
      "  Processed 90000/700000 Khmer texts\n",
      "  Processed 100000/700000 Khmer texts\n",
      "  Processed 110000/700000 Khmer texts\n",
      "  Processed 120000/700000 Khmer texts\n",
      "  Processed 130000/700000 Khmer texts\n",
      "  Processed 140000/700000 Khmer texts\n",
      "  Processed 150000/700000 Khmer texts\n",
      "  Processed 160000/700000 Khmer texts\n",
      "  Processed 170000/700000 Khmer texts\n",
      "  Processed 180000/700000 Khmer texts\n",
      "  Processed 190000/700000 Khmer texts\n",
      "  Processed 200000/700000 Khmer texts\n",
      "  Processed 210000/700000 Khmer texts\n",
      "  Processed 220000/700000 Khmer texts\n",
      "  Processed 230000/700000 Khmer texts\n",
      "  Processed 240000/700000 Khmer texts\n",
      "  Processed 250000/700000 Khmer texts\n",
      "  Processed 260000/700000 Khmer texts\n",
      "  Processed 270000/700000 Khmer texts\n",
      "  Processed 280000/700000 Khmer texts\n",
      "  Processed 290000/700000 Khmer texts\n",
      "  Processed 300000/700000 Khmer texts\n",
      "  Processed 310000/700000 Khmer texts\n",
      "  Processed 320000/700000 Khmer texts\n",
      "  Processed 330000/700000 Khmer texts\n",
      "  Processed 340000/700000 Khmer texts\n",
      "  Processed 350000/700000 Khmer texts\n",
      "  Processed 360000/700000 Khmer texts\n",
      "  Processed 370000/700000 Khmer texts\n",
      "  Processed 380000/700000 Khmer texts\n",
      "  Processed 390000/700000 Khmer texts\n",
      "  Processed 400000/700000 Khmer texts\n",
      "  Processed 410000/700000 Khmer texts\n",
      "  Processed 420000/700000 Khmer texts\n",
      "  Processed 430000/700000 Khmer texts\n",
      "  Processed 440000/700000 Khmer texts\n",
      "  Processed 450000/700000 Khmer texts\n",
      "  Processed 460000/700000 Khmer texts\n",
      "  Processed 470000/700000 Khmer texts\n",
      "  Processed 480000/700000 Khmer texts\n",
      "  Processed 490000/700000 Khmer texts\n",
      "  Processed 500000/700000 Khmer texts\n",
      "  Processed 510000/700000 Khmer texts\n",
      "  Processed 520000/700000 Khmer texts\n",
      "  Processed 530000/700000 Khmer texts\n",
      "  Processed 540000/700000 Khmer texts\n",
      "  Processed 550000/700000 Khmer texts\n",
      "  Processed 560000/700000 Khmer texts\n",
      "  Processed 570000/700000 Khmer texts\n",
      "  Processed 580000/700000 Khmer texts\n",
      "  Processed 590000/700000 Khmer texts\n",
      "  Processed 600000/700000 Khmer texts\n",
      "  Processed 610000/700000 Khmer texts\n",
      "  Processed 620000/700000 Khmer texts\n",
      "  Processed 630000/700000 Khmer texts\n",
      "  Processed 640000/700000 Khmer texts\n",
      "  Processed 650000/700000 Khmer texts\n",
      "  Processed 660000/700000 Khmer texts\n",
      "  Processed 670000/700000 Khmer texts\n",
      "  Processed 680000/700000 Khmer texts\n",
      "  Processed 690000/700000 Khmer texts\n",
      "  Processed 700000/700000 Khmer texts\n",
      "Tokenizing Khmer texts...\n",
      "Tokenizing 700000 Vietnamese texts...\n",
      "  Processed 10000/700000 Vietnamese texts\n",
      "  Processed 20000/700000 Vietnamese texts\n",
      "  Processed 30000/700000 Vietnamese texts\n",
      "  Processed 40000/700000 Vietnamese texts\n",
      "  Processed 50000/700000 Vietnamese texts\n",
      "  Processed 60000/700000 Vietnamese texts\n",
      "  Processed 70000/700000 Vietnamese texts\n",
      "  Processed 80000/700000 Vietnamese texts\n",
      "  Processed 90000/700000 Vietnamese texts\n",
      "  Processed 100000/700000 Vietnamese texts\n",
      "  Processed 110000/700000 Vietnamese texts\n",
      "  Processed 120000/700000 Vietnamese texts\n",
      "  Processed 130000/700000 Vietnamese texts\n",
      "  Processed 140000/700000 Vietnamese texts\n",
      "  Processed 150000/700000 Vietnamese texts\n",
      "  Processed 160000/700000 Vietnamese texts\n",
      "  Processed 170000/700000 Vietnamese texts\n",
      "  Processed 180000/700000 Vietnamese texts\n",
      "  Processed 190000/700000 Vietnamese texts\n",
      "  Processed 200000/700000 Vietnamese texts\n",
      "  Processed 210000/700000 Vietnamese texts\n",
      "  Processed 220000/700000 Vietnamese texts\n",
      "  Processed 230000/700000 Vietnamese texts\n",
      "  Processed 240000/700000 Vietnamese texts\n",
      "  Processed 250000/700000 Vietnamese texts\n",
      "  Processed 260000/700000 Vietnamese texts\n",
      "  Processed 270000/700000 Vietnamese texts\n",
      "  Processed 280000/700000 Vietnamese texts\n",
      "  Processed 290000/700000 Vietnamese texts\n",
      "  Processed 300000/700000 Vietnamese texts\n",
      "  Processed 310000/700000 Vietnamese texts\n",
      "  Processed 320000/700000 Vietnamese texts\n",
      "  Processed 330000/700000 Vietnamese texts\n",
      "  Processed 340000/700000 Vietnamese texts\n",
      "  Processed 350000/700000 Vietnamese texts\n",
      "  Processed 360000/700000 Vietnamese texts\n",
      "  Processed 370000/700000 Vietnamese texts\n",
      "  Processed 380000/700000 Vietnamese texts\n",
      "  Processed 390000/700000 Vietnamese texts\n",
      "  Processed 400000/700000 Vietnamese texts\n",
      "  Processed 410000/700000 Vietnamese texts\n",
      "  Processed 420000/700000 Vietnamese texts\n",
      "  Processed 430000/700000 Vietnamese texts\n",
      "  Processed 440000/700000 Vietnamese texts\n",
      "  Processed 450000/700000 Vietnamese texts\n",
      "  Processed 460000/700000 Vietnamese texts\n",
      "  Processed 470000/700000 Vietnamese texts\n",
      "  Processed 480000/700000 Vietnamese texts\n",
      "  Processed 490000/700000 Vietnamese texts\n",
      "  Processed 500000/700000 Vietnamese texts\n",
      "  Processed 510000/700000 Vietnamese texts\n",
      "  Processed 520000/700000 Vietnamese texts\n",
      "  Processed 530000/700000 Vietnamese texts\n",
      "  Processed 540000/700000 Vietnamese texts\n",
      "  Processed 550000/700000 Vietnamese texts\n",
      "  Processed 560000/700000 Vietnamese texts\n",
      "  Processed 570000/700000 Vietnamese texts\n",
      "  Processed 580000/700000 Vietnamese texts\n",
      "  Processed 590000/700000 Vietnamese texts\n",
      "  Processed 600000/700000 Vietnamese texts\n",
      "  Processed 610000/700000 Vietnamese texts\n",
      "  Processed 620000/700000 Vietnamese texts\n",
      "  Processed 630000/700000 Vietnamese texts\n",
      "  Processed 640000/700000 Vietnamese texts\n",
      "  Processed 650000/700000 Vietnamese texts\n",
      "  Processed 660000/700000 Vietnamese texts\n",
      "  Processed 670000/700000 Vietnamese texts\n",
      "  Processed 680000/700000 Vietnamese texts\n",
      "  Processed 690000/700000 Vietnamese texts\n",
      "  Processed 700000/700000 Vietnamese texts\n",
      "Total dataset size: 700000 examples\n",
      "Train dataset: 696000 examples (for training)\n",
      "Dev dataset  : 3000 examples (for validation during training)\n",
      "Test dataset : 1000 examples (for final evaluation)\n",
      "\n",
      "Data split and shuffle completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Load and prepare data\n",
    "DATA_DIR = \"dataset\"\n",
    "\n",
    "def load_parallel(src_file, tgt_file):\n",
    "    with open(src_file, encoding=\"utf-8\") as f:\n",
    "        src = [l.strip() for l in f]\n",
    "    with open(tgt_file, encoding=\"utf-8\") as f:\n",
    "        tgt = [l.strip() for l in f]\n",
    "    \n",
    "    assert len(src) == len(tgt)\n",
    "\n",
    "    print(\"Tokenizing Vietnamese texts...\")\n",
    "    src_tokenized = tokenize_batch_khmer(src)\n",
    "    \n",
    "    print(\"Tokenizing Khmer texts...\")\n",
    "    tgt_tokenized = tokenize_batch_vietnamese(tgt) \n",
    "    \n",
    "    return Dataset.from_dict({\n",
    "        \"src_text\": src_tokenized,\n",
    "        \"tgt_text\": tgt_tokenized\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "# Load data from train.khm and train.vi\n",
    "full_dataset = load_parallel(\n",
    "    f\"{DATA_DIR}/train_khmer_to_vi_shuf.khm\",\n",
    "    f\"{DATA_DIR}/train_khmer_to_vi_shuf.vi\"\n",
    ")\n",
    "\n",
    "print(f\"Total dataset size: {len(full_dataset)} examples\")\n",
    "\n",
    "# Split dataset\n",
    "test_size = 1000\n",
    "dev_size = 3000\n",
    "\n",
    "test_start_idx = len(full_dataset) - test_size\n",
    "dev_start_idx = test_start_idx - dev_size\n",
    "\n",
    "test_dataset = full_dataset.select(range(test_start_idx, len(full_dataset)))\n",
    "dev_dataset = full_dataset.select(range(dev_start_idx, test_start_idx))\n",
    "train_dataset = full_dataset.select(range(0, dev_start_idx))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} examples (for training)\")\n",
    "print(f\"Dev dataset  : {len(dev_dataset)} examples (for validation during training)\")\n",
    "print(f\"Test dataset : {len(test_dataset)} examples (for final evaluation)\")\n",
    "print(\"\\nData split and shuffle completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f25cea6d-9112-470c-b670-b95a2decef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Preprocessing function\n",
    "MAX_LEN = 256\n",
    "\n",
    "def preprocess(batch):\n",
    "    tokenizer.src_lang = \"km\"  # Changed from \"lo\" to \"km\" for Khmer\n",
    "    tokenizer.tgt_lang = \"vi\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        batch[\"src_text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"tgt_text\"],\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e359669-5a43-41c8-bd08-ddf3e0fa302a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe58d01340ab46e3ac59b7d740c6700c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/696000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f61f34ffe848e2be060aca6aefe0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Apply preprocessing\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=8\n",
    ")\n",
    "\n",
    "dev_dataset = dev_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dev_dataset.column_names,\n",
    "    num_proc=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af19f5c0-f5f5-4f92-b198-43e4437e1435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10a25016-366a-4367-b572-6817faf76108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Training arguments\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "training_args_phase2 = TrainingArguments(\n",
    "    output_dir=\"./km_to_vi/phase2\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.05,\n",
    "    weight_decay=0.005,\n",
    "    max_grad_norm=1.0,\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=8,\n",
    "    dataloader_pin_memory=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c35071e-3bf6-4cd8-a6e0-146ef1eba4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11168/3872925220.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_phase2 = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Create trainer\n",
    "trainer_phase2 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_phase2,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49ded4e1-2b42-406c-86c9-5bb70500ba4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 1: Training Vietnamese - Khmer model\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2719' max='2719' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2719/2719 08:28, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>0.981631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.952400</td>\n",
       "      <td>0.961576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.952500</td>\n",
       "      <td>0.945875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.960300</td>\n",
       "      <td>0.931303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.989300</td>\n",
       "      <td>0.917607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2719, training_loss=0.9798073651116312, metrics={'train_runtime': 509.9908, 'train_samples_per_second': 1364.731, 'train_steps_per_second': 5.331, 'total_flos': 1.2114846123609293e+17, 'train_loss': 0.9798073651116312, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 12: Train\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 1: Training Vietnamese - Khmer model\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "trainer_phase2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3d3abf9-ea15-4b46-8529-80336ed10fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 2 model saved!\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Save model\n",
    "trainer_phase2.save_model(\"./km_to_vi/phase2/best\")\n",
    "tokenizer.save_pretrained(\"./km_to_vi/phase2/best\")\n",
    "print(\"\\nPhase 2 model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df31b7e2-32c5-4f1a-ad5b-aef09eb69b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set size: 1000 examples\n",
      "\n",
      "Translating test set...\n",
      "Translated 320/1000\n",
      "Translated 640/1000\n",
      "Translated 960/1000\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Evaluation functions\n",
    "def translate_batch(texts, model, tokenizer, batch_size=32):\n",
    "    \"\"\"Batch translation for speed\"\"\"\n",
    "    model.eval()\n",
    "    tokenizer.src_lang = \"km\"  # Changed from \"lo\" to \"km\"\n",
    "    tokenizer.tgt_lang = \"vi\"\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=tokenizer.get_lang_id(\"vi\"),\n",
    "                num_beams=5,\n",
    "                max_length=256\n",
    "            )\n",
    "        \n",
    "        texts_out = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "        outputs.extend(texts_out)\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"Translated {i+len(batch)}/{len(texts)}\")\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Get test data\n",
    "test_vi = test_dataset[\"tgt_text\"]\n",
    "test_khm = test_dataset[\"src_text\"]\n",
    "\n",
    "print(f\"\\nTest set size: {len(test_khm)} examples\")\n",
    "print(\"\\nTranslating test set...\")\n",
    "preds_phase2 = translate_batch(test_khm, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35787e10-8ee6-4525-b42d-0e33bcf3312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 1 BLEU Score: 46.74\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Calculate BLEU score\n",
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "bleu_phase2 = corpus_bleu(preds_phase2, [test_vi])\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PHASE 1 BLEU Score: {bleu_phase2.score:.2f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Save predictions\n",
    "with open(\"./km_to_vi/phase2/phase2_predictions_km_vi.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(preds_phase2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e66161f-fefd-438b-9949-efbba442c9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAMPLE TRANSLATIONS\n",
      "======================================================================\n",
      "\n",
      "Example 1:\n",
      "Source (Khmer): ▁ទោះជាយ៉ាងណា ក៏ដោយ ▁ ក្នុងអំឡុងពេល ប្រតិបត្តិការ របស់ក្រុមហ៊ុន ▁T i C o ▁លោក ▁Th u an ▁គឺជា អ្នក ទទួលបន្ទុក ដោយផ្ទាល់ លើ ប្រតិបត្តិការ ទាំងអស់ ▁និង ទទួលខុសត្រូវ ទាំងស្រុង ។\n",
      "Reference (Vietnamese): Tuy_nhiên , trong quá_trình Công_ty TiCo hoạt_động , Thuận là người trực_tiếp điều_hành mọi công_việc và chịu trách_nhiệm .\n",
      "Prediction: Tuy_nhiên , trong quá_trình hoạt_động của TiCo , ông Thuận là người trực_tiếp phụ_trách mọi hoạt_động và hoàn_toàn chịu trách_nhiệm .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Source (Khmer): ▁ ថ្នាំ ▁M e th ad on e ▁ ជួយ បន្ថយ រោគសញ្ញា នៃការ ដក ថ្នាំ ▁ដោយ កាត់បន្ថយ ការ ឃ្លា ន ▁ការ ប្រកួតប្រជែង ▁និង ផលប៉ះពាល់ នៃ ថ្នាំ ហេ រ៉ូ អ៊ីន យ៉ាងច្រើន ។\n",
      "Reference (Vietnamese): Methadone có tác_dụng làm mất các biểu_hiện của hội_chứng_cai , giảm đáng_kể thèm nhớ , cạnh_tranh và khóa tác_động của heroin .\n",
      "Prediction: Methadone giúp giảm bớt các triệu_chứng của việc bỏ thuốc bằng cách giảm đáng_kể cơn đói , sự cạnh_tranh và tác_dụng của heroin .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Source (Khmer): ▁ ទ ួ រ ប៊ី ន ចំនួន ប្រាំ មួយ មកពី ក្រុមហ៊ុន ឧស្សាហកម្ម អាល្លឺម៉ង់ ▁S i e m en s ▁ត្រូវបាន នាំយក មក ទីក្រុង ម៉ុង រ៉េ អាល់ សម្រាប់ការ ថែទាំ ▁នៅពេលដែល ប្រទេស កាណាដា បានប្រកាស ពី ទ ណ្ឌ កម្ម ប្រឆាំងនឹង ប្រទេស រុស្ស៊ី ដោយសារតែ ជម្លោះ នៅ អ៊ុ យ ក្រ ែន ។\n",
      "Reference (Vietnamese): Có 6 tuabin của Tập_đoàn công_nghiệp Đức_Siemens đã được đưa đến Montreal để bảo_trì khi Canada tuyên_bố cấm_vận Nga do xung_đột ở Ukraine .\n",
      "Prediction: Sáu tuabin của công_ty công_nghiệp Siemens của Đức đã được đưa đến Montreal để bảo_dưỡng khi Canada tuyên_bố các lệnh trừng_phạt Nga do xung_đột ở Ukraine .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "Source (Khmer): ▁ ថ្មីៗនេះ ▁ ខោ <0xE2> <0x80> <0x8B> ដែល <0xE2> <0x80> <0x8B> មាន <0xE2> <0x80> <0x8B> រាង <0xE2> <0x80> <0x8B> ប៉ ោង <0xE2> <0x80> <0x8B> បាន <0xE2> <0x80> <0x8B> ចាប់ផ្តើម <0xE2> <0x80> <0x8B> វិល <0xE2> <0x80> <0x8B> មក <0xE2> <0x80> <0x8B> រក <0xE2> <0x80> <0x8B> ភាព <0xE2> <0x80> <0x8B> ពេញ <0xE2> <0x80> <0x8B> និយម <0xE2> <0x80> <0x8B> វិញ ▁ហើយ <0xE2> <0x80> <0x8B> កាន់តែ <0xE2> <0x80> <0x8B> មាន <0xE2> <0x80> <0x8B> ប្រជាប្រិយ ភាព <0xE2> <0x80> <0x8B> នៅ <0xE2> <0x80> <0x8B> ក្នុង <0xE2> <0x80> <0x8B> ពិភព <0xE2> <0x80> <0x8B> ម៉ូ ដ ។\n",
      "Reference (Vietnamese): Thời_gian gần đây , quần ông loe bắt_đầu quay trở_lại và trở_nên phổ_biến hơn trong giới thời_trang .\n",
      "Prediction: Thời_gian gần đây , quần búp_bê đã bắt_đầu trở_lại độ trọn_vẹn và trở_lại với phong_cách cao_cấp .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "Source (Khmer): ▁បន្ទាប់ពី ការប្រកួត ស្មើ ▁ ០ - ០ ▁ ជាមួយ ក្រុម ▁ U 2 3 ▁ ហ្វីលីពីន ▁អ្នក យុទ្ធសាស្ត្រ កូរ៉េខាងត្បូង បានដាក់ ការ សង្កត់ធ្ងន់ ជាពិសេស ទៅលើ សមត្ថភាព ស៊ុ ត បញ្ចូល ទី របស់ ខ្សែ ប្រយុទ្ធ ។\n",
      "Reference (Vietnamese): Sau trận hòa 0 - 0 với U23_Philippines , chiến_lược gia người Hàn_Quốc đặc_biệt chú_trọng đến khả_năng ghi_bàn của các tiền_đạo .\n",
      "Prediction: Sau trận hòa U23_Philippines với tỷ_số 0 - 0 , chiến_lược gia người Hàn_Quốc đặc_biệt chú_trọng đến khả_năng ghi_bàn của tiền_đạo .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 6:\n",
      "Source (Khmer): ▁ តម្រូវការ ជា មុន មួយ សម្រាប់ គម្រោង ស មាស ភាគ ទាំង បួន នៃ ផ្លូវ ល្បឿនលឿន ជើង - ត្បូង ▁( ផ្នែក ខាងកើត ) ▁ ដែលត្រូវ បញ្ចប់ នៅចុង ឆ្នាំ ▁២០ ២២ ▁គឺ ការ ជំនួស ផ្ទះ យ៉ាង ឆាប់ រហ័ស ។\n",
      "Reference (Vietnamese): Một trong những yếu_tố tiên_quyết để 4 dự_án thành_phần cao_tốc Bắc - Nam phía Đông_kịp hoàn_thành vào cuối năm 2022 là phải thay_thế nhanh các nhà .\n",
      "Prediction: Một trong những yêu_cầu tiên_quyết của 4 dự_án thành_phần cao_tốc Bắc - Nam phía Đông hoàn_thành vào cuối năm 2022 là phải nhanh_chóng thay_thế nhà ở .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 7:\n",
      "Source (Khmer): ▁ អ្នកស្រាវជ្រាវ បានរកឃើញ ថា ▁ការ ទុក កង ្ ហា រ ឱ្យ បើក យូរ ពេក អាច នាំឱ្យ មានការ ខ្ សោះ ជាតិ ទឹក ▁ មាត់ ▁និង ច្រមុះ ស្ងួត ▁ដោយសារតែ ញ ើ ស ▁និង សំណើ ម ហួត ចេញពី រាងកាយ ។\n",
      "Reference (Vietnamese): Các nhà nghiên_cứu đã phát_hiện ra rằng việc bật quạt quá lâu có_thể dẫn đến mất nước và làm khô miệng , mũi do mồ_hôi và hơi ẩm bốc_hơi từ cơ_thể .\n",
      "Prediction: Các nhà nghiên_cứu phát_hiện ra rằng để quạt mở quá lâu có_thể dẫn đến mất nước_mũi và khô mũi do nôn_mửa và độ_ẩm bốc_hơi ra khỏi cơ_thể .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 8:\n",
      "Source (Khmer): ▁ការ សម្រេចចិត្ត របស់ វៀតណាម ក្នុងការ នាំ ការប្រកួត ត្រឡប់ទៅ រក លទ្ធផល ស្មើ វិញ បាន ធ្វើឱ្យ ការប្រកួត កាន់តែ គួរឱ្យ រំភើប ▁និង តា ន តឹង នៅ នាទី បន្ត បន្ទាប់ ។\n",
      "Reference (Vietnamese): Việc đưa trận_đấu trở về vạch xuất_phát của tuyển Việt_Nam đã giúp cho trận_đấu càng trở_nên hấp_dẫn và quyết_liệt hơn ở những phút sau đó .\n",
      "Prediction: Việc Việt_Nam đưa trận_đấu trở_lại với tỷ_số hòa khiến trận_đấu trở_nên hấp_dẫn và căng_thẳng hơn ở những phút tiếp_theo .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 9:\n",
      "Source (Khmer): ▁សូម ឱ្យ យើង អម ដំណើរ អ្នក ក្នុង ដំណើរ កម្សាន្ត ហា យ៉ាង ដែល មានតម្លៃ សមរម្យ របស់អ្នក ពី ទីក្រុង ហូជីមិញ ▁ហើយ ជួយ អ្នក ឱ្យ ទទួលបាន បទពិសោធន៍ ដ៏អស្ចារ្យ !\n",
      "Reference (Vietnamese): Hãy để chúng_tôi đồng_hành cùng bạn trong hành_trình tour du_lịch Hà_Giang từ TP HCM giá tốt và thu về những trải nghiệm tuyệt_vời nhé !\n",
      "Prediction: Hãy cùng chúng_tôi cùng bạn tham_gia chuyến du_lịch Hà_Giang giá rẻ từ TPHCM và giúp bạn trải nghiệm tuyệt_vời nhé !\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 10:\n",
      "Source (Khmer): ▁ ថ្មីៗនេះ ▁ រដ្ឋបាល លោក ▁B id en ▁បាន ព្យាយាម ដាក់ សម្ពាធ លើ ប្រទេស ហូ ឡង់ ឱ្យ ស្វែងរក មធ្យោបាយ ដើម្បី រារាំង ▁A S M L ▁ដែលជា ក្រុមហ៊ុន ផលិត បន្ ទះ ឈ ី ប ធំជាងគេ បំផុត របស់ ពិភពលោក ▁ ពីការ កាត់បន្ថយ កិច្ច សហប្រតិបត្តិការ របស់ខ្លួន ជាមួយ ក្រុមហ៊ុន ចិន ។\n",
      "Reference (Vietnamese): Gần đây , chính_quyền Biden tìm cách gây áp_lực với Hà_Lan , yêu_cầu tìm cách can_ngăn ASML , nhà cung_cấp sản_xuất chip lớn nhất thế_giới , giảm bớt sự hợp_tác với các doanh_nghiệp Trung_Quốc .\n",
      "Prediction: Vừa_qua , chính_quyền Biden đã cố_gắng gây áp_lực lên Hà_Lan để tìm cách ngăn_cản ASML , nhà sản_xuất chip lớn nhất thế_giới , cắt_giảm hợp_tác với các công_ty Trung_Quốc .\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Show sample translations\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE TRANSLATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Source (Khmer): {test_khm[i]}\")\n",
    "    print(f\"Reference (Vietnamese): {test_vi[i]}\")\n",
    "    print(f\"Prediction: {preds_phase2[i]}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474fe58c-7af2-4b00-ba63-0a3fe85529aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714bf3fc-e284-4b3d-b817-79ae1eb5253b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
