{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7b05662-f5f9-450d-ba14-21f39b63165c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.9.1)\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.57.3)\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.10/site-packages (4.4.2)\n",
      "Requirement already satisfied: sentencepiece in ./.local/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: sacrebleu in ./.local/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.10/site-packages (1.12.0)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.local/lib/python3.10/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.local/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.local/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.local/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.local/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.local/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.local/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.local/lib/python3.10/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.local/lib/python3.10/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.local/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.local/lib/python3.10/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.local/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.local/lib/python3.10/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: portalocker in ./.local/lib/python3.10/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in ./.local/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in ./.local/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in ./.local/lib/python3.10/site-packages (from sacrebleu) (6.0.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.local/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers datasets sentencepiece sacrebleu accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00da644b-0b65-4731-9d9c-1e8cc1026c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import html\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    M2M100ForConditionalGeneration,\n",
    "    M2M100Tokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoModelForSeq2SeqLM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26a27070-2ef6-4d80-9ceb-da17d15c868e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA H200\n",
      "VRAM: 150.02 GB\n",
      "Model ready (gradient checkpointing ON)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "MODEL_NAME = \"facebook/m2m100_418M\"\n",
    "\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(MODEL_NAME).cuda()\n",
    "\n",
    "# BẮT BUỘC để giảm VRAM\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "print(\"Model ready (gradient checkpointing ON)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f800454-889e-46f8-a5fa-fe742271be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def clean_text(s: str) -> str:\n",
    "    s = html.unescape(s)                  \n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", s)\n",
    "    return s.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f00a7de-1832-4049-b180-566c087971b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def load_parallel_clean(src_file, tgt_file):\n",
    "    with open(src_file, encoding=\"utf-8\") as f:\n",
    "        src_raw = [clean_text(l) for l in f]\n",
    "\n",
    "    with open(tgt_file, encoding=\"utf-8\") as f:\n",
    "        tgt_raw = [clean_text(l) for l in f]\n",
    "\n",
    "    pairs = [\n",
    "        (s, t)\n",
    "        for s, t in zip(src_raw, tgt_raw)\n",
    "        if s and t\n",
    "    ]\n",
    "\n",
    "    src, tgt = zip(*pairs)\n",
    "\n",
    "    print(f\"Loaded {len(src)} aligned sentence pairs\")\n",
    "\n",
    "    return Dataset.from_dict({\n",
    "        \"src_text\": list(src),\n",
    "        \"tgt_text\": list(tgt)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2fdbff0-646e-42fd-af95-fa34c2df25dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 133166 aligned sentence pairs\n",
      "Loaded 1268 aligned sentence pairs\n",
      "Train: 133166 | Dev: 1268\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "DATA_DIR = \"/home/admin/dataset\"\n",
    "\n",
    "train_dataset = load_parallel_clean(\n",
    "    f\"{DATA_DIR}/train.vi.txt\",\n",
    "    f\"{DATA_DIR}/train.en.txt\"\n",
    ")\n",
    "\n",
    "dev_dataset = load_parallel_clean(\n",
    "    f\"{DATA_DIR}/dev2012.vi.txt\",\n",
    "    f\"{DATA_DIR}/dev2012.en.txt\"\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} | Dev: {len(dev_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce836d37-fb01-409d-a948-494796a06962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "MAX_LEN = 192   \n",
    "\n",
    "def preprocess(batch):\n",
    "    tokenizer.src_lang = \"vi\"\n",
    "    tokenizer.tgt_lang = \"en\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        batch[\"src_text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"tgt_text\"],\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f794fcf0-37a5-4fa2-916e-69692ae6a2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4994ecd8f0aa436f96b7d7149b75aacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/133166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d777db73489348c2bdd744c339e6241f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/1268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=8\n",
    ")\n",
    "\n",
    "dev_dataset = dev_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dev_dataset.column_names,\n",
    "    num_proc=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d1b4072-1dc4-4897-a818-18c448bb5963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f932b36a-2d3a-4799-9eff-7e24904a905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/home/admin/checkpoint4\",\n",
    "\n",
    "    eval_strategy=\"steps\", \n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "\n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=8,    \n",
    "\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,\n",
    "    warmup_steps=1000,\n",
    "\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c28c0ec5-b493-45b6-b216-7564d1750555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18494/649299607.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92b2f9b9-d0ec-4f8d-ab54-5fc69a38c0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6500' max='10410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6500/10410 1:04:01 < 38:31, 1.69 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.616800</td>\n",
       "      <td>1.382349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.561500</td>\n",
       "      <td>1.376668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.455000</td>\n",
       "      <td>1.334916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.416300</td>\n",
       "      <td>1.308038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.272800</td>\n",
       "      <td>1.291595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.289300</td>\n",
       "      <td>1.281267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.283141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.163600</td>\n",
       "      <td>1.267825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.051700</td>\n",
       "      <td>1.276792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.067900</td>\n",
       "      <td>1.266086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.982200</td>\n",
       "      <td>1.283059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.988700</td>\n",
       "      <td>1.274878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.908900</td>\n",
       "      <td>1.281029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6500, training_loss=1.2445401083139274, metrics={'train_runtime': 3842.157, 'train_samples_per_second': 346.592, 'train_steps_per_second': 2.709, 'total_flos': 1.3059161107454362e+17, 'train_loss': 1.2445401083139274, 'epoch': 6.244142736993872})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f1097e6-8b9d-41d5-a34f-c90121048232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/admin/checkpoint3/best_vi_en/tokenizer_config.json',\n",
       " '/home/admin/checkpoint3/best_vi_en/special_tokens_map.json',\n",
       " '/home/admin/checkpoint3/best_vi_en/vocab.json',\n",
       " '/home/admin/checkpoint3/best_vi_en/sentencepiece.bpe.model',\n",
       " '/home/admin/checkpoint3/best_vi_en/added_tokens.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"/home/admin/checkpoint4/best_vi_en\")\n",
    "tokenizer.save_pretrained(\"/home/admin/checkpoint3/best_vi_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb0e1501-39ed-4f45-ae3e-695a33d928fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def load_clean_lines(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return [clean_text(l) for l in f if l.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e7e15d2-d689-46ec-88a9-f7e3504a8f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b21a984a-7465-4dbc-ae39-fc6e63f73db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def translate_en_vi(lines):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "\n",
    "    tokenizer.src_lang = \"vi\"\n",
    "    tokenizer.tgt_lang = \"en\"\n",
    "\n",
    "    for line in lines:\n",
    "        inputs = tokenizer(\n",
    "            line,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=256\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=tokenizer.get_lang_id(\"vi\"),\n",
    "                num_beams=5,\n",
    "                max_length=256\n",
    "            )\n",
    "\n",
    "        outputs.append(tokenizer.decode(gen[0], skip_special_tokens=True))\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64ff1e3b-793b-44e2-a2d2-b1771980a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SacreBLEU: 29.65\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "src_test = load_clean_lines(f\"{DATA_DIR}/tst2012.vi.txt\")\n",
    "ref_test = load_clean_lines(f\"{DATA_DIR}/tst2012.en.txt\")\n",
    "\n",
    "preds = translate_en_vi(src_test)\n",
    "\n",
    "bleu = corpus_bleu(preds, [ref_test], tokenize=\"intl\")\n",
    "print(f\"SacreBLEU: {bleu.score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d722614c-7b1a-4856-9faf-5d120a8e64c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CÂU NGẪU NHIÊN 1 (idx=1309) ---\n",
      "VI (SOURCE):\n",
      "Nhưng những ứng dụng như cộng nghệ nhắc nhở rằng chúng ta không chỉ là người tiêu dùng, và chúng ta không chỉ là nhà tiêu dùng của chính phủ, đóng góp thuế và nhận lại dịch vụ.\n",
      "\n",
      "EN (MODEL):\n",
      "But applications like technology remind us that we 're not just consumers, and we 're not just consumers of government, paying taxes and receiving services.\n",
      "\n",
      "EN (REAL):\n",
      "But these apps are like little digital reminders that we 're not just consumers, and we 're not just consumers of government, putting in our taxes and getting back services.\n",
      "============================================================\n",
      "\n",
      "--- CÂU NGẪU NHIÊN 2 (idx=228) ---\n",
      "VI (SOURCE):\n",
      "nhưng vẫn có cách nghĩ khác về chúng ta đang ở đâu trong nhân cách của mình.\n",
      "\n",
      "EN (MODEL):\n",
      "But there 's a different way of thinking about where we are in our personalities.\n",
      "\n",
      "EN (REAL):\n",
      "But there 's another way of thinking about where we are in our identity.\n",
      "============================================================\n",
      "\n",
      "--- CÂU NGẪU NHIÊN 3 (idx=51) ---\n",
      "VI (SOURCE):\n",
      "Các trung tâm được thành lập.\n",
      "\n",
      "EN (MODEL):\n",
      "Centers were created.\n",
      "\n",
      "EN (REAL):\n",
      "Centers were established.\n",
      "============================================================\n",
      "\n",
      "--- CÂU NGẪU NHIÊN 4 (idx=1518) ---\n",
      "VI (SOURCE):\n",
      "Vào tuần trước, trong một buổi gặp với đại diện chính phủ Hà Lan, Tôi có đặt câu hỏi cho một lãnh đạo của họ rằng liệu ông có thấy là hợp lý không nếu có ai đó phải chết vì việc DigiNotar bị tấn công.\n",
      "\n",
      "EN (MODEL):\n",
      "Last week, in a meeting with the Dutch government representative, I asked one of their leaders if he thought it was appropriate to have someone die because of the DigiNotar attack.\n",
      "\n",
      "EN (REAL):\n",
      "And I asked last week in a meeting with Dutch government representatives, I asked one of the leaders of the team whether he found plausible that people died because of the DigiNotar hack.\n",
      "============================================================\n",
      "\n",
      "--- CÂU NGẪU NHIÊN 5 (idx=563) ---\n",
      "VI (SOURCE):\n",
      "cho nên điều tôi muốn đề nghị bạn hôm nay là 4 kĩ thuật đơn giản Những kĩ thuật mà chúng tôi đã kiểm tra bằng nhiều cách trong nhiều bản khảo sát mà bạn có thể áp dụng dễ dàng trong kinh doanh của bạn\n",
      "\n",
      "EN (MODEL):\n",
      "So what I 'd like to offer you today are four simple techniques that we 've tested in many ways in many surveys that you can easily apply in your business.\n",
      "\n",
      "EN (REAL):\n",
      "So what I want to propose to you today are four simple techniques -- techniques that we have tested in one way or another in different research venues -- that you can easily apply in your businesses.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "N = 5  # số câu muốn xem ngẫu nhiên\n",
    "\n",
    "# Đọc source EN (CLEAN)\n",
    "with open(f\"{DATA_DIR}/tst2012.vi.txt\", encoding=\"utf-8\") as f:\n",
    "    src_lines = [clean_text(l) for l in f if l.strip()]\n",
    "\n",
    "# Đọc reference VI (CLEAN)\n",
    "with open(f\"{DATA_DIR}/tst2012.en.txt\", encoding=\"utf-8\") as f:\n",
    "    ref_lines = [clean_text(l) for l in f if l.strip()]\n",
    "\n",
    "\n",
    "assert len(src_lines) == len(ref_lines)\n",
    "\n",
    "model.eval()\n",
    "tokenizer.src_lang = \"vi\"\n",
    "tokenizer.tgt_lang = \"en\"\n",
    "\n",
    "idxs = random.sample(range(len(src_lines)), N)\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    src = src_lines[idx]\n",
    "    ref = ref_lines[idx]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        src,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.get_lang_id(\"en\"),\n",
    "            num_beams=5,\n",
    "            max_length=256\n",
    "        )\n",
    "\n",
    "    pred = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n--- CÂU NGẪU NHIÊN {i+1} (idx={idx}) ---\")\n",
    "    print(\"VI (SOURCE):\")\n",
    "    print(src)\n",
    "    print(\"\\nEN (MODEL):\")\n",
    "    print(pred)\n",
    "    print(\"\\nEN (REAL):\")\n",
    "    print(ref)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1077b152-02ab-4b2d-8dff-618bd1dd7b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: /home/admin/checkpoint4/best_vi_en.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "src_dir = \"/home/admin/checkpoint4/best_vi_en\"\n",
    "zip_path = \"/home/admin/checkpoint4/best_vi_en\"\n",
    "\n",
    "shutil.make_archive(zip_path, 'zip', src_dir)\n",
    "\n",
    "print(\"Done:\", zip_path + \".zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
