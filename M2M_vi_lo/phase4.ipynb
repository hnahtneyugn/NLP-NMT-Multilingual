{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd13816-7db1-40ee-8419-b5e227f76b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.6.0+cu124)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pyvi\n",
      "  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting laonlp\n",
      "  Downloading LaoNLP-1.2.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.10/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py310-none-any.whl.metadata (7.5 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (3.6.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (2022.12.7)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (3.4)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Downloading lxml-6.0.2-cp310-cp310-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from pyvi) (1.2.2)\n",
      "Collecting sklearn-crfsuite (from pyvi)\n",
      "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting pythainlp>=3.0.0 (from laonlp)\n",
      "  Downloading pythainlp-5.2.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (22.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (3.1.0)\n",
      "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->pyvi)\n",
      "  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m148.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m167.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m154.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.4.2-py3-none-any.whl (512 kB)\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading multiprocess-0.70.18-py310-none-any.whl (134 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m142.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading LaoNLP-1.2.0-py3-none-any.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m150.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m180.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "Downloading yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (346 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (219 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (196 kB)\n",
      "Downloading pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (47.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m151.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pythainlp-5.2.0-py3-none-any.whl (19.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m158.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m791.7/791.7 kB\u001b[0m \u001b[31m123.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading lxml-6.0.2-cp310-cp310-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: xxhash, tqdm, tabulate, sentencepiece, safetensors, requests, regex, python-crfsuite, pyarrow, propcache, portalocker, multidict, lxml, hf-xet, h11, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, sacrebleu, multiprocess, huggingface-hub, httpcore, aiosignal, tokenizers, sklearn-crfsuite, pythainlp, httpx, aiohttp, transformers, pyvi, laonlp, accelerate, datasets\n",
      "\u001b[2K  Attempting uninstall: tqdm\n",
      "\u001b[2K    Found existing installation: tqdm 4.65.0\n",
      "\u001b[2K    Uninstalling tqdm-4.65.0:\n",
      "\u001b[2K      Successfully uninstalled tqdm-4.65.0\n",
      "\u001b[2K  Attempting uninstall: requests\n",
      "\u001b[2K    Found existing installation: requests 2.28.2\n",
      "\u001b[2K    Uninstalling requests-2.28.2:\n",
      "\u001b[2K      Successfully uninstalled requests-2.28.2\n",
      "\u001b[2K  Attempting uninstall: pyarrow0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/36\u001b[0m [regex]\n",
      "\u001b[2K    Found existing installation: pyarrow 11.0.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/36\u001b[0m [regex]\n",
      "\u001b[2K    Uninstalling pyarrow-11.0.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/36\u001b[0m [regex]\n",
      "\u001b[2K      Successfully uninstalled pyarrow-11.0.0━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/36\u001b[0m [pyarrow]\n",
      "\u001b[2K  Attempting uninstall: fsspec╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/36\u001b[0m [lxml]ow]\n",
      "\u001b[2K    Found existing installation: fsspec 2023.3.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/36\u001b[0m [lxml]\n",
      "\u001b[2K    Uninstalling fsspec-2023.3.0:90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/36\u001b[0m [lxml]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2023.3.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/36\u001b[0m [lxml]\n",
      "\u001b[2K  Attempting uninstall: dill\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/36\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: dill 0.3.6━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/36\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling dill-0.3.6:m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/36\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled dill-0.3.6━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/36\u001b[0m [fsspec]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36/36\u001b[0m [datasets]/36\u001b[0m [datasets]e]s]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.12.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 async-timeout-5.0.1 datasets-4.4.2 dill-0.4.0 frozenlist-1.8.0 fsspec-2025.10.0 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.36.0 laonlp-1.2.0 lxml-6.0.2 multidict-6.7.0 multiprocess-0.70.18 portalocker-3.2.0 propcache-0.4.1 pyarrow-22.0.0 pythainlp-5.2.0 python-crfsuite-0.9.11 pyvi-0.1.1 regex-2025.11.3 requests-2.32.5 sacrebleu-2.5.1 safetensors-0.7.0 sentencepiece-0.2.1 sklearn-crfsuite-0.5.0 tabulate-0.9.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.3 xxhash-3.6.0 yarl-1.22.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers sentencepiece datasets sacrebleu accelerate pyvi laonlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5761b115-1554-4d7d-b1a4-cd662dd46def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-22 04:41:50,904] INFO numexpr.utils: Note: detected 192 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-22 04:41:50,904] INFO numexpr.utils: Note: NumExpr detected 192 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[2025-12-22 04:41:50,904] INFO numexpr.utils: NumExpr defaulting to 8 threads.\n",
      "NVIDIA H200\n",
      "VRAM: 150.0217344 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    M2M100ForConditionalGeneration,\n",
    "    M2M100Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import random\n",
    "\n",
    "from pyvi import ViTokenizer\n",
    "from laonlp.tokenize import word_tokenize as lao_word_tokenize\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(\"VRAM:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ecb800-5be8-4f26-93f5-fbbbe6cd16b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vietnamese and Lao tokenizers loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def tokenize_vietnamese(text):\n",
    "    \"\"\"Tokenize Vietnamese text using PyVi\"\"\"\n",
    "    try:\n",
    "        return ViTokenizer.tokenize(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing Vietnamese: {e}\")\n",
    "        return text\n",
    "\n",
    "def tokenize_lao(text):\n",
    "    \"\"\"Tokenize Lao text using LaoNLP\"\"\"\n",
    "    try:\n",
    "        # LaoNLP word_tokenize không có parameter engine\n",
    "        tokens = lao_word_tokenize(text)\n",
    "        return \" \".join(tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing Lao: {e}\")\n",
    "        return text\n",
    "\n",
    "def tokenize_batch_vietnamese(texts):\n",
    "    \"\"\"Batch tokenize Vietnamese texts\"\"\"\n",
    "    print(f\"Tokenizing {len(texts)} Vietnamese texts...\")\n",
    "    results = []\n",
    "    for i, text in enumerate(texts):\n",
    "        results.append(tokenize_vietnamese(text))\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(texts)} Vietnamese texts\")\n",
    "    return results\n",
    "\n",
    "def tokenize_batch_lao(texts):\n",
    "    \"\"\"Batch tokenize Lao texts\"\"\"\n",
    "    print(f\"Tokenizing {len(texts)} Lao texts...\")\n",
    "    results = []\n",
    "    for i, text in enumerate(texts):\n",
    "        results.append(tokenize_lao(text))\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(texts)} Lao texts\")\n",
    "    return results\n",
    "\n",
    "print(\"Vietnamese and Lao tokenizers loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24993ec2-666f-4771-b95e-ddd4a994ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Configuration\n",
    "# ============================================================\n",
    "PHASE3_MODEL = \"/work/m2m_vi_lo_phase3_progressive/best\"\n",
    "DATA_DIR = \"/work/data/dataset\"\n",
    "OUTPUT_DIR = \"/work/phase4_full\"\n",
    "\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 128  # Slightly reduced for full model\n",
    "GRAD_ACCUM = 4\n",
    "LEARNING_RATE = 5e-5  # Very low for full fine-tuning\n",
    "NUM_EPOCHS = 12  # More epochs with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aae468b-46e4-4cfc-b110-bfddfd59055b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Phase 3 model...\n",
      "Model parameters: 483.9M\n",
      "Phase 3 model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Load Phase 3 Model\n",
    "# ============================================================\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "\n",
    "print(\"Loading Phase 3 model...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(PHASE3_MODEL).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(PHASE3_MODEL)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")\n",
    "print(\"Phase 3 model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d1d0785-e3e9-4d60-948f-5f049f55df05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unfreezing all parameters...\n",
      "\n",
      "======================================================================\n",
      "PARAMETER STATISTICS\n",
      "======================================================================\n",
      "Total params        : 483.9M\n",
      "Trainable params    : 483.9M\n",
      "  - Encoder         : 201.6M\n",
      "  - Decoder         : 151.2M\n",
      "Trainable %         : 100.0%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Unfreeze ALL Parameters\n",
    "# ============================================================\n",
    "print(\"\\nUnfreezing all parameters...\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Verify\n",
    "total, trainable = 0, 0\n",
    "encoder_trainable, decoder_trainable = 0, 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    n = param.numel()\n",
    "    total += n\n",
    "    \n",
    "    if param.requires_grad:\n",
    "        trainable += n\n",
    "        if \"encoder\" in name:\n",
    "            encoder_trainable += n\n",
    "        elif \"decoder\" in name:\n",
    "            decoder_trainable += n\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PARAMETER STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total params        : {total/1e6:.1f}M\")\n",
    "print(f\"Trainable params    : {trainable/1e6:.1f}M\")\n",
    "print(f\"  - Encoder         : {encoder_trainable/1e6:.1f}M\")\n",
    "print(f\"  - Decoder         : {decoder_trainable/1e6:.1f}M\")\n",
    "print(f\"Trainable %         : 100.0%\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fac730b-90c3-49cd-a487-fecb9908bbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading datasets...\n",
      "Tokenizing Vietnamese texts...\n",
      "Tokenizing 695512 Vietnamese texts...\n",
      "  Processed 10000/695512 Vietnamese texts\n",
      "  Processed 20000/695512 Vietnamese texts\n",
      "  Processed 30000/695512 Vietnamese texts\n",
      "  Processed 40000/695512 Vietnamese texts\n",
      "  Processed 50000/695512 Vietnamese texts\n",
      "  Processed 60000/695512 Vietnamese texts\n",
      "  Processed 70000/695512 Vietnamese texts\n",
      "  Processed 80000/695512 Vietnamese texts\n",
      "  Processed 90000/695512 Vietnamese texts\n",
      "  Processed 100000/695512 Vietnamese texts\n",
      "  Processed 110000/695512 Vietnamese texts\n",
      "  Processed 120000/695512 Vietnamese texts\n",
      "  Processed 130000/695512 Vietnamese texts\n",
      "  Processed 140000/695512 Vietnamese texts\n",
      "  Processed 150000/695512 Vietnamese texts\n",
      "  Processed 160000/695512 Vietnamese texts\n",
      "  Processed 170000/695512 Vietnamese texts\n",
      "  Processed 180000/695512 Vietnamese texts\n",
      "  Processed 190000/695512 Vietnamese texts\n",
      "  Processed 200000/695512 Vietnamese texts\n",
      "  Processed 210000/695512 Vietnamese texts\n",
      "  Processed 220000/695512 Vietnamese texts\n",
      "  Processed 230000/695512 Vietnamese texts\n",
      "  Processed 240000/695512 Vietnamese texts\n",
      "  Processed 250000/695512 Vietnamese texts\n",
      "  Processed 260000/695512 Vietnamese texts\n",
      "  Processed 270000/695512 Vietnamese texts\n",
      "  Processed 280000/695512 Vietnamese texts\n",
      "  Processed 290000/695512 Vietnamese texts\n",
      "  Processed 300000/695512 Vietnamese texts\n",
      "  Processed 310000/695512 Vietnamese texts\n",
      "  Processed 320000/695512 Vietnamese texts\n",
      "  Processed 330000/695512 Vietnamese texts\n",
      "  Processed 340000/695512 Vietnamese texts\n",
      "  Processed 350000/695512 Vietnamese texts\n",
      "  Processed 360000/695512 Vietnamese texts\n",
      "  Processed 370000/695512 Vietnamese texts\n",
      "  Processed 380000/695512 Vietnamese texts\n",
      "  Processed 390000/695512 Vietnamese texts\n",
      "  Processed 400000/695512 Vietnamese texts\n",
      "  Processed 410000/695512 Vietnamese texts\n",
      "  Processed 420000/695512 Vietnamese texts\n",
      "  Processed 430000/695512 Vietnamese texts\n",
      "  Processed 440000/695512 Vietnamese texts\n",
      "  Processed 450000/695512 Vietnamese texts\n",
      "  Processed 460000/695512 Vietnamese texts\n",
      "  Processed 470000/695512 Vietnamese texts\n",
      "  Processed 480000/695512 Vietnamese texts\n",
      "  Processed 490000/695512 Vietnamese texts\n",
      "  Processed 500000/695512 Vietnamese texts\n",
      "  Processed 510000/695512 Vietnamese texts\n",
      "  Processed 520000/695512 Vietnamese texts\n",
      "  Processed 530000/695512 Vietnamese texts\n",
      "  Processed 540000/695512 Vietnamese texts\n",
      "  Processed 550000/695512 Vietnamese texts\n",
      "  Processed 560000/695512 Vietnamese texts\n",
      "  Processed 570000/695512 Vietnamese texts\n",
      "  Processed 580000/695512 Vietnamese texts\n",
      "  Processed 590000/695512 Vietnamese texts\n",
      "  Processed 600000/695512 Vietnamese texts\n",
      "  Processed 610000/695512 Vietnamese texts\n",
      "  Processed 620000/695512 Vietnamese texts\n",
      "  Processed 630000/695512 Vietnamese texts\n",
      "  Processed 640000/695512 Vietnamese texts\n",
      "  Processed 650000/695512 Vietnamese texts\n",
      "  Processed 660000/695512 Vietnamese texts\n",
      "  Processed 670000/695512 Vietnamese texts\n",
      "  Processed 680000/695512 Vietnamese texts\n",
      "  Processed 690000/695512 Vietnamese texts\n",
      "Tokenizing Lao texts...\n",
      "Tokenizing 695512 Lao texts...\n",
      "  Processed 10000/695512 Lao texts\n",
      "  Processed 20000/695512 Lao texts\n",
      "  Processed 30000/695512 Lao texts\n",
      "  Processed 40000/695512 Lao texts\n",
      "  Processed 50000/695512 Lao texts\n",
      "  Processed 60000/695512 Lao texts\n",
      "  Processed 70000/695512 Lao texts\n",
      "  Processed 80000/695512 Lao texts\n",
      "  Processed 90000/695512 Lao texts\n",
      "  Processed 100000/695512 Lao texts\n",
      "  Processed 110000/695512 Lao texts\n",
      "  Processed 120000/695512 Lao texts\n",
      "  Processed 130000/695512 Lao texts\n",
      "  Processed 140000/695512 Lao texts\n",
      "  Processed 150000/695512 Lao texts\n",
      "  Processed 160000/695512 Lao texts\n",
      "  Processed 170000/695512 Lao texts\n",
      "  Processed 180000/695512 Lao texts\n",
      "  Processed 190000/695512 Lao texts\n",
      "  Processed 200000/695512 Lao texts\n",
      "  Processed 210000/695512 Lao texts\n",
      "  Processed 220000/695512 Lao texts\n",
      "  Processed 230000/695512 Lao texts\n",
      "  Processed 240000/695512 Lao texts\n",
      "  Processed 250000/695512 Lao texts\n",
      "  Processed 260000/695512 Lao texts\n",
      "  Processed 270000/695512 Lao texts\n",
      "  Processed 280000/695512 Lao texts\n",
      "  Processed 290000/695512 Lao texts\n",
      "  Processed 300000/695512 Lao texts\n",
      "  Processed 310000/695512 Lao texts\n",
      "  Processed 320000/695512 Lao texts\n",
      "  Processed 330000/695512 Lao texts\n",
      "  Processed 340000/695512 Lao texts\n",
      "  Processed 350000/695512 Lao texts\n",
      "  Processed 360000/695512 Lao texts\n",
      "  Processed 370000/695512 Lao texts\n",
      "  Processed 380000/695512 Lao texts\n",
      "  Processed 390000/695512 Lao texts\n",
      "  Processed 400000/695512 Lao texts\n",
      "  Processed 410000/695512 Lao texts\n",
      "  Processed 420000/695512 Lao texts\n",
      "  Processed 430000/695512 Lao texts\n",
      "  Processed 440000/695512 Lao texts\n",
      "  Processed 450000/695512 Lao texts\n",
      "  Processed 460000/695512 Lao texts\n",
      "  Processed 470000/695512 Lao texts\n",
      "  Processed 480000/695512 Lao texts\n",
      "  Processed 490000/695512 Lao texts\n",
      "  Processed 500000/695512 Lao texts\n",
      "  Processed 510000/695512 Lao texts\n",
      "  Processed 520000/695512 Lao texts\n",
      "  Processed 530000/695512 Lao texts\n",
      "  Processed 540000/695512 Lao texts\n",
      "  Processed 550000/695512 Lao texts\n",
      "  Processed 560000/695512 Lao texts\n",
      "  Processed 570000/695512 Lao texts\n",
      "  Processed 580000/695512 Lao texts\n",
      "  Processed 590000/695512 Lao texts\n",
      "  Processed 600000/695512 Lao texts\n",
      "  Processed 610000/695512 Lao texts\n",
      "  Processed 620000/695512 Lao texts\n",
      "  Processed 630000/695512 Lao texts\n",
      "  Processed 640000/695512 Lao texts\n",
      "  Processed 650000/695512 Lao texts\n",
      "  Processed 660000/695512 Lao texts\n",
      "  Processed 670000/695512 Lao texts\n",
      "  Processed 680000/695512 Lao texts\n",
      "  Processed 690000/695512 Lao texts\n",
      "Total dataset size: 695512 examples\n",
      "Train dataset: 691512 examples (for training)\n",
      "Dev dataset  : 3000 examples (for validation during training)\n",
      "Test dataset : 1000 examples (for final evaluation)\n",
      "\n",
      "Data split and shuffle completed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Load Data\n",
    "# ============================================================\n",
    "def load_parallel(src_file, tgt_file):\n",
    "    with open(src_file, encoding=\"utf-8\") as f:\n",
    "        src = [l.strip() for l in f]\n",
    "    with open(tgt_file, encoding=\"utf-8\") as f:\n",
    "        tgt = [l.strip() for l in f]\n",
    "    \n",
    "    assert len(src) == len(tgt)\n",
    "    \n",
    "    # Apply language-specific tokenization\n",
    "    print(\"Tokenizing Vietnamese texts...\")\n",
    "    src_tokenized = tokenize_batch_vietnamese(src)\n",
    "    \n",
    "    print(\"Tokenizing Lao texts...\")\n",
    "    tgt_tokenized = tokenize_batch_lao(tgt)\n",
    "    \n",
    "    return Dataset.from_dict({\n",
    "        \"src_text\": src_tokenized,\n",
    "        \"tgt_text\": tgt_tokenized\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"\\nLoading datasets...\")\n",
    "# Load toàn bộ dữ liệu từ train.vi và train.lo\n",
    "full_dataset = load_parallel(\n",
    "    f\"{DATA_DIR}/train.vi\",\n",
    "    f\"{DATA_DIR}/train.lo\"\n",
    ")\n",
    "\n",
    "print(f\"Total dataset size: {len(full_dataset)} examples\")\n",
    "\n",
    "# Chia dataset: \n",
    "# - Test: 1000 dòng cuối cùng\n",
    "# - Dev: 3000 dòng trước test\n",
    "# - Train: phần còn lại\n",
    "\n",
    "test_size = 1000\n",
    "dev_size = 3000\n",
    "\n",
    "# Tính chỉ số\n",
    "test_start_idx = len(full_dataset) - test_size\n",
    "dev_start_idx = test_start_idx - dev_size\n",
    "\n",
    "# Chia dataset\n",
    "test_dataset = full_dataset.select(range(test_start_idx, len(full_dataset)))\n",
    "dev_dataset = full_dataset.select(range(dev_start_idx, test_start_idx))\n",
    "train_dataset = full_dataset.select(range(0, dev_start_idx))\n",
    "\n",
    "# Shuffle training data để tránh bias thứ tự\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} examples (for training)\")\n",
    "print(f\"Dev dataset  : {len(dev_dataset)} examples (for validation during training)\")\n",
    "print(f\"Test dataset : {len(test_dataset)} examples (for final evaluation)\")\n",
    "print(\"\\nData split and shuffle completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3b9e1f5-176a-4bf2-9a84-8d674eb347f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4586bcf220c42b98a5446c41c45159a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train (num_proc=8):   0%|          | 0/691512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3902354b39df4823ae7adf48675ab8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dev (num_proc=8):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Preprocessing\n",
    "# ============================================================\n",
    "def preprocess(batch):\n",
    "    tokenizer.src_lang = \"vi\"\n",
    "    tokenizer.tgt_lang = \"lo\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        batch[\"src_text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"tgt_text\"],\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "print(\"\\nPreprocessing datasets...\")\n",
    "train_processed = train_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=8,\n",
    "    desc=\"Processing train\"\n",
    ")\n",
    "\n",
    "dev_processed = dev_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dev_dataset.column_names,\n",
    "    num_proc=8,\n",
    "    desc=\"Processing dev\"\n",
    ")\n",
    "\n",
    "print(\"Preprocessing completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c36f3c1-792e-4ece-8268-07135281cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Data Collator\n",
    "# ============================================================\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa1778f3-41e6-4c07-a1dc-d6913f31271f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training configuration:\n",
      "Effective batch size: 512\n",
      "Learning rate: 5e-05\n",
      "Epochs: 12\n",
      "Label smoothing: 0.15\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Training Arguments\n",
    "# ============================================================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Evaluation & Saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=100,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    \n",
    "    # Batch size\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    \n",
    "    # Learning rate - VERY LOW for full fine-tuning\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.08,\n",
    "    \n",
    "    # Regularization - STRONGER to prevent overfitting\n",
    "    weight_decay=0.1,\n",
    "    max_grad_norm=0.4,  # Stricter clipping\n",
    "    \n",
    "    # Training\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    \n",
    "    # FP16\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "\n",
    "    # Speed\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=8,\n",
    "    dataloader_pin_memory=True,\n",
    "    \n",
    "    \n",
    "    # Best model\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"\\nTraining configuration:\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRAD_ACCUM}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Label smoothing: 0.15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c17a20a3-e687-4abf-9ad2-37814d4b0c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_159/2989425937.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 9: Create Trainer\n",
    "# ============================================================\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_processed,\n",
    "        eval_dataset=dev_processed,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(early_stopping_patience=8)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36148642-bead-49ae-8ad6-2e8ee3d0b763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING PHASE 4: FULL FINE-TUNING\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16212' max='16212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16212/16212 1:53:02, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.812800</td>\n",
       "      <td>0.851697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.830800</td>\n",
       "      <td>0.861033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.826200</td>\n",
       "      <td>0.870433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.832300</td>\n",
       "      <td>0.861779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.829700</td>\n",
       "      <td>0.855557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.787700</td>\n",
       "      <td>0.850067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.798200</td>\n",
       "      <td>0.843864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.789200</td>\n",
       "      <td>0.834091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.756300</td>\n",
       "      <td>0.834912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.757100</td>\n",
       "      <td>0.826340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.822348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.722700</td>\n",
       "      <td>0.821385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.724500</td>\n",
       "      <td>0.815910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.688000</td>\n",
       "      <td>0.816086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.694100</td>\n",
       "      <td>0.811625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.697600</td>\n",
       "      <td>0.810006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.665700</td>\n",
       "      <td>0.810868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.671300</td>\n",
       "      <td>0.806923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.661800</td>\n",
       "      <td>0.809777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.645500</td>\n",
       "      <td>0.807142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.806279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.625500</td>\n",
       "      <td>0.807067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.627100</td>\n",
       "      <td>0.805578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.632700</td>\n",
       "      <td>0.802182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.607600</td>\n",
       "      <td>0.800104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.615600</td>\n",
       "      <td>0.800497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.615300</td>\n",
       "      <td>0.796525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.593700</td>\n",
       "      <td>0.796743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.602800</td>\n",
       "      <td>0.797694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.583400</td>\n",
       "      <td>0.795846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.590100</td>\n",
       "      <td>0.795928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.582900</td>\n",
       "      <td>0.794451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETED\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 10: Train\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING PHASE 4: FULL FINE-TUNING\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "609fc963-edba-46d5-b652-0cb976048f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving best model...\n",
      "Model saved to /work/phase4_full/best\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 11: Save Model\n",
    "# ============================================================\n",
    "print(\"\\nSaving best model...\")\n",
    "trainer.save_model(f\"{OUTPUT_DIR}/best\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/best\")\n",
    "print(f\"Model saved to {OUTPUT_DIR}/best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95504577-5450-41ea-a473-c27d069a922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 12: Evaluation Function\n",
    "# ============================================================\n",
    "def translate_batch(texts, model, tokenizer, batch_size=32):\n",
    "    \"\"\"Batch translation for speed\"\"\"\n",
    "    model.eval()\n",
    "    tokenizer.src_lang = \"vi\"\n",
    "    tokenizer.tgt_lang = \"lo\"\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=tokenizer.get_lang_id(\"lo\"),\n",
    "                num_beams=5,\n",
    "                max_length=256\n",
    "            )\n",
    "        \n",
    "        texts_out = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "        outputs.extend(texts_out)\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"Translated {i+len(batch)}/{len(texts)}\")\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f30a369c-2dd3-4c39-bd93-2fb8017cdcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set size: 1000 examples\n",
      "\n",
      "Translating test set...\n",
      "\n",
      "Translating test set...\n",
      "Translated 320/1000\n",
      "Translated 640/1000\n",
      "Translated 960/1000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 13: Evaluate on Test Set\n",
    "# ============================================================\n",
    "# Load test data\n",
    "# Lấy dữ liệu test từ test_dataset (đã chia từ train.vi/train.lo)\n",
    "test_vi = test_dataset[\"src_text\"]\n",
    "test_lo = test_dataset[\"tgt_text\"]\n",
    "\n",
    "print(f\"\\nTest set size: {len(test_vi)} examples\")\n",
    "print(\"\\nTranslating test set...\")\n",
    "\n",
    "# Translate\n",
    "print(\"\\nTranslating test set...\")\n",
    "predictions = translate_batch(test_vi, model, tokenizer)\n",
    "\n",
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "\n",
    "# Calculate BLEU\n",
    "bleu_score = corpus_bleu(predictions, [test_lo])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3076473f-036c-411c-98b5-330f0500188b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PHASE 4 RESULTS (FINAL)\n",
      "======================================================================\n",
      "BLEU Score: 28.72\n",
      "======================================================================\n",
      "\n",
      "Predictions saved to /work/phase4_full/test_predictions.txt\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 14: Final Results\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 4 RESULTS (FINAL)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"BLEU Score: {bleu_score.score:.2f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save predictions\n",
    "with open(f\"{OUTPUT_DIR}/test_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(predictions))\n",
    "\n",
    "print(f\"\\nPredictions saved to {OUTPUT_DIR}/test_predictions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adb02a20-e19e-4cee-9ad9-2d65e22db212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAMPLE TRANSLATIONS\n",
      "======================================================================\n",
      "\n",
      "Example 1:\n",
      "Source    : Chính_phủ Hoa_Kỳ sẽ tiếp_tục hợp_tác với Văn_phòng Tình_báo Quốc_gia ( NRA ) và UXO Lao để xác_định các phương_thức khác nhằm tăng_cường năng_lực của ngành rà phá bom mìn chưa nổ , phù_hợp với các ưu_tiên của chính_phủ Lào và tầm nhìn_chung về việc loại_bỏ bom mìn chưa nổ như một trở_ngại cho sự phát_triển vào năm 2030 .\n",
      "Reference : ລັດຖະບານ   ສະຫະລັດ ອາເມຣິກາ ຈະ ສືບຕໍ່ ເຮັດວຽກ ກັບ   ຄຊກລ   ( NRA )   ແລະ   ຄກລ   ( UXO   Lao )   ເພື່ອ ກຳນົດ ວິທີການ ອື່ນ ໆ   ໃນ ການຍົກສູງ   ຄວາມ ອາດ ສາ   ມາດ ຂອງ ຂະແໜງ ເກັບ ກູ້ ລະເບີດ ບໍ່ທັນ ແຕກ   ໃຫ້ ສອດຄ່ອງ   ກັບ ບູລິມະສິດ ຂອງ ລັດຖະບານ ລາວ   ແລະ ມີ ວິໄສທັດ ຮ່ວມກັນ   ໃນ ການລົບລ້າງ ລະເບີດ ບໍ່ທັນ ແຕກ   ທີ ເປັນ ສິ່ງ ກີດຂວາງ ຕໍ່ ການພັດທະນາ   ພາຍໃນ ປີ   2030\n",
      "Prediction: ລັດຖະບານ ສະຫະລັດ ອາເມຣິກາ ຈະ ສືບຕໍ່ ເຮັດວຽກ ຮ່ວມ ກັບ ຫ້ອງການ ສືບ ລັບ ແຫ່ງຊາດ ( NRA ) ແລະ UXO Lao ເພື່ອ ກໍານົດ ວິທີການ ອື່ນ ໆ ເພື່ອ ເສີມສ້າງ ຄວາມ ອາດ ສາມາດ ໃນ ການເກັບ ກູ້ ລະເບີດ ບໍ່ທັນ ແຕກ ໃຫ້ ສອດຄ່ອງ ກັບ ບູລິມະສິດ ຂອງ ລັດຖະບານ ລາວ ແລະ ວິໄສທັດ ຮ່ວມກັນ ກ່ຽວກັບ ການເກັບ ກູ້ ລະເບີດ ບໍ່ທັນ ແຕກ ໃຫ້ ເປັນ ອຸບ ປະ ສັກ ຕໍ່ ການພັດທະນາ ພາຍໃນ ປີ 2030\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Source    : Tóm lại , Arizona là một tiểu_bang sa_mạc ở Hoa_Kỳ , nơi đã chào_đón người Lào thuộc mọi sắc_tộc đến định_cư từ năm 1975 , và ông Bandit_Phonpiboun đã có đóng_góp đáng_kể trong việc giúp_đỡ và phát_triển cộng_đồng người Lào tại tiểu_bang này .\n",
      "Reference : ສະຫລຸບ ແລ້ວ   ລັດ ອາຣີ ໂຊນາ ເປັນ ລັດທີ່ມີ ທະເລຊາຍ ແຫ່ງ ນຶ່ງ ຂອງ ສະຫະລັດທີ່ ໄດ້ ມີການຮັບ ເອົາ ຄົນ ລາວ ທຸກ ຊົນຊາດ ຊົນເຜົ່າ ໄປ ຕັ້ງ ຖິ່ນຖານ ຢູ່ໃນ ທີ່ ນັ້ນ   ນັບແຕ່ ປີ   1975   ເປັນຕົ້ນມາ   ແລະ ທ່ານ ບັນດິດ   ພົນ ພິບູນ ກໍ   ໄດ້ ມີ ສ່ວນປະກອບ   ອັນ ຫລວງຫລາຍ ໃນ ການຊ່ວຍເຫລືອ ແລະ ພັດທະນາ ຊຸມຊົນ ລາວ ຢູ່ໃນ ລັດ ນັ້ນ\n",
      "Prediction: ສະຫລຸບ ແລ້ວ ອາຣີ ໂຊນາ ເປັນ ລັດທີ່ ຕັ້ງ ຢູ່ ແຄມ ທະເລຊາຍ ໃນ ສະຫະລັດ ອາເມຣິກາ ທີ່ ໄດ້ ໃຫ້ການຕ້ອນຮັບ ຊາວລາວ ບັນດາ ເຜົ່າ ເຂົ້າມາ ຕັ້ງ ຖິ່ນຖານ ນັບແຕ່ ປີ 1975 ເປັນຕົ້ນມາ ແລະ ທ່ານ ບັນດິດ ພອນ ພີ ບູນ ກໍ ໄດ້ ປະກອບສ່ວນ ອັນ ສໍາຄັນ ເຂົ້າໃນ ການຊ່ວຍເຫລືອ ແລະ ພັດທະນາ ຊຸມຊົນ ລາວ ຢູ່ໃນ ລັດ ດັ່ງກ່າວ\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Source    : Về việc cấp quyền mua và sở_hữu đất_đai và các tòa nhà thương_mại tại Lào trong thời_hạn lên đến 99 năm cho người nước_ngoài đã đầu_tư từ 300.000 đô_la trở lên , chính_quyền Lào sẽ xác_định rõ khu_vực để tránh ảnh_hưởng đến nông_dân Lào , những người chiếm đa_số dân_số cả nước .\n",
      "Reference : ສ່ວນ ກ່ຽວກັບ ການໃຫ້ ສິດ ໃນ ການຊື້ ເພື່ອການຖື ຄອງ ທີ່ດິນ   ແລະ   ອາຄານ ການຄ້າ ໃນ ລາວ   ໄດ້ ເຖິງ   99   ປີ   ສໍ າ ລັບ ຊາວ ຕ່າງຊາດ ທີ່ ໄດ້ ລົງທຶນ ໃນ ມູນຄ່າ   ຕັ້ງແຕ່   3   ແສນ ໂດລາ ຂຶ້ນ ໄປ ນັ້ນ   ທາງການລາວ ກໍ່ ຈະ ມີ   ການກຳນົດ ພື້ນທີ່ ຢ່າງ ຈະແຈ້ງ   ເພື່ອ ປ້ອງກັນ ບໍ່ໃຫ້ ສົ່ງ ຜົນກະທົບ   ຕໍ່ ກະເສດຕະກອນ ລາວ   ທີ່ເປັນ ຄົນ ສ່ວນໃຫຍ່ ຂອງ ປະເທດ ຕໍ່ໄປ\n",
      "Prediction: ສ່ວນ ໃນດ້ານ ການໃຫ້ ສິດ ໃນ ການຊື້ - ຄອບຄອງ ທີ່ດິນ ແລະ ສິ່ງ ປູກ ສ້າງ ຕ່າງໆ ໃນ ລາວ ເປັນ ໄລຍະ ເວລາ ເຖິງ 99 ປີ ແກ່ ຊາວ ຕ່າງຊາດ ທີ່ ລົງທຶນ ລະຫວ່າງ 3 ແສນ ໂດລາ ຂຶ້ນ ໄປ ນັ້ນ ທາງການລາວ ກໍ ຈະ ກໍານົດ ພື້ນທີ່ ຢ່າງ ຊັດເຈນ ເພື່ອ ຫລີກລ້ຽງ ຜົນກະທົບ ຕໍ່ ກະເສດຕະກອນ ລາວ ທີ່ມີ ປະຊາກອນ ສ່ວນໃຫຍ່ ໃນ ລາວ\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "Source    : Khoảng 20 xe_bồn chở nhiên_liệu đã bị đốt cháy trong vụ tấn_công mới nhất vào tuyến đường tiếp_tế mà chính_quyền Pakistan đã đóng_cửa hôm thứ Năm để phản_đối các cuộc không_kích của NATO qua biên_giới .\n",
      "Reference : ບັນທຸກ ນໍ້າມັນ   ປະມານ   20   ຄັນໄດ້ ຖືກ ຈູດເຜົາ ໃນ ການໂຈມຕີ ເທື່ອ ຫລ້າສຸດ   ຢູ່ ເທິງ ເສັ້ນທາງ ລຳລຽງ ທີ່ ທາງການປາກີ ສ   ຖານ   ໄດ້ ປິດ ລົງ ໃນ ວັນພະຫັດ ແລ້ວນີ້   ເພື່ອ ປະທ້ວງ ຕໍ່ ການທີ່ ອົງການ   NATO   ໄດ້ ທໍ າ ການໂຈມຕີ   ທາງອາກາດ ຂ້າມ   ຊາຍແດນ ເຂົ້າໄປ ນັ້ນ\n",
      "Prediction: ລົດບັນທຸກ ນໍ້າມັນ ປະມານ 20 ຄັນໄດ້ ຖືກ ຈູດເຜົາ ໃນ ການໂຈມຕີ ຄັ້ງ ຫຼ້າສຸດ ຕໍ່ ເສັ້ນທາງ ລໍ າ ລຽງ ສະບຽງ ທີ່ ທາງການປາກິສຖານ ໄດ້ ປິດ ລົງ ໃນ ວັນພະຫັດ ວານນີ້ ເພື່ອ ປະທ້ວງ ຕໍ່ ການໂຈມຕີ ທາງອາກາດ ຂອງ ກຸ່ມ ເນໂຕ້ ຂ້າມ ຊາຍແດນ\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "Source    : Theo một tuyên_bố từ Bộ Tư_lệnh Ấn_Độ_Dương - Thái_Bình_Dương , Paparo đã kêu_gọi PLA \" xem_xét_lại việc sử_dụng các chiến_thuật cưỡng_ép , nguy_hiểm và có khả_năng leo_thang ở Biển_Đông và các khu_vực khác \" , đồng_thời mô_tả cuộc trao_đổi là \" mang tính xây_dựng và tôn_trọng lẫn nhau \" .\n",
      "Reference : ທ່ານ   ປາປາ ໂຣ   ໄດ້ ອອກມາ ຮຽກຮ້ອງ ໃຫ້   PLA   ຈົ່ງ   “ ພິຈາລະນາກ່ຽວກັບ ການ   ນໍ າ ໃຊ້ ສິ່ງ ທີ່ເປັນ ການ ບີບບັງຄັບ ,   ເປັນ ຍຸດທະສາດ ທີ່ ອັນຕະລາຍ   ແລະ ອາດຈະ   ເພີ້ມ ຂຶ້ນ ໃນ ຂົງເຂດ ທະເລຈີນໃຕ້   ພ້ອມ ທັງ ບໍລິເວນ ອື່ນ ໆ ອີກ ຄັ້ງ ,”   ອີງ ຕາມ ການ   ກ່າວ ໃນ ຖະແຫຼງການ ຂອງ ຜູ້ບັນຊາການ ປະຈໍາ ພາກພື້ນ   ອິນໂດ - ປາຊີຟິກ   ທີ່ ອະ   ທິບາຍ ກ່ຽວກັບ ການແລກປ່ຽນ ວ່າ   ເປັນ ສິ່ງ ທີ່   “ ສ້າງສັນ   ແລະ ເຄົາລົບ ເຊິ່ງ ກັນ   ແລະ ກັນ\n",
      "Prediction: ທ່ານ ປາປາ ໂຣ ໄດ້ ຮຽກຮ້ອງ ໃຫ້ PLA “ ທົບທວນ ຄືນ ການໃຊ້ ກົນລະຍຸດ ແບບ ບີບບັງຄັບ ທີ່ເປັນ ອັນຕະລາຍ ແລະ ອາດ ຂະຫຍາຍ ວົງ ກວ້າງ ອອກໄປ ໃນ ທະເລຈີນໃຕ້ ແລະ ພາກພື້ນ ອື່ນ ໆ ,” ອີງ ຕາມ ຖະແຫຼງການ ຂອງ ກອງ ບັນຊາການ ອິນໂດ - ປາຊີຟິກ , ໂດຍ ອະທິບາຍ ການແລກປ່ຽນ ດັ່ງກ່າວ ວ່າ “ ສ້າງສັນ ແລະ ເຄົາລົບ ເຊິ່ງກັນແລະກັນ\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 6:\n",
      "Source    : Tổng_thống Pháp Francois_Hollande đã đến Nga hôm thứ Năm như một phần trong nỗ_lực xây_dựng liên_minh toàn_cầu để tăng_cường cuộc_chiến chống lại Nhà_nước Hồi_giáo sau các vụ tấn_công chết người ở Paris .\n",
      "Reference : ປະທານາທິບໍດີ   ຝຣັ່ງ   ທ່ານ   Francois   Hollande   ໄດ້ ເດີນທາງ   ໄປ ປະເທດ ຣັດເຊຍ   ໃນ ວັນພະຫັດ ມື້ນີ້   ເຊິ່ງ ເປັນ ສ່ວນ ໜຶ່ງ ໃນ ຄວາມ   ພະຍາ   ຍາມ ທີ່ຈະ ສ້າງ ພັນທະມິດ ທົ່ວໂລກ   ເພື່ອ ເພີ້ມ ທະວີການຕໍ່ສູ້   ກັບ ພວກ ລັດ   ອິສລາມ   ອັນ ເປັນ ຜົນ ເນື່ອງ ມາຈາກການໂຈມຕີ ທີ່ ຮ້າຍ   ແຮງ ໃນ ນະຄອນ   ຫຼວງ   ປາຣີ\n",
      "Prediction: ປະທານາທິບໍດີ ຝຣັ່ງ ທ່ານ Francois Hollande ເດີນທາງ ໄປ ຮອດ ຣັດເຊຍ ໃນ ວັນພະຫັດ ມື້ນີ້ ອັນ ເປັນ ພາກສ່ວນ ນຶ່ງ ໃນ ການດໍາເນີນ ຄວາມພະຍາຍາມ ເພື່ອ ສ້າງ ພັນທະມິດ ໃນ ທົ່ວໂລກ ເພື່ອ ເສີມ ຂະຫຍາຍ ການຕໍ່ສູ້ ຕ້ານ ພວກ ລັດ ອິສລາມ ຫຼັງຈາກການໂຈມຕີ ທີ່ ຮ້າຍແຮງ ເຖິງ ແກ່ ຊີວິດ ໃນ ນະຄອນຫຼວງ ປາຣີ\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 7:\n",
      "Source    : Việt_Nam đứng thứ 116 , Indonesia thứ 110 , Thái_Lan thứ 78 ( đồng hạng với Trung_Quốc ) , Malaysia thứ 56 , Hàn_Quốc thứ 39 , Brunei thứ 38 , Bhutan thứ 36 , Đài_Loan thứ 33 , Nhật_Bản thứ 17 và Hồng_Kông thứ 13 nằm trong số các quốc_gia ở vị_trí giữa danh_sách .\n",
      "Reference : ຫວຽດນາມ ມາ ໃນ ອັນດັບ   116   ອິນໂດເນເຊຍ ອັນດັບ   110   ໄທ ອັນດັບ   78   ເທົ່າກັນກັບ ຈີນ ,   ມາເລເຊຍ   ຕິດ ອັນດັບ ທີ   56   ເກົາຫລີ ໃຕ້ ອັນດັບ ທີ   39   ບຣູໄນ ອັນດັບ ທີ   38   Bhutan   ທີ   36   ໄຕ້ຫວັນ ທີ   33   ຍີ່ປຸ່ນ   ທີ   17   ແລະ ຮອງ ກົງ ທີ   13   ແມ່ນ ໂຮມ ຢູ່ໃນ ບັນດາ ປະເທດ ທີ່ ຢູ່ໃນ ລະດັບ ກາງ   ຂອງ ບັນຊີ\n",
      "Prediction: ຫວຽດນາມ ຢູ່ໃນ ອັນດັບ 116 ອິນໂດເນເຊຍ 110 ໄທ 78 ເທົ່າກັບ ຈີນ ມາເລເຊຍ 56 ເກົາຫຼີ 39 ບຣູໄນ 38 ພູ ຖານ 36 ໄຕ້ຫວັນ 33 ຍີ່ປຸ່ນ 17 ແລະ ຮົງກົງ 13 ແມ່ນ ຮວມ ຢູ່ໃນ ບັນດາ ປະເທດ ທີ່ ຢູ່ໃນ ກາງ ບັນຊີ\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 8:\n",
      "Source    : Fang_Bin và các thành_viên khác của xã_hội dân_sự , được mệnh_danh là nhà_báo nhân_dân , đã đăng_tải chi_tiết về dịch COVID - 19 vào đầu năm 2020 trên Internet và mạng xã_hội , làm bẽ_mặt các quan_chức Trung_Quốc , những người đang phải đối_mặt với chỉ_trích vì không kiểm_soát được dịch_bệnh .\n",
      "Reference : ທ້າວ ຟາງ   ບິນ   ( Fang   Bin )   ແລະ ສະມາຊິກ ທາງ ສັງຄົມ ຄົນອື່ນ ໆ   ໄດ້ ຖືກ ຕັ້ງ ນາມ ໃຫ້ ວ່າ   ເປັນ ນັກຂ່າວ ປະຊາຊົນ ,   ໄດ້ ພາກັນ ໂພສ ລາຍລະອຽດ ກ່ຽວກັບ ການລະບາດໃຫຍ່ ຂອງ ພະຍາດ ໂຄວິດ   ໃນ ຊ່ວງ ຕົ້ນ ປີ   2020   ລົງ ໃນ ລະບົບ ອິນເຕີເນັດ   ແລະ ສື່ ສັງຄົມ ອອນລາຍ ,   ເຊິ່ງ ໄດ້ ສ້າງ ຄວາມອັບອາຍ ໃຫ້ ກັບ ເຈົ້າໜ້າທີ່ ຈີນທີ່ ປະເຊີນ ກັບ ການຖືກ ຕິຕຽນ   ເນື່ອງຈາກ ປະສົບ ກັບ ຄວາມ ລົ້ມ ແຫຼວ ໃນ ການຄວບຄຸມ ການລະບາດ ຂອງ ພະຍາດ ດັ່ງກ່າວ\n",
      "Prediction: ທ່ານ ຟັງ ບິນ ແລະ ສະມາຊິກ ສັງຄົມ ພົນລະເຮືອນ ຄົນອື່ນ ໆ ທີ່ ຖືກ ຂະຫນານນາມ ວ່າເປັນ ນັກຂ່າວ ປະຊາຊົນ ໄດ້ ພິມ ເຜີຍແຜ່ ລາຍລະອຽດ ຂອງ ການລະບາດ ຂອງ ພະຍາດ ໂຄວິດ - 19 ໃນ ຕົ້ນ ປີ 2020 ຜ່ານ ທາງ ອິນເຕີແນັດ ແລະ ສື່ ສັງຄົມ ອອນລາຍ , ເຮັດໃຫ້ ເຈົ້າຫນ້າທີ່ ຈີນ ອັບອາຍ ຂາຍຫນ້າ , ຜູ້ ທີ່ ກໍາລັງ ປະເຊີນ ກັບ ການວິພາກວິຈານ ສໍາລັບ ການ ບໍ່ສາມາດ ຄວບຄຸມ ການລະບາດ ຂອງ ພະຍາດ ດັ່ງກ່າວ\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 9:\n",
      "Source    : 3 % , chủ_yếu là do người dân_nghèo , trong khi chính_phủ Lào cũng đang đối_mặt với thâm_hụt ngân_sách nghiêm_trọng , điều này cũng ảnh_hưởng đến kế_hoạch phát_triển giáo_dục trên cả nước , như ông Bounpan đã xác_nhận :\n",
      "Reference : 3   ເປີເຊັນ   ໂດຍ ມີ ສາເຫດ ສຳຄັນ ມາຈາກ ບັນຫາຍາກຈົນ ຂອງ ປະຊາຊົນ ໃນ ຂະນະທີ່ ລັດຖະບານ ລາວ   ກໍ ຍັງ   ປະເຊີນ ກັບ ບັນຫາ ຂາດແຄນ ດ້ານ ງົບປະມານ ຢ່າງ ໜັກໜ່ວງ ດ້ວຍ ແລ້ວ   ຈຶ່ງ ເຮັດໃຫ້ ກະທົບ ຕໍ່ ແຜນການພັດທະນາດ້ານ ການສຶກສາ ໃນ ທົ່ວ ປະເທດ ອີກ ດ້ວຍ   ດັ່ງທີ່ ທ່ານ ບຸນ ປັນ   ໄດ້ ຢືນຢັນ ວ່າ :\n",
      "Prediction: 3 ເປີເຊັນ ໂດຍ ສ່ວນໃຫຍ່ ເປັນ ຍ້ອນ ບັນຫາ ຄວາມຍາກຈົນ ຂອງ ປະຊາຊົນ ໃນ ຂະນະທີ່ ລັດຖະບານ ລາວ ກໍ ຕ້ອງ ປະເຊີນ ກັບ ການຂາດ ດຸນ ດ້ານ ງົບປະມານ ຢ່າງ ຫນັກຫນ່ວງ ດ້ວຍ ນັ້ນ ກໍ ຍັງ ໄດ້ ສົ່ງ ຜົນກະທົບ ຕໍ່ ແຜນການພັດທະນາການສຶກສາ ໃນ ທົ່ວ ປະເທດ ອີກ ດ້ວຍ ດັ່ງທີ່ ທ່ານ ບຸນ ປັນ ໄດ້ ໃຫ້ການຢືນຢັນ ວ່າ :\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 10:\n",
      "Source    : Tuy_nhiên , xét về chất_lượng cuộc_sống của trẻ_em dưới 5 tuổi , Lào đã bị tụt hạng nhẹ từ vị_trí thứ 19 năm 2010 xuống vị_trí thứ 22 trong số 44 quốc_gia kém phát_triển nhất . Tại châu Á , chỉ có Campuchia , Đông_Timor và Yemen xếp_hạng thấp hơn Lào .\n",
      "Reference : ແຕ່ ຢ່າງໃດ ກໍຕາມ   ສໍ າ ລັບ ໃນດ້ານ ຄຸນນະພາບ ຊີວິດ ຄວາມເປັນຢູ່ ຂອງ ເດັກນ້ອຍ ລາວ ອາຍຸ ຕໍ່າ   ກວ່າ   5   ປີນັ້ນ ພັດ ໄດ້ ຖືກ ປັບ ລົດ ອັນດັບ ໃຫ້ ຕໍ່າລົງ ເລັກ ໜ້ອຍ ກໍຄື ຈາກ ອັນ   ດັບ ທີ   19   ໃນ ປີ   2010   ມາເປັນ ອັນດັບ ທີ   22   ຈາກ   44   ປະເທດ ດ້ອຍ ພັດທະນາດ້ວຍກັນ   ແລະ ໃນ ອາຊີດ້ວຍກັນ ກໍ ປະກົດ ວ່າ   ມີ ພຽງ ກໍ າ ປູ ເຈຍ ,   ຕີ ມໍ ຕາເວັນອອກ   ແລະ ເຢເມນ ເທົ່ານັ້ນ ທີ່ມີ ອັນດັບ ຕໍ່າ ກວ່າ ລາວ\n",
      "Prediction: ແຕ່ ຢ່າງໃດ ກໍຕາມ ສໍ າ ລັບ ໃນດ້ານ ຄຸນນະພາບ ຊີວິດ ການເປັນຢູ່ ຂອງ ເດັກນ້ອຍ ລາວ ທີ່ ອາຍຸ ຕໍ່າ ກວ່າ 5 ປີນັ້ນ ກໍ ປະກົດ ວ່າ ລາວ ໄດ້ ຕົກຕໍ່າລົງ ເລັກນ້ອຍ ຈາກ ອັນດັບ ທີ 19 ໃນ ປີ 2010 ມາເປັນ ອັນດັບ ທີ 22 ໃນ ຈໍ າ ນວນ 44 ປະເທດ ດ້ອຍ ພັດທະນາ ແລະ ໃນ ເອເຊຍ ກໍ ປະກົດ ວ່າ ມີ ພຽງ ແຕ່ ປະເທດ ກໍ າ ປູ ເຈຍ , ຕີ ມໍ ຕາເວັນອອກ ແລະ ເຢເມນ ເທົ່ານັ້ນ ທີ່ມີ ອັນດັບ ຕໍ່າ ກວ່າ ລາວ\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 15: Sample Translations\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE TRANSLATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Source    : {test_vi[i]}\")\n",
    "    print(f\"Reference : {test_lo[i]}\")\n",
    "    print(f\"Prediction: {predictions[i]}\")\n",
    "    print(\"-\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "118792a2-48fa-44cf-a51e-fabcae930b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING RESULTS FROM ALL PHASES\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 16: Load All Phase Results\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING RESULTS FROM ALL PHASES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = {\n",
    "    \"phase1\": 23.28,\n",
    "    \"phase2\": 25.71,\n",
    "    \"phase3\": 26.92,\n",
    "    \"phase4\": bleu_score.score\n",
    "}\n",
    "\n",
    "# Try to load previous results\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"/work/phase2_predictions.txt\"):\n",
    "    with open(\"/work/phase2_predictions.txt\", encoding=\"utf-8\") as f:\n",
    "        phase2_preds = [l.strip() for l in f]\n",
    "    results[\"phase2\"] = corpus_bleu(phase2_preds, [test_lo]).score\n",
    "\n",
    "if os.path.exists(\"/work/phase3_predictions.tx\"):\n",
    "    with open(\"/work/phase3_predictions.tx\", encoding=\"utf-8\") as f:\n",
    "        phase3_preds = [l.strip() for l in f]\n",
    "    results[\"phase3\"] = corpus_bleu(phase3_preds, [test_lo]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8feb585d-a3dd-4746-baf9-46690474a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL RESULTS COMPARISON\n",
      "======================================================================\n",
      "Phase1 (no freeze)       : 23.28 BLEU\n",
      "Phase 2 (decoder only)     : 25.71 BLEU\n",
      "Phase 3 (progressive)      : 26.92 BLEU\n",
      "Phase 4 (full fine-tuning) : 28.72 BLEU\n",
      "\n",
      "Total improvement: +5.44 BLEU\n",
      "======================================================================\n",
      "\n",
      "All results saved to /work/phase4_full/all_results.json\n",
      "\n",
      "✓ All 4 phases completed successfully!\n",
      "\n",
      "======================================================================\n",
      "TRAINING PIPELINE FINISHED\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 17: Final Comparison\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Phase1 (no freeze)       : {results['phase1']:.2f} BLEU\")\n",
    "if results['phase2']:\n",
    "    print(f\"Phase 2 (decoder only)     : {results['phase2']:.2f} BLEU\")\n",
    "if results['phase3']:\n",
    "    print(f\"Phase 3 (progressive)      : {results['phase3']:.2f} BLEU\")\n",
    "print(f\"Phase 4 (full fine-tuning) : {results['phase4']:.2f} BLEU\")\n",
    "\n",
    "if results['phase2']:\n",
    "    improvement = results['phase4'] - results['phase1']\n",
    "    print(f\"\\nTotal improvement: +{improvement:.2f} BLEU\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save results\n",
    "with open(f\"{OUTPUT_DIR}/all_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nAll results saved to {OUTPUT_DIR}/all_results.json\")\n",
    "\n",
    "print(\"\\n✓ All 4 phases completed successfully!\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING PIPELINE FINISHED\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adfb5e4-f45d-434d-a6fb-86a85b03c6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed5022-2cdf-497c-918d-7d92698ed740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988b93da-5783-482b-b834-a430e896877b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1x GPU H200 SXM5",
   "language": "python",
   "name": "python-gpu-h200-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
