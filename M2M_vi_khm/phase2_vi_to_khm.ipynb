{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45656f5e-51f4-4995-879d-cbb4d8ae0db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.9.1)\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.57.3)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.10/site-packages (4.4.2)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.10/site-packages (1.12.0)\n",
      "Collecting pyvi\n",
      "  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.local/lib/python3.10/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.local/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.local/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.local/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.local/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.local/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.local/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.local/lib/python3.10/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.local/lib/python3.10/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.local/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.local/lib/python3.10/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.local/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.local/lib/python3.10/site-packages (from datasets) (0.70.18)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Downloading lxml-6.0.2-cp310-cp310-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (7.0.0)\n",
      "Collecting scikit-learn (from pyvi)\n",
      "  Downloading scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting sklearn-crfsuite (from pyvi)\n",
      "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.local/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2025.3)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn->pyvi)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->pyvi)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->pyvi)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->pyvi)\n",
      "  Downloading python_crfsuite-0.9.12-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Downloading sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading lxml-6.0.2-cp310-cp310-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m208.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Downloading scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m241.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading python_crfsuite-0.9.12-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m131.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m232.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, tabulate, sentencepiece, scipy, python-crfsuite, portalocker, lxml, joblib, colorama, scikit-learn, sacrebleu, sklearn-crfsuite, pyvi\n",
      "\u001b[33m  WARNING: The script tabulate is installed in '/home/admin/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script sacrebleu is installed in '/home/admin/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script pyvi is installed in '/home/admin/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed colorama-0.4.6 joblib-1.5.3 lxml-6.0.2 portalocker-3.2.0 python-crfsuite-0.9.12 pyvi-0.1.1 sacrebleu-2.5.1 scikit-learn-1.7.2 scipy-1.15.3 sentencepiece-0.2.1 sklearn-crfsuite-0.5.0 tabulate-0.9.0 threadpoolctl-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers sentencepiece datasets sacrebleu accelerate pyvi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b20a937-2520-4d69-b219-5e18e82b840b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA H200\n",
      "VRAM: 150.0217344 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup and imports\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    M2M100ForConditionalGeneration,\n",
    "    M2M100Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import random\n",
    "\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(\"VRAM:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54bc32a-965b-4cb8-9a7a-c60bb3307e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2831e17443704dbe9fe71fd7b69d4aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/196 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4223a3ddfc2d43fe883db112125314a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/164k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a47fa8d5994ca690626511be4af8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khmer tokenizer loaded successfully!\n",
      "Vietnamese and Khmer tokenizers loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Khmer tokenizer\n",
    "# Load Khmer tokenizer from Hugging Face\n",
    "khmer_word_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"khopilot/km-tokenizer-khmer\", \n",
    "    use_fast=False\n",
    ")\n",
    "print(\"Khmer tokenizer loaded successfully!\")\n",
    "\n",
    "# Cell 3b: Define tokenization functions\n",
    "def tokenize_vietnamese(text):\n",
    "    \"\"\"Tokenize Vietnamese text using PyVi\"\"\"\n",
    "    try:\n",
    "        return ViTokenizer.tokenize(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing Vietnamese: {e}\")\n",
    "        return text\n",
    "\n",
    "def tokenize_khmer(text):\n",
    "    \"\"\"Tokenize Khmer text using khopilot/km-tokenizer-khmer\"\"\"\n",
    "    try:\n",
    "        tokens = khmer_word_tokenizer.tokenize(text)\n",
    "        return \" \".join(tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing Khmer: {e}\")\n",
    "        return text\n",
    "\n",
    "def tokenize_batch_vietnamese(texts):\n",
    "    \"\"\"Batch tokenize Vietnamese texts\"\"\"\n",
    "    print(f\"Tokenizing {len(texts)} Vietnamese texts...\")\n",
    "    results = []\n",
    "    for i, text in enumerate(texts):\n",
    "        results.append(tokenize_vietnamese(text))\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(texts)} Vietnamese texts\")\n",
    "    return results\n",
    "\n",
    "def tokenize_batch_khmer(texts):\n",
    "    \"\"\"Batch tokenize Khmer texts\"\"\"\n",
    "    print(f\"Tokenizing {len(texts)} Khmer texts...\")\n",
    "    results = []\n",
    "    for i, text in enumerate(texts):\n",
    "        results.append(tokenize_khmer(text))\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(texts)} Khmer texts\")\n",
    "    return results\n",
    "\n",
    "print(\"Vietnamese and Khmer tokenizers loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21fbba61-05a1-4bd5-b57a-9d7838cd9ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 model loaded!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"vi_to_km/phase1/best\"\n",
    ").cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"vi_to_km/phase1/best\"\n",
    ")\n",
    "\n",
    "print(\"Phase 1 model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5ed2ec5-3340-4fa2-949c-388da06bb0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params    : 483.9M\n",
      "Frozen params   : 282.3M\n",
      "Trainable params: 201.6M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for param in model.model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Verify\n",
    "total, frozen, trainable = 0, 0, 0\n",
    "for name, param in model.named_parameters():\n",
    "    n = param.numel()\n",
    "    total += n\n",
    "    if not param.requires_grad:\n",
    "        frozen += n\n",
    "    else:\n",
    "        trainable += n\n",
    "\n",
    "print(f\"Total params    : {total/1e6:.1f}M\")\n",
    "print(f\"Frozen params   : {frozen/1e6:.1f}M\")\n",
    "print(f\"Trainable params: {trainable/1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "669a18ba-6f24-4200-b04b-8479ab2aecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Vietnamese texts...\n",
      "Tokenizing 599999 Vietnamese texts...\n",
      "  Processed 10000/599999 Vietnamese texts\n",
      "  Processed 20000/599999 Vietnamese texts\n",
      "  Processed 30000/599999 Vietnamese texts\n",
      "  Processed 40000/599999 Vietnamese texts\n",
      "  Processed 50000/599999 Vietnamese texts\n",
      "  Processed 60000/599999 Vietnamese texts\n",
      "  Processed 70000/599999 Vietnamese texts\n",
      "  Processed 80000/599999 Vietnamese texts\n",
      "  Processed 90000/599999 Vietnamese texts\n",
      "  Processed 100000/599999 Vietnamese texts\n",
      "  Processed 110000/599999 Vietnamese texts\n",
      "  Processed 120000/599999 Vietnamese texts\n",
      "  Processed 130000/599999 Vietnamese texts\n",
      "  Processed 140000/599999 Vietnamese texts\n",
      "  Processed 150000/599999 Vietnamese texts\n",
      "  Processed 160000/599999 Vietnamese texts\n",
      "  Processed 170000/599999 Vietnamese texts\n",
      "  Processed 180000/599999 Vietnamese texts\n",
      "  Processed 190000/599999 Vietnamese texts\n",
      "  Processed 200000/599999 Vietnamese texts\n",
      "  Processed 210000/599999 Vietnamese texts\n",
      "  Processed 220000/599999 Vietnamese texts\n",
      "  Processed 230000/599999 Vietnamese texts\n",
      "  Processed 240000/599999 Vietnamese texts\n",
      "  Processed 250000/599999 Vietnamese texts\n",
      "  Processed 260000/599999 Vietnamese texts\n",
      "  Processed 270000/599999 Vietnamese texts\n",
      "  Processed 280000/599999 Vietnamese texts\n",
      "  Processed 290000/599999 Vietnamese texts\n",
      "  Processed 300000/599999 Vietnamese texts\n",
      "  Processed 310000/599999 Vietnamese texts\n",
      "  Processed 320000/599999 Vietnamese texts\n",
      "  Processed 330000/599999 Vietnamese texts\n",
      "  Processed 340000/599999 Vietnamese texts\n",
      "  Processed 350000/599999 Vietnamese texts\n",
      "  Processed 360000/599999 Vietnamese texts\n",
      "  Processed 370000/599999 Vietnamese texts\n",
      "  Processed 380000/599999 Vietnamese texts\n",
      "  Processed 390000/599999 Vietnamese texts\n",
      "  Processed 400000/599999 Vietnamese texts\n",
      "  Processed 410000/599999 Vietnamese texts\n",
      "  Processed 420000/599999 Vietnamese texts\n",
      "  Processed 430000/599999 Vietnamese texts\n",
      "  Processed 440000/599999 Vietnamese texts\n",
      "  Processed 450000/599999 Vietnamese texts\n",
      "  Processed 460000/599999 Vietnamese texts\n",
      "  Processed 470000/599999 Vietnamese texts\n",
      "  Processed 480000/599999 Vietnamese texts\n",
      "  Processed 490000/599999 Vietnamese texts\n",
      "  Processed 500000/599999 Vietnamese texts\n",
      "  Processed 510000/599999 Vietnamese texts\n",
      "  Processed 520000/599999 Vietnamese texts\n",
      "  Processed 530000/599999 Vietnamese texts\n",
      "  Processed 540000/599999 Vietnamese texts\n",
      "  Processed 550000/599999 Vietnamese texts\n",
      "  Processed 560000/599999 Vietnamese texts\n",
      "  Processed 570000/599999 Vietnamese texts\n",
      "  Processed 580000/599999 Vietnamese texts\n",
      "  Processed 590000/599999 Vietnamese texts\n",
      "Tokenizing Khmer texts...\n",
      "Tokenizing 599999 Khmer texts...\n",
      "  Processed 10000/599999 Khmer texts\n",
      "  Processed 20000/599999 Khmer texts\n",
      "  Processed 30000/599999 Khmer texts\n",
      "  Processed 40000/599999 Khmer texts\n",
      "  Processed 50000/599999 Khmer texts\n",
      "  Processed 60000/599999 Khmer texts\n",
      "  Processed 70000/599999 Khmer texts\n",
      "  Processed 80000/599999 Khmer texts\n",
      "  Processed 90000/599999 Khmer texts\n",
      "  Processed 100000/599999 Khmer texts\n",
      "  Processed 110000/599999 Khmer texts\n",
      "  Processed 120000/599999 Khmer texts\n",
      "  Processed 130000/599999 Khmer texts\n",
      "  Processed 140000/599999 Khmer texts\n",
      "  Processed 150000/599999 Khmer texts\n",
      "  Processed 160000/599999 Khmer texts\n",
      "  Processed 170000/599999 Khmer texts\n",
      "  Processed 180000/599999 Khmer texts\n",
      "  Processed 190000/599999 Khmer texts\n",
      "  Processed 200000/599999 Khmer texts\n",
      "  Processed 210000/599999 Khmer texts\n",
      "  Processed 220000/599999 Khmer texts\n",
      "  Processed 230000/599999 Khmer texts\n",
      "  Processed 240000/599999 Khmer texts\n",
      "  Processed 250000/599999 Khmer texts\n",
      "  Processed 260000/599999 Khmer texts\n",
      "  Processed 270000/599999 Khmer texts\n",
      "  Processed 280000/599999 Khmer texts\n",
      "  Processed 290000/599999 Khmer texts\n",
      "  Processed 300000/599999 Khmer texts\n",
      "  Processed 310000/599999 Khmer texts\n",
      "  Processed 320000/599999 Khmer texts\n",
      "  Processed 330000/599999 Khmer texts\n",
      "  Processed 340000/599999 Khmer texts\n",
      "  Processed 350000/599999 Khmer texts\n",
      "  Processed 360000/599999 Khmer texts\n",
      "  Processed 370000/599999 Khmer texts\n",
      "  Processed 380000/599999 Khmer texts\n",
      "  Processed 390000/599999 Khmer texts\n",
      "  Processed 400000/599999 Khmer texts\n",
      "  Processed 410000/599999 Khmer texts\n",
      "  Processed 420000/599999 Khmer texts\n",
      "  Processed 430000/599999 Khmer texts\n",
      "  Processed 440000/599999 Khmer texts\n",
      "  Processed 450000/599999 Khmer texts\n",
      "  Processed 460000/599999 Khmer texts\n",
      "  Processed 470000/599999 Khmer texts\n",
      "  Processed 480000/599999 Khmer texts\n",
      "  Processed 490000/599999 Khmer texts\n",
      "  Processed 500000/599999 Khmer texts\n",
      "  Processed 510000/599999 Khmer texts\n",
      "  Processed 520000/599999 Khmer texts\n",
      "  Processed 530000/599999 Khmer texts\n",
      "  Processed 540000/599999 Khmer texts\n",
      "  Processed 550000/599999 Khmer texts\n",
      "  Processed 560000/599999 Khmer texts\n",
      "  Processed 570000/599999 Khmer texts\n",
      "  Processed 580000/599999 Khmer texts\n",
      "  Processed 590000/599999 Khmer texts\n",
      "Total dataset size: 599999 examples\n",
      "Train dataset: 595999 examples (for training)\n",
      "Dev dataset  : 3000 examples (for validation during training)\n",
      "Test dataset : 1000 examples (for final evaluation)\n",
      "\n",
      "Data split and shuffle completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Load and prepare data\n",
    "DATA_DIR = \"dataset\"\n",
    "\n",
    "def load_parallel(src_file, tgt_file):\n",
    "    with open(src_file, encoding=\"utf-8\") as f:\n",
    "        src = [l.strip() for l in f]\n",
    "    with open(tgt_file, encoding=\"utf-8\") as f:\n",
    "        tgt = [l.strip() for l in f]\n",
    "    \n",
    "    assert len(src) == len(tgt)\n",
    "\n",
    "    print(\"Tokenizing Vietnamese texts...\")\n",
    "    src_tokenized = tokenize_batch_vietnamese(src)\n",
    "    \n",
    "    print(\"Tokenizing Khmer texts...\")\n",
    "    tgt_tokenized = tokenize_batch_khmer(tgt) \n",
    "    \n",
    "    return Dataset.from_dict({\n",
    "        \"src_text\": src_tokenized,\n",
    "        \"tgt_text\": tgt_tokenized\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "# Load data from train.khm and train.vi\n",
    "full_dataset = load_parallel(\n",
    "    f\"{DATA_DIR}/train_khm.vi\",\n",
    "    f\"{DATA_DIR}/train_khm.khm\"\n",
    ")\n",
    "\n",
    "print(f\"Total dataset size: {len(full_dataset)} examples\")\n",
    "\n",
    "# Split dataset\n",
    "test_size = 1000\n",
    "dev_size = 3000\n",
    "\n",
    "test_start_idx = len(full_dataset) - test_size\n",
    "dev_start_idx = test_start_idx - dev_size\n",
    "\n",
    "test_dataset = full_dataset.select(range(test_start_idx, len(full_dataset)))\n",
    "dev_dataset = full_dataset.select(range(dev_start_idx, test_start_idx))\n",
    "train_dataset = full_dataset.select(range(0, dev_start_idx))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} examples (for training)\")\n",
    "print(f\"Dev dataset  : {len(dev_dataset)} examples (for validation during training)\")\n",
    "print(f\"Test dataset : {len(test_dataset)} examples (for final evaluation)\")\n",
    "print(\"\\nData split and shuffle completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f25cea6d-9112-470c-b670-b95a2decef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Preprocessing function\n",
    "MAX_LEN = 256\n",
    "\n",
    "def preprocess(batch):\n",
    "    tokenizer.src_lang = \"vi\"  # Changed from \"lo\" to \"km\" for Khmer\n",
    "    tokenizer.tgt_lang = \"km\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        batch[\"src_text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"tgt_text\"],\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e359669-5a43-41c8-bd08-ddf3e0fa302a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb53d7080f404543b7c022123e814a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/595999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8455d6829f8a4cf09452e9bdf0ebac90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Apply preprocessing\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=8\n",
    ")\n",
    "\n",
    "dev_dataset = dev_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dev_dataset.column_names,\n",
    "    num_proc=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af19f5c0-f5f5-4f92-b198-43e4437e1435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10a25016-366a-4367-b572-6817faf76108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Training arguments\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "training_args_phase2 = TrainingArguments(\n",
    "    output_dir=\"./vi_to_km/phase2\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.05,\n",
    "    weight_decay=0.005,\n",
    "    max_grad_norm=1.0,\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=8,\n",
    "    dataloader_pin_memory=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c35071e-3bf6-4cd8-a6e0-146ef1eba4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3827/3872925220.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_phase2 = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Create trainer\n",
    "trainer_phase2 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_phase2,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49ded4e1-2b42-406c-86c9-5bb70500ba4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 1: Training Vietnamese - Khmer model\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2329' max='2329' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2329/2329 08:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.896900</td>\n",
       "      <td>0.857996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.873300</td>\n",
       "      <td>0.835001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.853500</td>\n",
       "      <td>0.812967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.860800</td>\n",
       "      <td>0.798562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2329, training_loss=0.8810450012154618, metrics={'train_runtime': 494.728, 'train_samples_per_second': 1204.7, 'train_steps_per_second': 4.708, 'total_flos': 7.596092301798605e+16, 'train_loss': 0.8810450012154618, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 12: Train\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 1: Training Vietnamese - Khmer model\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "trainer_phase2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3d3abf9-ea15-4b46-8529-80336ed10fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 2 model saved!\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Save model\n",
    "trainer_phase2.save_model(\"./vi_to_km/phase2/best\")\n",
    "tokenizer.save_pretrained(\"./vi_to_km/phase2/best\")\n",
    "print(\"\\nPhase 2 model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df31b7e2-32c5-4f1a-ad5b-aef09eb69b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set size: 1000 examples\n",
      "\n",
      "Translating test set...\n",
      "Translated 320/1000\n",
      "Translated 640/1000\n",
      "Translated 960/1000\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Evaluation functions\n",
    "def translate_batch(texts, model, tokenizer, batch_size=32):\n",
    "    \"\"\"Batch translation for speed\"\"\"\n",
    "    model.eval()\n",
    "    tokenizer.src_lang = \"vi\"  # Changed from \"lo\" to \"km\"\n",
    "    tokenizer.tgt_lang = \"km\"\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=tokenizer.get_lang_id(\"km\"),\n",
    "                num_beams=5,\n",
    "                max_length=256\n",
    "            )\n",
    "        \n",
    "        texts_out = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "        outputs.extend(texts_out)\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"Translated {i+len(batch)}/{len(texts)}\")\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Get test data\n",
    "test_khm = test_dataset[\"tgt_text\"]\n",
    "test_vi = test_dataset[\"src_text\"]\n",
    "\n",
    "print(f\"\\nTest set size: {len(test_khm)} examples\")\n",
    "print(\"\\nTranslating test set...\")\n",
    "preds_phase2 = translate_batch(test_vi, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35787e10-8ee6-4525-b42d-0e33bcf3312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 1 BLEU Score: 19.67\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Calculate BLEU score\n",
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "bleu_phase2 = corpus_bleu(preds_phase2, [test_khm])\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PHASE 1 BLEU Score: {bleu_phase2.score:.2f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Save predictions\n",
    "with open(\"./vi_to_km/phase2/phase2_predictions_vi_km.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(preds_phase2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e66161f-fefd-438b-9949-efbba442c9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAMPLE TRANSLATIONS\n",
      "======================================================================\n",
      "\n",
      "Example 1:\n",
      "Source (Vietnamese): Ủy_ban bầu_cử Myanmar đã bác_bỏ tuyên_bố này .\n",
      "Reference (Khmer): ▁ គណៈកម្មការ រៀបចំការបោះឆ្នោត របស់ប្រទេស មីយ៉ាន់ម៉ា បាន ច្រ ា ន ចោល ការ អះអាង នេះ ។\n",
      "Prediction: នេះបើតាមការ បញ្ជាក់ របស់ គណៈកម្មការ រៀបចំការបោះឆ្នោត មីយ៉ាន់ម៉ា ។\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Source (Vietnamese): Bộ_trưởng nói : \" Phụ_nữ cũng muốn được lên_tiếng , được nêu lên những vấn_đề của chính mình . Trong quá_khứ , phụ_nữ nhận thấy rằng những người_làm chính_trị không thực_sự nêu lên những vấn_đề của chính họ vì bản_thân họ chưa từng đối_mặt với những vấn_đề đó . \"\n",
      "Reference (Khmer): ▁លោកស្រី រដ្ឋមន្ត្រី មានប្រសាសន៍ថា ៖ « ស្ត្រី ក៏ ចង់បាន ស្រ្តី ជា សំឡេង ខ្លួន ដែរ ▁ លើក បញ្ហា របស់ ខ្លួនឯង ▁ដែល កន្លងមក ស្ត្រី ឃើញថា ▁ អ្នកដែល ធ្វើ នយោបាយ ▁លោក មិនបាន លើក បញ្ហា ខ្លួន ឲ្យ ពិត ជាក់ស្តែង ▁ដោយសារ លោក មិនបាន ជួប បញ្ហា ហ្នឹង ដោយ ខ្លួន គាត់ ។\n",
      "Prediction: លោក រដ្ឋមន្រ្តី ថ្លែងថា ៖ « ស្ត្រី ក៏ ចង់ឲ្យ មានការ និយាយ ដោយ លើកឡើង ពី បញ្ហា ខ្លួនឯង កន្លងមក ស្ត្រី មើលឃើញ ថា អ្នក ធ្វើ នយោបាយ មិនបាន លើកឡើង ពី បញ្ហា ខ្លួនឯង ពិតប្រាកដ ទេ ពីព្រោះ ពួកគេ មិន ដែលបាន ប្រឈមមុខ នឹង បញ្ហា ទាំងនោះ ។\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Source (Vietnamese): Thầy Om Phearum cho biết vì thầy dạy toán nên thầy hướng_dẫn học_sinh_học môn này vào lúc 7 giờ sáng trong khoảng 45 phút , theo một video do một giáo_viên khác quay .\n",
      "Reference (Khmer): ▁ លោកគ្រូ ▁ អ៊ ុំ ▁ ភា រ ម ្យ បន្តថា ▁ដោយសារ លោក បង្រៀន គណ ិត វិទ្យា ▁ដូច្នេះ លោក ដឹកនាំ សិស្ស រៀន មុខ វិជ្ជា នេះ នៅពេល ព្រឹក ម៉ោង ៧ ▁ ប្រហែលជា ៤៥ នាទី ▁ ទៅតាម វីដេអូ ដែល ថត បង្រៀន ដោយ គ្រូ ផ្សេង ។\n",
      "Prediction: លោកគ្រូ ឱ ម ភា រ ម ្យ មានប្រសាសន៍ថា ដោយសារ លោកគ្រូ ជា គ្រូបង្រៀន គណ ិត វិទ្យា លោកគ្រូ បាន ណែនាំ សិស្ស ឲ្យ រៀន វិជ្ជា នេះ នៅម៉ោង ៧ ព្រឹក រយៈពេល ប្រហែល ៤៥ នាទី នេះបើយោងតាម វីដេអូ ដែល ថត ដោយ គ្រូ ម្នាក់ទៀត ។\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "Source (Vietnamese): Thủ_tướng Hun_Sen hôm thứ Tư đã gọi tuyên_bố của Sam Rainsy thông_qua video là sự liên_hệ đến video của Kem_Sokha , người đang bị buộc_tội phản_quốc vì tiết_lộ kế_hoạch lật_đổ chính_phủ với sự thông_đồng của Hoa_Kỳ .\n",
      "Reference (Khmer): ▁លោក នាយករដ្ឋមន្រ្តី ▁ហ៊ុន ▁សែន ▁ កាលពីថ្ងៃពុធ ▁បាន ហៅ ការ ថ្លែង របស់លោក ▁សម ▁រង្ស៊ី ▁តាមរយៈ វីដេអូ នេះ ថា ▁ ជាការ ផ្សារ ភ្ជាប់ ទៅនឹង វីដេអូ របស់ ▁លោក ▁កឹម ▁សុខា ▁ ដែលកំពុង ជាប់ ចោទ ពីបទ ក ្ បត់ ជាតិ ▁ ព្រោះតែ បង្ហាញ គម្រោង ផ្ ត ួល រំ លំ រាជរដ្ឋាភិបាល ▁ដោយ ឃ ុប ឃ ិត ជាមួយ សហរដ្ឋអាមេរិក ។\n",
      "Prediction: លោក នាយករដ្ឋមន្ត្រី ហ៊ុន សែន នៅ ថ្ងៃពុធ នេះ បាន ហៅ ស េចក្តីថ្លែងការណ៍ របស់លោក សម រង្ស៊ី តាមរយៈ វីដេអូ នោះ ថា ជាការ ទាក់ទងនឹង វីដេអូ របស់លោក កឹម សុខា ដែលកំពុង រង ការចោទប្រកាន់ ពីបទ ក ្ បត់ ជាតិ ពីបទ បង្ហាញ ផែនការ ផ្ ត ួល រំ លំ រដ្ឋាភិបាល ដោយ មានការ ឯកភាព ពី សហរដ្ឋអាមេរិក ។\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "Source (Vietnamese): Tuần trước , truyền_thông nhà_nước Trung_Quốc đưa tin rằng Bắc_Kinh hy_vọng sẽ hoàn_tất bộ quy_tắc ứng_xử cho Biển Đông vốn bị trì_hoãn từ lâu trong năm 2017 .\n",
      "Reference (Khmer): ▁ សារព័ត៌មាន រដ្ឋ របស់ ចិន កាលពី សប្តាហ៍ មុន បាន រាយការណ៍ ថា ▁ រដ្ឋាភិបាល ក្រុង ប៉េ ក ាំង សង្ឃឹមថា ▁នឹង សម្រេច ក្រម ប្រតិបត្តិ ដែលត្រូវបាន ពន្ យា រ ពេល យ៉ាង យូរ សម្រាប់ ស មុទ្រចិនខាងត្បូង ▁នៅក្នុង ឆ្នាំ២០១៧ ខាងមុខនេះ ។\n",
      "Prediction: កាលពី សប្តាហ៍ មុន ប្រព័ន្ធ ផ្សព្វផ្សាយ រដ្ឋ របស់ ចិន បាន រាយការណ៍ ថា រដ្ឋាភិបាល ក្រុង ប៉េ ក ាំង សង្ឃឹមថា នឹង បញ្ចប់ ក្រម ប្រតិបត្តិ សម្រាប់ ស មុទ្រចិនខាងត្បូង ដែលត្រូវបាន ពន្ យា រ ពេល ជា យូរ មកហើយ នៅក្នុង ឆ្នាំ២០១៧ ។\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 6:\n",
      "Source (Vietnamese): Trong bối_cảnh Campuchia đang hướng tới cuộc bầu_cử quốc_hội sắp tới , ông Say Chhum cũng đề_cập trong thông_điệp của mình rằng người dân Campuchia sẽ có quyền thành_lập Quốc_hội , từ đó dẫn đến việc bầu ra một chính_phủ mới .\n",
      "Reference (Khmer): ▁ដោយ ហេតុ ថា ▁ កម្ពុជា ក៏ កំពុង ឆ្ពោះទៅ រក ការបោះឆ្នោត ជ្រើសតាំង តំណាងរាស្ត្រ នៅពេល ខាងមុខនេះ ▁លោក ▁ សាយ ▁ ឈ ុំ ▁ក៏ បានលើកឡើង ក្នុង សារ លិខិត របស់លោក ថា ▁ ប្រជាពលរដ្ឋខ្មែរ ▁ នឹងមាន សិទ្ធិ ក្នុងការ បង្កើត រដ្ឋសភា ដែល នឹង នាំ ដល់ ការ ជ្រើសតាំង រដ្ឋាភិបាល ថ្មីមួយ ។\n",
      "Prediction: ស្រប ពេលដែល កម្ពុជា កំពុង ឆ្ពោះទៅ រក ការបោះឆ្នោត ជ្រើសតាំង តំណាង រា ស្ដ ្ រ នាពេលខាងមុខ នេះ លោក សាយ ឈ ុំ ក៏ បានលើកឡើង នៅក្នុង សារ របស់លោក ថា ប្រជាពលរដ្ឋ កម្ពុជា នឹងមាន សិទ្ធិ បង្កើត រដ្ឋសភា ដែល នាំឲ្យ មាន ការបោះឆ្នោត ជ្រើសរើស រដ្ឋាភិបាល ថ្មី ។\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 7:\n",
      "Source (Vietnamese): \" Họ nói họ muốn đánh_dấu vào phiếu bầu , nhưng họ không_thể . \"\n",
      "Reference (Khmer): ▁ ថា ចង់ គូ ស សន្លឹកឆ្នោត ▁មិន ឲ្យបាន ការ » ។\n",
      "Prediction: ពួកគេ និយាយថា ពួកគេ ចង់ គូ ស សន្លឹកឆ្នោត ប៉ុន្តែ ពួកគេ មិនអាច » ។\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 8:\n",
      "Source (Vietnamese): Đây không phải là lần đầu_tiên bà Sithar bị bắt vì đòi quyền_lợi cho người lao_động .\n",
      "Reference (Khmer): ▁ នេះ មិនមែនជា លើកទី ▁១ ▁ ទេ ដែល ▁ ស៊ី ថ រ ▁ត្រូវបាន ចាប់ខ្លួន ដោយសារ ការ ទាមទារ សិទ្ធិ ▁និង អត្ថប្រយោជន៍ ឲ្យ បុគ្គលិក ។\n",
      "Prediction: នេះ មិនមែនជា លើកទី មួយ ទេ ដែល អ្នកស្រី ស៊ី ថ រ ត្រូវបាន ចាប់ខ្លួន ដោយសារតែ ការ ទាមទារ សិទ្ធិ ដល់ កម្មករ ។\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 9:\n",
      "Source (Vietnamese): Sự gia_tăng bạo_lực dựa trên giới_tính được gọi là một đại_dịch tiềm_ẩn .\n",
      "Reference (Khmer): ▁បាន ហៅ ការ កើនឡើង នៃ អំពើហិង្សា ផ្អែក លើ យ៉ ែន ឌ ័រ នេះ ថា ▁ ជាការ រា ត ត ្ បាត ដោយ លាក់ កំ បាំង ។\n",
      "Prediction: ការ កើនឡើង នៃ អំពើហិង្សា ផ្អែក លើ យ េន ឌ ័រ ត្រូវបានគេ ស្គាល់ ថា ជា ជំងឺ រា ត ត ្ បាត ដែលអាច កើតមានឡើង ។\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 10:\n",
      "Source (Vietnamese): \" Trưởng lâm_nghiệp đã lấy nó rồi ... việc này không liên_quan gì đến chúng_tôi cả . \"\n",
      "Reference (Khmer): ▁ មេ ព្រៃ គេ ទៅ យក . ... អត់ មាន ពាក់ព័ន្ធ អី ជាមួយ ពួក ខ្ញុំ ផង » ។\n",
      "Prediction: « មេ ព្រៃឈើ គាត់ យក ហើយ ... វា អត់ មាន ពាក់ព័ន្ធនឹង យើង ទេ ។\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Show sample translations\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE TRANSLATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Source (Vietnamese): {test_vi[i]}\")\n",
    "    print(f\"Reference (Khmer): {test_khm[i]}\")\n",
    "    print(f\"Prediction: {preds_phase2[i]}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474fe58c-7af2-4b00-ba63-0a3fe85529aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714bf3fc-e284-4b3d-b817-79ae1eb5253b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
