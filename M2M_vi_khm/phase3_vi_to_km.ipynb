{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a8c21a5-f06f-4187-b4a3-bc99aafef9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.9.1)\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.57.3)\n",
      "Requirement already satisfied: sentencepiece in ./.local/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.10/site-packages (4.4.2)\n",
      "Requirement already satisfied: sacrebleu in ./.local/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.10/site-packages (1.12.0)\n",
      "Requirement already satisfied: pyvi in ./.local/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.local/lib/python3.10/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.local/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.local/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.local/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.local/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.local/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.local/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.local/lib/python3.10/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.local/lib/python3.10/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.local/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.local/lib/python3.10/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.local/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.local/lib/python3.10/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: portalocker in ./.local/lib/python3.10/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in ./.local/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in ./.local/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in ./.local/lib/python3.10/site-packages (from sacrebleu) (6.0.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.10/site-packages (from pyvi) (1.7.2)\n",
      "Requirement already satisfied: sklearn-crfsuite in ./.local/lib/python3.10/site-packages (from pyvi) (0.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.local/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.local/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.local/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.local/lib/python3.10/site-packages (from scikit-learn->pyvi) (3.6.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.7 in ./.local/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (0.9.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers sentencepiece datasets sacrebleu accelerate pyvi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cfc5fcf-4be3-4f05-923c-ea600a292b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA H200\n",
      "VRAM: 150.0217344 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup and imports\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    M2M100ForConditionalGeneration,\n",
    "    M2M100Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import random\n",
    "\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(\"VRAM:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4038682a-6b87-4c9e-bf62-483cca34489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khmer tokenizer loaded successfully!\n",
      "Vietnamese and Khmer tokenizers loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Khmer tokenizer\n",
    "# Load Khmer tokenizer from Hugging Face\n",
    "khmer_word_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"khopilot/km-tokenizer-khmer\", \n",
    "    use_fast=False\n",
    ")\n",
    "print(\"Khmer tokenizer loaded successfully!\")\n",
    "\n",
    "# Cell 3b: Define tokenization functions\n",
    "def tokenize_vietnamese(text):\n",
    "    \"\"\"Tokenize Vietnamese text using PyVi\"\"\"\n",
    "    try:\n",
    "        return ViTokenizer.tokenize(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing Vietnamese: {e}\")\n",
    "        return text\n",
    "\n",
    "def tokenize_khmer(text):\n",
    "    \"\"\"Tokenize Khmer text using khopilot/km-tokenizer-khmer\"\"\"\n",
    "    try:\n",
    "        tokens = khmer_word_tokenizer.tokenize(text)\n",
    "        return \" \".join(tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing Khmer: {e}\")\n",
    "        return text\n",
    "\n",
    "def tokenize_batch_vietnamese(texts):\n",
    "    \"\"\"Batch tokenize Vietnamese texts\"\"\"\n",
    "    print(f\"Tokenizing {len(texts)} Vietnamese texts...\")\n",
    "    results = []\n",
    "    for i, text in enumerate(texts):\n",
    "        results.append(tokenize_vietnamese(text))\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(texts)} Vietnamese texts\")\n",
    "    return results\n",
    "\n",
    "def tokenize_batch_khmer(texts):\n",
    "    \"\"\"Batch tokenize Khmer texts\"\"\"\n",
    "    print(f\"Tokenizing {len(texts)} Khmer texts...\")\n",
    "    results = []\n",
    "    for i, text in enumerate(texts):\n",
    "        results.append(tokenize_khmer(text))\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(texts)} Khmer texts\")\n",
    "    return results\n",
    "\n",
    "print(\"Vietnamese and Khmer tokenizers loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e32971b-df08-4c92-92bd-6eeac1009270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 2 model loaded!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"vi_to_km/phase2/best\"\n",
    ").cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"vi_to_km/phase2/best\"\n",
    ")\n",
    "\n",
    "print(\"Phase 2 model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "185d4593-a494-4605-93df-d7c551397af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params       : 483.9M\n",
      "Frozen params      : 206.8M\n",
      "Trainable params   : 277.1M\n",
      "Encoder trainable  : 126.0M\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Unfreeze Top 6 Encoder Layers (layers 6-11)\n",
    "# ============================================================\n",
    "# 1. Freeze toàn bộ encoder trước\n",
    "for param in model.model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. Unfreeze top 6 layers\n",
    "for i in range(6, 12):\n",
    "    for param in model.model.encoder.layers[i].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Verify\n",
    "total, frozen, trainable = 0, 0, 0\n",
    "encoder_trainable = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    n = param.numel()\n",
    "    total += n\n",
    "    if not param.requires_grad:\n",
    "        frozen += n\n",
    "    else:\n",
    "        trainable += n\n",
    "        if \"encoder\" in name:\n",
    "            encoder_trainable += n\n",
    "\n",
    "print(f\"Total params       : {total/1e6:.1f}M\")\n",
    "print(f\"Frozen params      : {frozen/1e6:.1f}M\")\n",
    "print(f\"Trainable params   : {trainable/1e6:.1f}M\")\n",
    "print(f\"Encoder trainable  : {encoder_trainable/1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0933c69c-a773-4ccd-a92b-6198d509785e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Vietnamese texts...\n",
      "Tokenizing 599999 Vietnamese texts...\n",
      "  Processed 10000/599999 Vietnamese texts\n",
      "  Processed 20000/599999 Vietnamese texts\n",
      "  Processed 30000/599999 Vietnamese texts\n",
      "  Processed 40000/599999 Vietnamese texts\n",
      "  Processed 50000/599999 Vietnamese texts\n",
      "  Processed 60000/599999 Vietnamese texts\n",
      "  Processed 70000/599999 Vietnamese texts\n",
      "  Processed 80000/599999 Vietnamese texts\n",
      "  Processed 90000/599999 Vietnamese texts\n",
      "  Processed 100000/599999 Vietnamese texts\n",
      "  Processed 110000/599999 Vietnamese texts\n",
      "  Processed 120000/599999 Vietnamese texts\n",
      "  Processed 130000/599999 Vietnamese texts\n",
      "  Processed 140000/599999 Vietnamese texts\n",
      "  Processed 150000/599999 Vietnamese texts\n",
      "  Processed 160000/599999 Vietnamese texts\n",
      "  Processed 170000/599999 Vietnamese texts\n",
      "  Processed 180000/599999 Vietnamese texts\n",
      "  Processed 190000/599999 Vietnamese texts\n",
      "  Processed 200000/599999 Vietnamese texts\n",
      "  Processed 210000/599999 Vietnamese texts\n",
      "  Processed 220000/599999 Vietnamese texts\n",
      "  Processed 230000/599999 Vietnamese texts\n",
      "  Processed 240000/599999 Vietnamese texts\n",
      "  Processed 250000/599999 Vietnamese texts\n",
      "  Processed 260000/599999 Vietnamese texts\n",
      "  Processed 270000/599999 Vietnamese texts\n",
      "  Processed 280000/599999 Vietnamese texts\n",
      "  Processed 290000/599999 Vietnamese texts\n",
      "  Processed 300000/599999 Vietnamese texts\n",
      "  Processed 310000/599999 Vietnamese texts\n",
      "  Processed 320000/599999 Vietnamese texts\n",
      "  Processed 330000/599999 Vietnamese texts\n",
      "  Processed 340000/599999 Vietnamese texts\n",
      "  Processed 350000/599999 Vietnamese texts\n",
      "  Processed 360000/599999 Vietnamese texts\n",
      "  Processed 370000/599999 Vietnamese texts\n",
      "  Processed 380000/599999 Vietnamese texts\n",
      "  Processed 390000/599999 Vietnamese texts\n",
      "  Processed 400000/599999 Vietnamese texts\n",
      "  Processed 410000/599999 Vietnamese texts\n",
      "  Processed 420000/599999 Vietnamese texts\n",
      "  Processed 430000/599999 Vietnamese texts\n",
      "  Processed 440000/599999 Vietnamese texts\n",
      "  Processed 450000/599999 Vietnamese texts\n",
      "  Processed 460000/599999 Vietnamese texts\n",
      "  Processed 470000/599999 Vietnamese texts\n",
      "  Processed 480000/599999 Vietnamese texts\n",
      "  Processed 490000/599999 Vietnamese texts\n",
      "  Processed 500000/599999 Vietnamese texts\n",
      "  Processed 510000/599999 Vietnamese texts\n",
      "  Processed 520000/599999 Vietnamese texts\n",
      "  Processed 530000/599999 Vietnamese texts\n",
      "  Processed 540000/599999 Vietnamese texts\n",
      "  Processed 550000/599999 Vietnamese texts\n",
      "  Processed 560000/599999 Vietnamese texts\n",
      "  Processed 570000/599999 Vietnamese texts\n",
      "  Processed 580000/599999 Vietnamese texts\n",
      "  Processed 590000/599999 Vietnamese texts\n",
      "Tokenizing Khmer texts...\n",
      "Tokenizing 599999 Khmer texts...\n",
      "  Processed 10000/599999 Khmer texts\n",
      "  Processed 20000/599999 Khmer texts\n",
      "  Processed 30000/599999 Khmer texts\n",
      "  Processed 40000/599999 Khmer texts\n",
      "  Processed 50000/599999 Khmer texts\n",
      "  Processed 60000/599999 Khmer texts\n",
      "  Processed 70000/599999 Khmer texts\n",
      "  Processed 80000/599999 Khmer texts\n",
      "  Processed 90000/599999 Khmer texts\n",
      "  Processed 100000/599999 Khmer texts\n",
      "  Processed 110000/599999 Khmer texts\n",
      "  Processed 120000/599999 Khmer texts\n",
      "  Processed 130000/599999 Khmer texts\n",
      "  Processed 140000/599999 Khmer texts\n",
      "  Processed 150000/599999 Khmer texts\n",
      "  Processed 160000/599999 Khmer texts\n",
      "  Processed 170000/599999 Khmer texts\n",
      "  Processed 180000/599999 Khmer texts\n",
      "  Processed 190000/599999 Khmer texts\n",
      "  Processed 200000/599999 Khmer texts\n",
      "  Processed 210000/599999 Khmer texts\n",
      "  Processed 220000/599999 Khmer texts\n",
      "  Processed 230000/599999 Khmer texts\n",
      "  Processed 240000/599999 Khmer texts\n",
      "  Processed 250000/599999 Khmer texts\n",
      "  Processed 260000/599999 Khmer texts\n",
      "  Processed 270000/599999 Khmer texts\n",
      "  Processed 280000/599999 Khmer texts\n",
      "  Processed 290000/599999 Khmer texts\n",
      "  Processed 300000/599999 Khmer texts\n",
      "  Processed 310000/599999 Khmer texts\n",
      "  Processed 320000/599999 Khmer texts\n",
      "  Processed 330000/599999 Khmer texts\n",
      "  Processed 340000/599999 Khmer texts\n",
      "  Processed 350000/599999 Khmer texts\n",
      "  Processed 360000/599999 Khmer texts\n",
      "  Processed 370000/599999 Khmer texts\n",
      "  Processed 380000/599999 Khmer texts\n",
      "  Processed 390000/599999 Khmer texts\n",
      "  Processed 400000/599999 Khmer texts\n",
      "  Processed 410000/599999 Khmer texts\n",
      "  Processed 420000/599999 Khmer texts\n",
      "  Processed 430000/599999 Khmer texts\n",
      "  Processed 440000/599999 Khmer texts\n",
      "  Processed 450000/599999 Khmer texts\n",
      "  Processed 460000/599999 Khmer texts\n",
      "  Processed 470000/599999 Khmer texts\n",
      "  Processed 480000/599999 Khmer texts\n",
      "  Processed 490000/599999 Khmer texts\n",
      "  Processed 500000/599999 Khmer texts\n",
      "  Processed 510000/599999 Khmer texts\n",
      "  Processed 520000/599999 Khmer texts\n",
      "  Processed 530000/599999 Khmer texts\n",
      "  Processed 540000/599999 Khmer texts\n",
      "  Processed 550000/599999 Khmer texts\n",
      "  Processed 560000/599999 Khmer texts\n",
      "  Processed 570000/599999 Khmer texts\n",
      "  Processed 580000/599999 Khmer texts\n",
      "  Processed 590000/599999 Khmer texts\n",
      "Total dataset size: 599999 examples\n",
      "Train dataset: 595999 examples (for training)\n",
      "Dev dataset  : 3000 examples (for validation during training)\n",
      "Test dataset : 1000 examples (for final evaluation)\n",
      "\n",
      "Data split and shuffle completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Load and prepare data\n",
    "DATA_DIR = \"dataset\"\n",
    "\n",
    "def load_parallel(src_file, tgt_file):\n",
    "    with open(src_file, encoding=\"utf-8\") as f:\n",
    "        src = [l.strip() for l in f]\n",
    "    with open(tgt_file, encoding=\"utf-8\") as f:\n",
    "        tgt = [l.strip() for l in f]\n",
    "    \n",
    "    assert len(src) == len(tgt)\n",
    "\n",
    "    print(\"Tokenizing Vietnamese texts...\")\n",
    "    src_tokenized = tokenize_batch_vietnamese(src)\n",
    "    \n",
    "    print(\"Tokenizing Khmer texts...\")\n",
    "    tgt_tokenized = tokenize_batch_khmer(tgt) \n",
    "    \n",
    "    return Dataset.from_dict({\n",
    "        \"src_text\": src_tokenized,\n",
    "        \"tgt_text\": tgt_tokenized\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "# Load data from train.khm and train.vi\n",
    "full_dataset = load_parallel(\n",
    "    f\"{DATA_DIR}/train_khm.vi\",\n",
    "    f\"{DATA_DIR}/train_khm.khm\"\n",
    ")\n",
    "\n",
    "print(f\"Total dataset size: {len(full_dataset)} examples\")\n",
    "\n",
    "# Split dataset\n",
    "test_size = 1000\n",
    "dev_size = 3000\n",
    "\n",
    "test_start_idx = len(full_dataset) - test_size\n",
    "dev_start_idx = test_start_idx - dev_size\n",
    "\n",
    "test_dataset = full_dataset.select(range(test_start_idx, len(full_dataset)))\n",
    "dev_dataset = full_dataset.select(range(dev_start_idx, test_start_idx))\n",
    "train_dataset = full_dataset.select(range(0, dev_start_idx))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} examples (for training)\")\n",
    "print(f\"Dev dataset  : {len(dev_dataset)} examples (for validation during training)\")\n",
    "print(f\"Test dataset : {len(test_dataset)} examples (for final evaluation)\")\n",
    "print(\"\\nData split and shuffle completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ad652-9554-4e60-b301-128f5cc3feff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "741ba8ab-4a4b-4822-a02c-aac3aa21bad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Preprocessing function\n",
    "MAX_LEN = 256\n",
    "\n",
    "def preprocess(batch):\n",
    "    tokenizer.src_lang = \"vi\"  # Changed from \"lo\" to \"km\" for Khmer\n",
    "    tokenizer.tgt_lang = \"km\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        batch[\"src_text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"tgt_text\"],\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58643cb0-efa6-421f-8df9-f23f8ea5caa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e475ddf896b24f5e91e04e385455d013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/595999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc77a7b1c994d13a495946b08380804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Apply preprocessing\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=8\n",
    ")\n",
    "\n",
    "dev_dataset = dev_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dev_dataset.column_names,\n",
    "    num_proc=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2b6d598-33d9-4cc7-af89-05809dcc82a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "907ead99-1041-4aab-8877-bfa90ce485d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# ============================================================\n",
    "# CELL 3: Training Arguments - Phase 3\n",
    "# ============================================================\n",
    "training_args_phase3 = TrainingArguments(\n",
    "    output_dir=\"./vi_to_km/phase3\",\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=100,\n",
    "    \n",
    "    # Batch size\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    \n",
    "    # Learning rate - LOWER than phase 2\n",
    "    learning_rate=3e-5,  # Lower for fine-tuning encoder\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    warmup_ratio=0.05,\n",
    "    \n",
    "    # Regularization\n",
    "    weight_decay=0.015,\n",
    "    max_grad_norm=0.8,\n",
    "    \n",
    "    # Precision\n",
    "    num_train_epochs=6,\n",
    "    \n",
    "    # FP16\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "\n",
    "    # Speed\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=8,\n",
    "    dataloader_pin_memory=True,\n",
    "    \n",
    "    # Best model\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "737b8522-e9d3-4b11-b687-f1d4644fa31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4769/2611886879.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_phase3 = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Create Trainer - Phase 3\n",
    "# ============================================================\n",
    "trainer_phase3 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_phase3,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=4)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cb82219-db62-465b-99e6-d8bd9afb19c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 3: Progressive Unfreezing (Top 6 Encoder Layers)\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13974' max='13974' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13974/13974 55:10, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.756700</td>\n",
       "      <td>0.813537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.845100</td>\n",
       "      <td>0.795820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.831200</td>\n",
       "      <td>0.781352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.836800</td>\n",
       "      <td>0.769065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>0.757109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.747858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.785800</td>\n",
       "      <td>0.738221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.782300</td>\n",
       "      <td>0.732611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.773200</td>\n",
       "      <td>0.729387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.744800</td>\n",
       "      <td>0.722806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.741700</td>\n",
       "      <td>0.717752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.742700</td>\n",
       "      <td>0.715399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.733500</td>\n",
       "      <td>0.711062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.722700</td>\n",
       "      <td>0.704688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.710600</td>\n",
       "      <td>0.704180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.709500</td>\n",
       "      <td>0.699987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.706900</td>\n",
       "      <td>0.696737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.704100</td>\n",
       "      <td>0.695472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.701000</td>\n",
       "      <td>0.693227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.691919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.690120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.686300</td>\n",
       "      <td>0.687975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.686400</td>\n",
       "      <td>0.687499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.679900</td>\n",
       "      <td>0.687465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.679700</td>\n",
       "      <td>0.686469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.676900</td>\n",
       "      <td>0.686303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.680200</td>\n",
       "      <td>0.686952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13974, training_loss=0.7373052697777492, metrics={'train_runtime': 3312.2724, 'train_samples_per_second': 1079.62, 'train_steps_per_second': 4.219, 'total_flos': 4.558102917807882e+17, 'train_loss': 0.7373052697777492, 'epoch': 6.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Train Phase 3\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 3: Progressive Unfreezing (Top 6 Encoder Layers)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "trainer_phase3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6e5fa3b-b88d-4a08-b9ec-7e5f480027fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 3 model saved!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Save Phase 3 Model\n",
    "# ============================================================\n",
    "trainer_phase3.save_model(\"./vi_to_km/phase3/best\")\n",
    "tokenizer.save_pretrained(\"./vi_to_km/phase3/best\")\n",
    "\n",
    "print(\"\\nPhase 3 model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c91f610-b2e1-49c6-ad71-ba56b952a14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 3 model loaded!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"vi_to_km/phase3/best\"\n",
    ").cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"vi_to_km/phase3/best\"\n",
    ")\n",
    "\n",
    "print(\"Phase 3 model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c57565-2698-48c0-8285-04f2745f1bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set size: 1000 examples\n",
      "\n",
      "Translating test set...\n",
      "Translated 320/1000\n",
      "Translated 640/1000\n",
      "Translated 960/1000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Evaluate Phase 3\n",
    "# ============================================================\n",
    "\n",
    "def translate_batch(texts, model, tokenizer, batch_size=32):\n",
    "    \"\"\"Batch translation for speed\"\"\"\n",
    "    model.eval()\n",
    "    tokenizer.src_lang = \"vi\"\n",
    "    tokenizer.tgt_lang = \"km\"\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=tokenizer.get_lang_id(\"km\"),\n",
    "                num_beams=5,\n",
    "                max_length=256\n",
    "            )\n",
    "        \n",
    "        texts_out = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "        outputs.extend(texts_out)\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"Translated {i+len(batch)}/{len(texts)}\")\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Lấy dữ liệu test từ test_dataset (đã chia từ train.vi/train.lo)\n",
    "test_vi = test_dataset[\"src_text\"]\n",
    "test_khm = test_dataset[\"tgt_text\"]\n",
    "\n",
    "print(f\"\\nTest set size: {len(test_vi)} examples\")\n",
    "\n",
    "# Translate\n",
    "print(\"\\nTranslating test set...\")\n",
    "preds_phase3 = translate_batch(test_vi, model, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6d44262-cd8e-4ae5-8248-593d910bb3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 3 BLEU Score: 22.55\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "# Calculate BLEU\n",
    "bleu_phase3 = corpus_bleu(preds_phase3, [test_khm])\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PHASE 3 BLEU Score: {bleu_phase3.score:.2f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Save predictions\n",
    "with open(\"./vi_to_km/phase3/phase3_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(preds_phase3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c35efa81-634a-4f0c-b0a2-cc65b90b58d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAMPLE TRANSLATIONS\n",
      "======================================================================\n",
      "\n",
      "Example 1:\n",
      "Source    : Ủy_ban bầu_cử Myanmar đã bác_bỏ tuyên_bố này .\n",
      "Reference : ▁ គណៈកម្មការ រៀបចំការបោះឆ្នោត របស់ប្រទេស មីយ៉ាន់ម៉ា បាន ច្រ ា ន ចោល ការ អះអាង នេះ ។\n",
      "Prediction: ការ អះអាង នេះត្រូវបាន គណៈកម្មការ រៀបចំការបោះឆ្នោត មីយ៉ាន់ម៉ា ច្រ ា ន ចោល ។\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Source    : Bộ_trưởng nói : \" Phụ_nữ cũng muốn được lên_tiếng , được nêu lên những vấn_đề của chính mình . Trong quá_khứ , phụ_nữ nhận thấy rằng những người_làm chính_trị không thực_sự nêu lên những vấn_đề của chính họ vì bản_thân họ chưa từng đối_mặt với những vấn_đề đó . \"\n",
      "Reference : ▁លោកស្រី រដ្ឋមន្ត្រី មានប្រសាសន៍ថា ៖ « ស្ត្រី ក៏ ចង់បាន ស្រ្តី ជា សំឡេង ខ្លួន ដែរ ▁ លើក បញ្ហា របស់ ខ្លួនឯង ▁ដែល កន្លងមក ស្ត្រី ឃើញថា ▁ អ្នកដែល ធ្វើ នយោបាយ ▁លោក មិនបាន លើក បញ្ហា ខ្លួន ឲ្យ ពិត ជាក់ស្តែង ▁ដោយសារ លោក មិនបាន ជួប បញ្ហា ហ្នឹង ដោយ ខ្លួន គាត់ ។\n",
      "Prediction: លោក រដ្ឋមន្រ្តី ថ្លែងថា ៖ « ស្ត្រី ក៏ ចង់ឲ្យ មានការ បញ្ចេញមតិ ការ លើកឡើង ពី បញ្ហា របស់ខ្លួន កន្លងមក ស្ត្រី មើលឃើញ ថា អ្នក ធ្វើ នយោបាយ មិនសូវ លើកឡើង ពី បញ្ហា របស់ខ្លួន ដោយសារតែ ខ្លួនឯង មិន ដែល ប្រឈមនឹង បញ្ហា ទាំងនោះ ។\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Source    : Thầy Om Phearum cho biết vì thầy dạy toán nên thầy hướng_dẫn học_sinh_học môn này vào lúc 7 giờ sáng trong khoảng 45 phút , theo một video do một giáo_viên khác quay .\n",
      "Reference : ▁ លោកគ្រូ ▁ អ៊ ុំ ▁ ភា រ ម ្យ បន្តថា ▁ដោយសារ លោក បង្រៀន គណ ិត វិទ្យា ▁ដូច្នេះ លោក ដឹកនាំ សិស្ស រៀន មុខ វិជ្ជា នេះ នៅពេល ព្រឹក ម៉ោង ៧ ▁ ប្រហែលជា ៤៥ នាទី ▁ ទៅតាម វីដេអូ ដែល ថត បង្រៀន ដោយ គ្រូ ផ្សេង ។\n",
      "Prediction: លោកគ្រូ អ៊ ុំ ភា រ ម ្យ បញ្ជាក់ថា ដោយសារ លោកគ្រូ ជា គ្រូ គណ ិត វិទ្យា លោកគ្រូ បាន បង្រៀន សិស្ស ឲ្យ រៀន មុខ វិជ្ជា នេះ នៅម៉ោង ៧ ព្រឹក ប្រមាណ ៤៥ នាទី តាម វីដេអូ ដែល ថត ដោយ គ្រូ ម្នាក់ទៀត ។\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "Source    : Thủ_tướng Hun_Sen hôm thứ Tư đã gọi tuyên_bố của Sam Rainsy thông_qua video là sự liên_hệ đến video của Kem_Sokha , người đang bị buộc_tội phản_quốc vì tiết_lộ kế_hoạch lật_đổ chính_phủ với sự thông_đồng của Hoa_Kỳ .\n",
      "Reference : ▁លោក នាយករដ្ឋមន្រ្តី ▁ហ៊ុន ▁សែន ▁ កាលពីថ្ងៃពុធ ▁បាន ហៅ ការ ថ្លែង របស់លោក ▁សម ▁រង្ស៊ី ▁តាមរយៈ វីដេអូ នេះ ថា ▁ ជាការ ផ្សារ ភ្ជាប់ ទៅនឹង វីដេអូ របស់ ▁លោក ▁កឹម ▁សុខា ▁ ដែលកំពុង ជាប់ ចោទ ពីបទ ក ្ បត់ ជាតិ ▁ ព្រោះតែ បង្ហាញ គម្រោង ផ្ ត ួល រំ លំ រាជរដ្ឋាភិបាល ▁ដោយ ឃ ុប ឃ ិត ជាមួយ សហរដ្ឋអាមេរិក ។\n",
      "Prediction: លោក នាយករដ្ឋមន្ត្រី ហ៊ុន សែន នៅ ថ្ងៃពុធ នេះ បាន ហៅ ស េចក្តីថ្លែងការណ៍ របស់លោក សម រង្ស៊ី តាមរយៈ វីដេអូ នោះ ថា ជាការ ភ្ជាប់ ទៅនឹង វីដេអូ របស់លោក កឹម សុខា ដែល រង ការចោទប្រកាន់ ពីបទ ក ្ បត់ ជាតិ ពីបទ បង្ហាញ ផែនការ ផ្ ត ួល រំ លំ រដ្ឋាភិបាល ដោយ មានការ ឃ ុប ឃ ិត ពី សហរដ្ឋអាមេរិក ។\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "Source    : Tuần trước , truyền_thông nhà_nước Trung_Quốc đưa tin rằng Bắc_Kinh hy_vọng sẽ hoàn_tất bộ quy_tắc ứng_xử cho Biển Đông vốn bị trì_hoãn từ lâu trong năm 2017 .\n",
      "Reference : ▁ សារព័ត៌មាន រដ្ឋ របស់ ចិន កាលពី សប្តាហ៍ មុន បាន រាយការណ៍ ថា ▁ រដ្ឋាភិបាល ក្រុង ប៉េ ក ាំង សង្ឃឹមថា ▁នឹង សម្រេច ក្រម ប្រតិបត្តិ ដែលត្រូវបាន ពន្ យា រ ពេល យ៉ាង យូរ សម្រាប់ ស មុទ្រចិនខាងត្បូង ▁នៅក្នុង ឆ្នាំ២០១៧ ខាងមុខនេះ ។\n",
      "Prediction: កាលពី សប្តាហ៍ មុន ប្រព័ន្ធ ផ្សព្វផ្សាយ រដ្ឋ របស់ ចិន បាន រាយការណ៍ ថា រដ្ឋាភិបាល ក្រុង ប៉េ ក ាំង សង្ឃឹមថា នឹង បញ្ចប់ ក្រម ប្រតិបត្តិ ស មុទ្រចិនខាងត្បូង ដែលបាន ពន្ យា រ ពេល ជា យូរ មកហើយ នៅក្នុង ឆ្នាំ២០១៧ ។\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 6:\n",
      "Source    : Trong bối_cảnh Campuchia đang hướng tới cuộc bầu_cử quốc_hội sắp tới , ông Say Chhum cũng đề_cập trong thông_điệp của mình rằng người dân Campuchia sẽ có quyền thành_lập Quốc_hội , từ đó dẫn đến việc bầu ra một chính_phủ mới .\n",
      "Reference : ▁ដោយ ហេតុ ថា ▁ កម្ពុជា ក៏ កំពុង ឆ្ពោះទៅ រក ការបោះឆ្នោត ជ្រើសតាំង តំណាងរាស្ត្រ នៅពេល ខាងមុខនេះ ▁លោក ▁ សាយ ▁ ឈ ុំ ▁ក៏ បានលើកឡើង ក្នុង សារ លិខិត របស់លោក ថា ▁ ប្រជាពលរដ្ឋខ្មែរ ▁ នឹងមាន សិទ្ធិ ក្នុងការ បង្កើត រដ្ឋសភា ដែល នឹង នាំ ដល់ ការ ជ្រើសតាំង រដ្ឋាភិបាល ថ្មីមួយ ។\n",
      "Prediction: ស្រប ពេលដែល កម្ពុជា កំពុង ងាក ទៅរក ការបោះឆ្នោត ជ្រើសតាំង តំណាងរាស្ត្រ នាពេលខាងមុខ លោក សាយ ឈ ុំ ក៏ បានលើកឡើង ក្នុង សារ របស់លោក ថា ប្រជាពលរដ្ឋខ្មែរ នឹងមាន សិទ្ធិ បង្កើត រដ្ឋសភា ដែល នឹង ឈានទៅ ដល់ ការ ជ្រើសរើស រដ្ឋាភិបាល ថ្មី ។\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 7:\n",
      "Source    : \" Họ nói họ muốn đánh_dấu vào phiếu bầu , nhưng họ không_thể . \"\n",
      "Reference : ▁ ថា ចង់ គូ ស សន្លឹកឆ្នោត ▁មិន ឲ្យបាន ការ » ។\n",
      "Prediction: ពួកគេ និយាយថា ពួកគេ ចង់ គូ ស សន្លឹកឆ្នោត ប៉ុន្តែ ពួកគេ មិនអាច ទេ » ។\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 8:\n",
      "Source    : Đây không phải là lần đầu_tiên bà Sithar bị bắt vì đòi quyền_lợi cho người lao_động .\n",
      "Reference : ▁ នេះ មិនមែនជា លើកទី ▁១ ▁ ទេ ដែល ▁ ស៊ី ថ រ ▁ត្រូវបាន ចាប់ខ្លួន ដោយសារ ការ ទាមទារ សិទ្ធិ ▁និង អត្ថប្រយោជន៍ ឲ្យ បុគ្គលិក ។\n",
      "Prediction: នេះ មិនមែនជា លើកទី មួយ ទេ ដែល កញ្ញា ស៊ី ថ រ ត្រូវបាន ចាប់ខ្លួន ដោយសារតែ ការ ទាមទារ សិទ្ធិ ដល់ កម្មករ ។\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 9:\n",
      "Source    : Sự gia_tăng bạo_lực dựa trên giới_tính được gọi là một đại_dịch tiềm_ẩn .\n",
      "Reference : ▁បាន ហៅ ការ កើនឡើង នៃ អំពើហិង្សា ផ្អែក លើ យ៉ ែន ឌ ័រ នេះ ថា ▁ ជាការ រា ត ត ្ បាត ដោយ លាក់ កំ បាំង ។\n",
      "Prediction: ការ កើនឡើង នៃ អំពើហិង្សា ផ្អែក លើ យ េន ឌ ័រ ត្រូវបានគេ ហៅថា ជា ជំងឺ រា ត ត ្ បាត ដែលអាច កើតមានឡើង ។\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 10:\n",
      "Source    : \" Trưởng lâm_nghiệp đã lấy nó rồi ... việc này không liên_quan gì đến chúng_tôi cả . \"\n",
      "Reference : ▁ មេ ព្រៃ គេ ទៅ យក . ... អត់ មាន ពាក់ព័ន្ធ អី ជាមួយ ពួក ខ្ញុំ ផង » ។\n",
      "Prediction: មេ ព្រៃឈើ គេ យក ហើយ ... អត់ មាន ពាក់ព័ន្ធ អី ជាមួយ យើង ទេ » ។\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 15: Sample Translations\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE TRANSLATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Source    : {test_vi[i]}\")\n",
    "    print(f\"Reference : {test_khm[i]}\")\n",
    "    print(f\"Prediction: {preds_phase3[i]}\")\n",
    "    print(\"-\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb6eae-33de-4e55-89d1-2715d9d0e1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
