{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09991825-5e86-47b7-a7d0-b7ae6ebbfaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.9.1)\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.57.3)\n",
      "Requirement already satisfied: sentencepiece in ./.local/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.10/site-packages (4.4.2)\n",
      "Requirement already satisfied: sacrebleu in ./.local/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.10/site-packages (1.12.0)\n",
      "Requirement already satisfied: pyvi in ./.local/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: laonlp in ./.local/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.local/lib/python3.10/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.local/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.local/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.local/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.local/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.local/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.local/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.local/lib/python3.10/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.local/lib/python3.10/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.local/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.local/lib/python3.10/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.local/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.local/lib/python3.10/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: portalocker in ./.local/lib/python3.10/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in ./.local/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in ./.local/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in ./.local/lib/python3.10/site-packages (from sacrebleu) (6.0.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.10/site-packages (from pyvi) (1.7.2)\n",
      "Requirement already satisfied: sklearn-crfsuite in ./.local/lib/python3.10/site-packages (from pyvi) (0.5.0)\n",
      "Requirement already satisfied: pythainlp>=3.0.0 in ./.local/lib/python3.10/site-packages (from laonlp) (5.2.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.local/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.local/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.local/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.local/lib/python3.10/site-packages (from scikit-learn->pyvi) (3.6.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.7 in ./.local/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (0.9.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers sentencepiece datasets sacrebleu accelerate pyvi laonlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c70c1e74-fc31-484d-8de0-c382520719b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA H200\n",
      "VRAM: 150.0217344 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    M2M100ForConditionalGeneration,\n",
    "    M2M100Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import random\n",
    "\n",
    "from pyvi import ViTokenizer\n",
    "from laonlp.tokenize import word_tokenize as lao_word_tokenize\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(\"VRAM:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9620c542-7c73-4016-84b3-d7198c04c9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vietnamese and Lao tokenizers loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def tokenize_vietnamese(text):\n",
    "    \"\"\"Tokenize Vietnamese text using PyVi\"\"\"\n",
    "    try:\n",
    "        return ViTokenizer.tokenize(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing Vietnamese: {e}\")\n",
    "        return text\n",
    "\n",
    "def tokenize_lao(text):\n",
    "    \"\"\"Tokenize Lao text using LaoNLP\"\"\"\n",
    "    try:\n",
    "        # LaoNLP word_tokenize không có parameter engine\n",
    "        tokens = lao_word_tokenize(text)\n",
    "        return \" \".join(tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing Lao: {e}\")\n",
    "        return text\n",
    "\n",
    "def tokenize_batch_vietnamese(texts):\n",
    "    \"\"\"Batch tokenize Vietnamese texts\"\"\"\n",
    "    print(f\"Tokenizing {len(texts)} Vietnamese texts...\")\n",
    "    results = []\n",
    "    for i, text in enumerate(texts):\n",
    "        results.append(tokenize_vietnamese(text))\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(texts)} Vietnamese texts\")\n",
    "    return results\n",
    "\n",
    "def tokenize_batch_lao(texts):\n",
    "    \"\"\"Batch tokenize Lao texts\"\"\"\n",
    "    print(f\"Tokenizing {len(texts)} Lao texts...\")\n",
    "    results = []\n",
    "    for i, text in enumerate(texts):\n",
    "        results.append(tokenize_lao(text))\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(texts)} Lao texts\")\n",
    "    return results\n",
    "\n",
    "print(\"Vietnamese and Lao tokenizers loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c366eca3-b4a5-493c-8ac5-1aaddcb9dba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12cd930ad714325bb21005fd7774a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5411ebbd0ee4444835280f186bb73f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd4b6e744b74f9fa68baf70cb0be7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d6f95c6a164209abddd84cada11071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb429d920f0444149ebb75c013e3f878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0debdc4d9243009b6037fb15fe760b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e835cba31340a5a7702aa530ad3996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5955f77bbda84c1e9d8902c3148b70aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"facebook/m2m100_418M\"\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90ed0eab-211e-4934-b52e-36d62717bdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params    : 483.9M\n",
      "Frozen params   : 0.0M\n",
      "Trainable params: 483.9M\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Check status\n",
    "# ============================================================\n",
    "# Verify\n",
    "total, frozen, trainable = 0, 0, 0\n",
    "for name, param in model.named_parameters():\n",
    "    n = param.numel()\n",
    "    total += n\n",
    "    if not param.requires_grad:\n",
    "        frozen += n\n",
    "    else:\n",
    "        trainable += n\n",
    "\n",
    "print(f\"Total params    : {total/1e6:.1f}M\")\n",
    "print(f\"Frozen params   : {frozen/1e6:.1f}M\")\n",
    "print(f\"Trainable params: {trainable/1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14de2069-afe0-41fc-b451-d35abb14841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"dataset\"\n",
    "def load_parallel(src_file, tgt_file):\n",
    "    with open(src_file, encoding=\"utf-8\") as f:\n",
    "        src = [l.strip() for l in f]\n",
    "    with open(tgt_file, encoding=\"utf-8\") as f:\n",
    "        tgt = [l.strip() for l in f]\n",
    "    \n",
    "    assert len(src) == len(tgt)\n",
    "    \n",
    "    # Apply language-specific tokenization\n",
    "    print(\"Tokenizing Vietnamese texts...\")\n",
    "    src_tokenized = tokenize_batch_lao(src)\n",
    "    \n",
    "    print(\"Tokenizing Lao texts...\")\n",
    "    tgt_tokenized = tokenize_batch_vietnamese(tgt)\n",
    "    \n",
    "    return Dataset.from_dict({\n",
    "        \"src_text\": src_tokenized,\n",
    "        \"tgt_text\": tgt_tokenized\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64e97a97-1cef-4c1a-b1b6-a331e29493bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Vietnamese texts...\n",
      "Tokenizing 700000 Lao texts...\n",
      "  Processed 10000/700000 Lao texts\n",
      "  Processed 20000/700000 Lao texts\n",
      "  Processed 30000/700000 Lao texts\n",
      "  Processed 40000/700000 Lao texts\n",
      "  Processed 50000/700000 Lao texts\n",
      "  Processed 60000/700000 Lao texts\n",
      "  Processed 70000/700000 Lao texts\n",
      "  Processed 80000/700000 Lao texts\n",
      "  Processed 90000/700000 Lao texts\n",
      "  Processed 100000/700000 Lao texts\n",
      "  Processed 110000/700000 Lao texts\n",
      "  Processed 120000/700000 Lao texts\n",
      "  Processed 130000/700000 Lao texts\n",
      "  Processed 140000/700000 Lao texts\n",
      "  Processed 150000/700000 Lao texts\n",
      "  Processed 160000/700000 Lao texts\n",
      "  Processed 170000/700000 Lao texts\n",
      "  Processed 180000/700000 Lao texts\n",
      "  Processed 190000/700000 Lao texts\n",
      "  Processed 200000/700000 Lao texts\n",
      "  Processed 210000/700000 Lao texts\n",
      "  Processed 220000/700000 Lao texts\n",
      "  Processed 230000/700000 Lao texts\n",
      "  Processed 240000/700000 Lao texts\n",
      "  Processed 250000/700000 Lao texts\n",
      "  Processed 260000/700000 Lao texts\n",
      "  Processed 270000/700000 Lao texts\n",
      "  Processed 280000/700000 Lao texts\n",
      "  Processed 290000/700000 Lao texts\n",
      "  Processed 300000/700000 Lao texts\n",
      "  Processed 310000/700000 Lao texts\n",
      "  Processed 320000/700000 Lao texts\n",
      "  Processed 330000/700000 Lao texts\n",
      "  Processed 340000/700000 Lao texts\n",
      "  Processed 350000/700000 Lao texts\n",
      "  Processed 360000/700000 Lao texts\n",
      "  Processed 370000/700000 Lao texts\n",
      "  Processed 380000/700000 Lao texts\n",
      "  Processed 390000/700000 Lao texts\n",
      "  Processed 400000/700000 Lao texts\n",
      "  Processed 410000/700000 Lao texts\n",
      "  Processed 420000/700000 Lao texts\n",
      "  Processed 430000/700000 Lao texts\n",
      "  Processed 440000/700000 Lao texts\n",
      "  Processed 450000/700000 Lao texts\n",
      "  Processed 460000/700000 Lao texts\n",
      "  Processed 470000/700000 Lao texts\n",
      "  Processed 480000/700000 Lao texts\n",
      "  Processed 490000/700000 Lao texts\n",
      "  Processed 500000/700000 Lao texts\n",
      "  Processed 510000/700000 Lao texts\n",
      "  Processed 520000/700000 Lao texts\n",
      "  Processed 530000/700000 Lao texts\n",
      "  Processed 540000/700000 Lao texts\n",
      "  Processed 550000/700000 Lao texts\n",
      "  Processed 560000/700000 Lao texts\n",
      "  Processed 570000/700000 Lao texts\n",
      "  Processed 580000/700000 Lao texts\n",
      "  Processed 590000/700000 Lao texts\n",
      "  Processed 600000/700000 Lao texts\n",
      "  Processed 610000/700000 Lao texts\n",
      "  Processed 620000/700000 Lao texts\n",
      "  Processed 630000/700000 Lao texts\n",
      "  Processed 640000/700000 Lao texts\n",
      "  Processed 650000/700000 Lao texts\n",
      "  Processed 660000/700000 Lao texts\n",
      "  Processed 670000/700000 Lao texts\n",
      "  Processed 680000/700000 Lao texts\n",
      "  Processed 690000/700000 Lao texts\n",
      "  Processed 700000/700000 Lao texts\n",
      "Tokenizing Lao texts...\n",
      "Tokenizing 700000 Vietnamese texts...\n",
      "  Processed 10000/700000 Vietnamese texts\n",
      "  Processed 20000/700000 Vietnamese texts\n",
      "  Processed 30000/700000 Vietnamese texts\n",
      "  Processed 40000/700000 Vietnamese texts\n",
      "  Processed 50000/700000 Vietnamese texts\n",
      "  Processed 60000/700000 Vietnamese texts\n",
      "  Processed 70000/700000 Vietnamese texts\n",
      "  Processed 80000/700000 Vietnamese texts\n",
      "  Processed 90000/700000 Vietnamese texts\n",
      "  Processed 100000/700000 Vietnamese texts\n",
      "  Processed 110000/700000 Vietnamese texts\n",
      "  Processed 120000/700000 Vietnamese texts\n",
      "  Processed 130000/700000 Vietnamese texts\n",
      "  Processed 140000/700000 Vietnamese texts\n",
      "  Processed 150000/700000 Vietnamese texts\n",
      "  Processed 160000/700000 Vietnamese texts\n",
      "  Processed 170000/700000 Vietnamese texts\n",
      "  Processed 180000/700000 Vietnamese texts\n",
      "  Processed 190000/700000 Vietnamese texts\n",
      "  Processed 200000/700000 Vietnamese texts\n",
      "  Processed 210000/700000 Vietnamese texts\n",
      "  Processed 220000/700000 Vietnamese texts\n",
      "  Processed 230000/700000 Vietnamese texts\n",
      "  Processed 240000/700000 Vietnamese texts\n",
      "  Processed 250000/700000 Vietnamese texts\n",
      "  Processed 260000/700000 Vietnamese texts\n",
      "  Processed 270000/700000 Vietnamese texts\n",
      "  Processed 280000/700000 Vietnamese texts\n",
      "  Processed 290000/700000 Vietnamese texts\n",
      "  Processed 300000/700000 Vietnamese texts\n",
      "  Processed 310000/700000 Vietnamese texts\n",
      "  Processed 320000/700000 Vietnamese texts\n",
      "  Processed 330000/700000 Vietnamese texts\n",
      "  Processed 340000/700000 Vietnamese texts\n",
      "  Processed 350000/700000 Vietnamese texts\n",
      "  Processed 360000/700000 Vietnamese texts\n",
      "  Processed 370000/700000 Vietnamese texts\n",
      "  Processed 380000/700000 Vietnamese texts\n",
      "  Processed 390000/700000 Vietnamese texts\n",
      "  Processed 400000/700000 Vietnamese texts\n",
      "  Processed 410000/700000 Vietnamese texts\n",
      "  Processed 420000/700000 Vietnamese texts\n",
      "  Processed 430000/700000 Vietnamese texts\n",
      "  Processed 440000/700000 Vietnamese texts\n",
      "  Processed 450000/700000 Vietnamese texts\n",
      "  Processed 460000/700000 Vietnamese texts\n",
      "  Processed 470000/700000 Vietnamese texts\n",
      "  Processed 480000/700000 Vietnamese texts\n",
      "  Processed 490000/700000 Vietnamese texts\n",
      "  Processed 500000/700000 Vietnamese texts\n",
      "  Processed 510000/700000 Vietnamese texts\n",
      "  Processed 520000/700000 Vietnamese texts\n",
      "  Processed 530000/700000 Vietnamese texts\n",
      "  Processed 540000/700000 Vietnamese texts\n",
      "  Processed 550000/700000 Vietnamese texts\n",
      "  Processed 560000/700000 Vietnamese texts\n",
      "  Processed 570000/700000 Vietnamese texts\n",
      "  Processed 580000/700000 Vietnamese texts\n",
      "  Processed 590000/700000 Vietnamese texts\n",
      "  Processed 600000/700000 Vietnamese texts\n",
      "  Processed 610000/700000 Vietnamese texts\n",
      "  Processed 620000/700000 Vietnamese texts\n",
      "  Processed 630000/700000 Vietnamese texts\n",
      "  Processed 640000/700000 Vietnamese texts\n",
      "  Processed 650000/700000 Vietnamese texts\n",
      "  Processed 660000/700000 Vietnamese texts\n",
      "  Processed 670000/700000 Vietnamese texts\n",
      "  Processed 680000/700000 Vietnamese texts\n",
      "  Processed 690000/700000 Vietnamese texts\n",
      "  Processed 700000/700000 Vietnamese texts\n",
      "Total dataset size: 700000 examples\n",
      "Train dataset: 696000 examples (for training)\n",
      "Dev dataset  : 3000 examples (for validation during training)\n",
      "Test dataset : 1000 examples (for final evaluation)\n",
      "\n",
      "Data split and shuffle completed.\n"
     ]
    }
   ],
   "source": [
    "# Load toàn bộ dữ liệu từ train.vi và train.lo\n",
    "full_dataset = load_parallel(\n",
    "    f\"{DATA_DIR}/train_lo_to_vi_shuf.lo\",\n",
    "    f\"{DATA_DIR}/train_lo_to_vi_shuf.vi\"\n",
    ")\n",
    "\n",
    "print(f\"Total dataset size: {len(full_dataset)} examples\")\n",
    "\n",
    "# Chia dataset: \n",
    "# - Test: 1000 dòng cuối cùng\n",
    "# - Dev: 3000 dòng trước test\n",
    "# - Train: phần còn lại\n",
    "\n",
    "test_size = 1000\n",
    "dev_size = 3000\n",
    "\n",
    "# Tính chỉ số\n",
    "test_start_idx = len(full_dataset) - test_size\n",
    "dev_start_idx = test_start_idx - dev_size\n",
    "\n",
    "# Chia dataset\n",
    "test_dataset = full_dataset.select(range(test_start_idx, len(full_dataset)))\n",
    "dev_dataset = full_dataset.select(range(dev_start_idx, test_start_idx))\n",
    "train_dataset = full_dataset.select(range(0, dev_start_idx))\n",
    "\n",
    "# Shuffle training data để tránh bias thứ tự\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} examples (for training)\")\n",
    "print(f\"Dev dataset  : {len(dev_dataset)} examples (for validation during training)\")\n",
    "print(f\"Test dataset : {len(test_dataset)} examples (for final evaluation)\")\n",
    "print(\"\\nData split and shuffle completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6345f953-679a-4cb9-b3fa-91e0ddee4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "\n",
    "def preprocess(batch):\n",
    "    tokenizer.src_lang = \"lo\"\n",
    "    tokenizer.tgt_lang = \"vi\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        batch[\"src_text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"tgt_text\"],\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24186107-c969-4c87-882a-9c479892d4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aecc25c86bd94f0cbd6b4385d020c8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/696000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e283b5d8e2465fa39041023dbd4de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/admin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=8\n",
    ")\n",
    "\n",
    "dev_dataset = dev_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dev_dataset.column_names,\n",
    "    num_proc=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ec7aeef-f541-406c-a86d-6eab7770e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3addc165-5067-4f3f-8f07-b29fe2d91514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1a9b7bd-05c4-4a55-a4e3-d8234f8cac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_phase1 = TrainingArguments(\n",
    "    output_dir=\"./lo_to_vi/phase1\",\n",
    "    # Eval / Save\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    logging_steps=100,\n",
    "\n",
    "    # Batch (encoder frozen → đẩy lớn)\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=2,\n",
    "\n",
    "    # Optim\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.05,\n",
    "    weight_decay=0.005,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # Epochs\n",
    "    num_train_epochs=1,\n",
    "\n",
    "    # Precision\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "\n",
    "    # Speed\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=8,\n",
    "    dataloader_pin_memory=True,\n",
    "\n",
    "    # Best model\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0b7e80f-a5ac-48c8-81b3-78aa81f3aaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1675/1009842188.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_phase1 = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Create Trainer - Phase 1\n",
    "# ============================================================\n",
    "trainer_phase1 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_phase1,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c55662a9-7e1f-4696-be31-22105e1dbb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 1: Training no freeze \n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2719' max='2719' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2719/2719 10:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.368000</td>\n",
       "      <td>1.220180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.170100</td>\n",
       "      <td>1.068407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.095500</td>\n",
       "      <td>1.003779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.050600</td>\n",
       "      <td>0.966719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.027200</td>\n",
       "      <td>0.945169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2719, training_loss=1.2415258394264481, metrics={'train_runtime': 634.7782, 'train_samples_per_second': 1096.446, 'train_steps_per_second': 4.283, 'total_flos': 9.38047480308695e+16, 'train_loss': 1.2415258394264481, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Train Phase 1\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 1: Training no freeze \")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "trainer_phase1.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d1b05b9-69fb-4434-81cc-a55986f7e26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 1 model saved!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Save Phase 1 Model\n",
    "# ============================================================\n",
    "trainer_phase1.save_model(\"./lo_to_vi/phase1/best\")\n",
    "tokenizer.save_pretrained(\"./lo_to_vi/phase1/best\")\n",
    "\n",
    "print(\"\\nPhase 1 model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df124241-7927-47cd-9c92-074587e65159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set size: 1000 examples\n",
      "\n",
      "Translating test set...\n",
      "Translated 320/1000\n",
      "Translated 640/1000\n",
      "Translated 960/1000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Evaluate Phase 1\n",
    "# ============================================================\n",
    "def translate_batch(texts, model, tokenizer, batch_size=32):\n",
    "    \"\"\"Batch translation for speed\"\"\"\n",
    "    model.eval()\n",
    "    tokenizer.src_lang = \"lo\"\n",
    "    tokenizer.tgt_lang = \"vi\"\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=tokenizer.get_lang_id(\"vi\"),\n",
    "                num_beams=5,\n",
    "                max_length=256\n",
    "            )\n",
    "        \n",
    "        texts_out = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "        outputs.extend(texts_out)\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"Translated {i+len(batch)}/{len(texts)}\")\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Lấy dữ liệu test từ test_dataset (đã chia từ train.vi/train.lo)\n",
    "test_lo = test_dataset[\"src_text\"]\n",
    "test_vi = test_dataset[\"tgt_text\"]\n",
    "\n",
    "print(f\"\\nTest set size: {len(test_vi)} examples\")\n",
    "print(\"\\nTranslating test set...\")\n",
    "preds_phase1 = translate_batch(test_lo, model, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd0e87ad-d75d-46e4-9f93-e1048b5cfa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 1 BLEU Score: 45.43\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "# Calculate BLEU\n",
    "bleu_phase1 = corpus_bleu(preds_phase1, [test_vi])\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PHASE 1 BLEU Score: {bleu_phase1.score:.2f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Save predictions\n",
    "with open(\"./lo_to_vi/phase1/phase1_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(preds_phase1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43f68e83-79eb-4637-82d7-824d5ef431ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAMPLE TRANSLATIONS\n",
      "======================================================================\n",
      "\n",
      "Example 1:\n",
      "Source    : ພຽງ ແຕ່ ການຈັບ ຄູ່ ກະໂປງ ສັ້ນ ລາຍ ດອກ ກັບ ເສື້ອ ເປີດ ບ່າໄຫລ່ ກໍ ຈະ ເຮັດໃຫ້ ສາວ ໆ ມີ ຊຸດ ທີ່ເບິ່ງ ດີ ແລະ ເທ່ ທັນທີ .\n",
      "Reference : Chỉ việc mix chân váy hoa dáng ngắn cùng áo trễ vai , các cô gái có ngay một set đồ \" hack \" dáng lại mát_rượi .\n",
      "Prediction: Chỉ cần kết_hợp chân váy ngắn lá hoa với áo khoác_áo , các cô gái sẽ lập_tức sở_hữu những bộ trang_phục đẹp_mắt , mịn_màng .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Source    : ຮູບເງົາ ບາງ ເລື່ອງ ທີ່   Phuong   Thanh   ໄດ້ ເຂົ້າຮ່ວມ ສະແດງ ລວມ ມີ :   Walking   and   Crying ,   Surrogate   Mother ,   When   Men   Get   Pregnant ,   The   Soul   of   Truong   Ba ,   the   Butcher ' s   Skin ,   The   Kiss   of   Death ,   Beautiful   Every   Centimeter ,   Rescuing   the   God   of   Death ,   ແລະ   Hot   Boy   Rebellion .\n",
      "Reference : Các bộ phim Phương_Thanh đã tham_gia như : Vừa đi vừa khóc , Đẻ_mướn , Khi đàn_ông có bầu , Hồn Trương Ba , da hàng thịt , Nụ hôn thần_chết , Đẹp từng centimet , Giải_cứu thần_chết , Hot_boy nổi_loạn .\n",
      "Prediction: Một_số bộ phim mà Phương_Thanh tham_gia bao_gồm : Đi khóc , Nắng mẹ , Khi đàn_ông mang thai , Linh_hồn Trường_Ba , Búp_bê da , Nụ hôn tử_vong , Beautiful Every Centymeter , Rescuing God of Death , Hot_Boy_Revellion .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Source    : ໃນ ວັນທີ   15   ມີນາ ,   ເຢຍລະມັນ ໄດ້ ບັນທຶກ ສະຖິຕິ ໃໝ່ ສຳລັບ ອັດຕາການຕິດເຊື້ອ   SARS-CoV- 2   ເປັນ ເວລາສີ່ມື້ ຕິດຕໍ່ກັນ .\n",
      "Reference : Hôm 15 / 3 , Đức ghi_nhận kỷ_lục mới về tỷ_lệ lây_nhiễm SARS - CoV - 2 trong 4 ngày liên_tiếp .\n",
      "Prediction: Ngày 15 / 3 , Đức ghi_nhận kỷ_lục mới về tỷ_lệ nhiễm SARS - CoV - 2 trong 4 ngày liên_tiếp .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "Source    : ຄູຝຶກ   Park   Hang-seo   ເຂົ້າໃຈ ເລື່ອງ ນີ້ ດີ ,   ເພາະວ່າ ລາວ ມາຈາກ ປະເທດ ທີ່ ບານເຕະ ພັດທະນາ ຫຼາຍ ກວ່າ ຫວຽດນາມ .\n",
      "Reference : HLV Park_Hang - seo hiểu rõ điều này , đơn_giản bởi ông đến từ một quốc_gia có nền bóng_đá đi trước Việt_Nam .\n",
      "Prediction: HLV Park_Hang - seo hiểu rõ điều này , bởi ông đến từ một quốc_gia mà bóng_đá phát_triển hơn Việt_Nam .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "Source    : ຜົນ ການສອບເສັງ ຈະ ຖືກ ນໍ າ ໃຊ້ ໂດຍ ຜູ້ ສະໝັກ ເພື່ອ ສະຫມັກ ເຂົ້າ ຮຽນ ໃນ ມະຫາວິທະຍາໄລ ແລະ ວິທະຍາໄລ ທີ່ໃຊ້ ວິທີການ ນີ້ ສໍ າ ລັບ ການລົງທະບຽນ ຮຽນ .\n",
      "Reference : Kết_quả kỳ thi sẽ được sử_dụng để thí_sinh đăng_ký xét tuyển vào các trường ĐH - CĐ có sử_dụng phương_thức này để tuyển_sinh .\n",
      "Prediction: Kết_quả thi sẽ được thí_sinh sử_dụng để đăng_ký xét tuyển vào các trường đại_học , cao_đẳng sử_dụng phương_thức này để đăng_ký xét tuyển .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 6:\n",
      "Source    : ນີ້ ເປັນ ຄັ້ງ ທຳອິດ ທີ່ ທັງສອງ ໄດ້ ຮ່ວມມື ກັນ ,   ດ້ວຍ ກົນລະຍຸດ ການ ຈີບ ຂອງ ພວກເຂົາ ...   ບໍ່ ຄື ກັບ ສິ່ງ ອື່ນ ໃດ ,   ແຕ່ ສ້າງ ຜົນກະທົບ ທີ່ມີ ພະລັງ .\n",
      "Reference : Đây là lần đầu cả hai kết_hợp , với những màn thả thính … không giống ai nhưng lại tạo nên hiệu_ứng cực mạnh .\n",
      "Prediction: Đây là lần đầu_tiên cả hai hợp_tác với nhau , với chiến_thuật chạm_trán ... không giống như bất_cứ thứ gì khác , mà tạo ra hiệu_ứng mạnh_mẽ .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 7:\n",
      "Source    : ກ່າວ ຄຳປາໄສ ທີ່ ກອງ ປະຊຸມ ສຳມະນາ   \" ການກວດສອບ ວິສາຫະກິດ ທີ່ມີ ທຶນ ຂອງ ລັດ ໜ້ອຍ ກວ່າ   50 %\"   ໃນ ຕອນເຊົ້າ ຂອງ ວັນທີ   6   ກໍລະກົດ ,   ດຣ.   ຫວ ູ ດິ່ງ ແອງ ,   ຜູ້ ຊ່ຽວຊານ ດ້ານ ເສດຖະກິດ ,   ໄດ້ ຊີ້ ໃຫ້ເຫັນ ເຖິງ ຂໍ້ ບົກຜ່ອງ   ແລະ   ຂໍ້ຈຳກັດ ໃນ ການຄຸ້ມຄອງ ທຶນ ໃນ ວິສາຫະກິດ ປະເພດ ນີ້ .\n",
      "Reference : Phát_biểu tại Hội_thảo “ Kiểm_toán đối_với DN do Nhà_nước nắm giữ dưới 50 % vốn điều_lệ ” sáng 6 / 7 , TS Vũ_Đình_Ánh - Chuyên_gia kinh_tế đã chỉ rõ những bất_cập và hạn_chế trong công_tác quản_lý vốn trong loại_hình DN này .\n",
      "Prediction: Phát_biểu tại Hội_thảo “ Kiểm_toán doanh_nghiệp dưới 50 % vốn nhà_nước ” sáng 6 / 7 , TS Vũ_Đình_Anh , chuyên_gia kinh_tế chỉ ra những bất_cập , hạn_chế trong quản_lý vốn của loại doanh_nghiệp này .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 8:\n",
      "Source    : ຖ້າ ບັນຫາການແບ່ງແຍກ ການປົກຄອງ   ແລະ   ການຄຸ້ມຄອງ ວິສາຫະກິດ ຂອງ ລັດ ບໍ່ໄດ້ ຮັບ ການແກ້ໄຂ ,   ບົດບາດ ຜູ້ນຳ ຈະ ກາຍເປັນ ພາລະ ຂອງ ພາກ ທຸລະກິດ .\n",
      "Reference : Nếu không gỡ bài_toán tách_bạch quản_trị và quản_lý doanh_nghiệp nhà_nước , vai_trò dẫn_dắt sẽ trở_thành gánh nặng của khu_vực doanh_nghiệp .\n",
      "Prediction: Nếu không giải_quyết được vấn_đề phân_cấp quản_trị , quản_lý doanh_nghiệp nhà_nước , vai_trò lãnh_đạo sẽ trở_thành gánh nặng của khu_vực kinh_doanh .\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 9:\n",
      "Source    : ຊອຍ ໝາກຫຸ່ງ ທີ່ ໜຽວ ໆ   ແລະ   ຫວານ ເລັກນ້ອຍ ,   ປະສົມ ກັບ ກິ່ນຫອມ ຂອງ ໝາກເຜັດ ,   ເປັນ ທີ່ ໜ້າ ສົນໃຈ ແທ້ໆ ສຳລັບ ທຸກ ຄົນ ທີ່ ໄດ້ ລອງ !\n",
      "Reference : Những miếng đu_đủ dai dai , ngọt nhẹ , thơm mùi chanh leo thực_sự hấp_dẫn bất_cứ ai thưởng_thức !\n",
      "Prediction: Một_chút ngọt_ngào , ngọt_ngào , kết_hợp với hương_vị của ớt thật_sự hấp_dẫn đối_với bất_kỳ ai đã thử nhé !\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 10:\n",
      "Source    : ທ່ານ   Doug   Shapiro   ຜູ້ອຳນວຍການບໍລິຫານ ສູນ ຄົ້ນຄວ້າ ຂອງ ອົງການ ດັ່ງກ່າວ   ກ່າວ ວ່າ   ການຫຼຸດລົງ ໃນ ເບື້ອງຕົ້ນ ອາດຈະ ເປັນ ຍ້ອນ ການຮັບຮອງ ເອົາ ຫຼັກສູດ ອອນ ໄລ ນ໌ ຢ່າງ ກວ້າງຂວາງ ໃນ ຊ່ວງ ຕົ້ນ ໆ ຂອງ ການລະບາດ .\n",
      "Reference : Doug_Shapiro - giám_đốc điều_hành trung_tâm nghiên_cứu của tổ_chức này , cho biết , sự giảm_sút này ban_đầu có_thể là do bị ảnh_hưởng từ việc ứng_dụng hàng_loạt các khóa học trực_tuyến sau những ngày đầu_tiên của dịch_bệnh .\n",
      "Prediction: Giám_đốc điều_hành Trung_tâm Nghiên_cứu của cơ_quan này , ông Doug_Shapiro , cho biết sự sụt_giảm ban_đầu có_thể là do các khóa học trực_tuyến được công_nhận rộng_rãi trong những ngày đầu đại_dịch .\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 15: Sample Translations\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE TRANSLATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Source    : {test_lo[i]}\")\n",
    "    print(f\"Reference : {test_vi[i]}\")\n",
    "    print(f\"Prediction: {preds_phase1[i]}\")\n",
    "    print(\"-\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5857cc-2938-46f1-b63a-406508d5bb13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
